{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my Static Site of Digital Humanities Assignments Fall 2023 IS578 Course - Introduction to Digital Humanities Site Layout All assignments are their own page found under the \"Digital Humanities\" Tab. Source code can be found here: https://github.com/rschneider98/is578-introduction-dh/","title":"Home"},{"location":"#welcome-to-my-static-site-of-digital-humanities-assignments","text":"Fall 2023 IS578 Course - Introduction to Digital Humanities","title":"Welcome to my Static Site of Digital Humanities Assignments"},{"location":"#site-layout","text":"All assignments are their own page found under the \"Digital Humanities\" Tab. Source code can be found here: https://github.com/rschneider98/is578-introduction-dh/","title":"Site Layout"},{"location":"DH-Mapping/","text":"Mapping I tested out Palladio Mapping with their sample dataset and was able to map birthplaces and places of death in colors and sizing on different layers and also add in a line connecting the two. This was a fairly decent map showing the relationship of movement and was very simple and easy to make with their tool. I then went on to try out GeoJSON.io. This was fairly easy and let you draw directly on the map to create objects in GeoJSON that could be used elsewhere. This would be helpful for defining a bunch of points that might be more easily done manually by the user with a GUI. I made a connection of a couple of ities I visited in Ireland when there this summer for my mother's retirement trip. And finally, I found a polygon shapefile for Lake Michigan's shoreline and loaded that into ArcGIS. ArcGIS seems incredibly powerful with many options for shapefiles and polygons as well as layer management. The graph I made was just a simple polygon overtop a more artistic background instead of the purely-informational terrain, street, or political boundary lines. This would be interesting to show this boundary overtime if I had historic data for terrain, or if I want to map say Huckleberry Finn using the Mississippi River of Mark Twain's time.","title":"Mapping"},{"location":"DH-Mapping/#mapping","text":"I tested out Palladio Mapping with their sample dataset and was able to map birthplaces and places of death in colors and sizing on different layers and also add in a line connecting the two. This was a fairly decent map showing the relationship of movement and was very simple and easy to make with their tool. I then went on to try out GeoJSON.io. This was fairly easy and let you draw directly on the map to create objects in GeoJSON that could be used elsewhere. This would be helpful for defining a bunch of points that might be more easily done manually by the user with a GUI. I made a connection of a couple of ities I visited in Ireland when there this summer for my mother's retirement trip. And finally, I found a polygon shapefile for Lake Michigan's shoreline and loaded that into ArcGIS. ArcGIS seems incredibly powerful with many options for shapefiles and polygons as well as layer management. The graph I made was just a simple polygon overtop a more artistic background instead of the purely-informational terrain, street, or political boundary lines. This would be interesting to show this boundary overtime if I had historic data for terrain, or if I want to map say Huckleberry Finn using the Mississippi River of Mark Twain's time.","title":"Mapping"},{"location":"DH-Minimal-Computing/","text":"Minimal Computing I am evaluating Princeton Library's Blue Mountain Project. The project digitized issues of important periodicals of the European avant-garde and made them fully text searchable. The collection contains 3,747 issues comprising 80,953 pages and 93,148 articles. URL: https://bluemountain.princeton.edu/bluemtn/ When I open the page, there is a website that appears, so at a minimum there is HTML and CSS in use here for this page. At the bottom right of the page I notice the words \"Powered by Veridian\". Following the link to Veridian shows that they are a software company that creates content management systems designed for digital humanities, specifically for digitized archives and collections. Looking again at the main homepage, and clicking inspect-element on the search bar, I find that the search bar is a HTML form field with a submit button. This means that when I click the button, I am directed to a new page with my query in the HTTP request to load the new page. This means that the website is doing some preprocessing on the server-side to load the page before handing it off, but we can keep looking. I switched the developer tools bar to the network tab so that my browser would track all of the requests that the website would send. Once there, I searched for \"death\" in the form. This lead to a new page in the URL (based on the HTML form GET submit button). The page had some parameters in the URL that specified the search criteria. URL: https://bluemountain.princeton.edu/bluemtn/?a=q&hs=1&r=1&results=1&txf=txIN&txq=death&e=-------en-20--1--txt-txIN------- has the txq parameter equal to \"death\" which is what I searched for. Additionally in the network tab there is loading of CSS, JavaScript, and Image, but there are no other API calls that occur, which further suggests that this is a server compiling the HTML that I am viewing (as opposed to a static HTML site or a static site with JavaScript to load and create content). Looking at the \"Veridian-Core.JS\" shows that the underlying JavaScript is not minified and is written with JQuery, which makes usage of any JS framework unlikely. All in all, this is a moderately small webpage at 542kB and does not require the end user render the full webpage. Looking at the September 1916 issue of Der Sturm, the size of the network traffic quickly reached 1MB, which for most people is not an unreasonable size. Broadband minimal speeds of 25 Mbps can still easily get these archive materials, and even average DSL speeds of 10 Mbps are still happy with retrieving and displaying this content. From a digital accessibility standpoint, I think this site meets the goals of minimal computing since the end-user requirements are limited. From a maintenance perspective, this webpage looks like it requires funding from Princeton's library to maintain their contract with Veridian's content management system. Additionally they would also need to pay technical people to host the CMS internally or pay an external service to manage all of the changes. This does not meet some people's definition of \"minimal computing\" since it requires continued maintenance of servers and code. I would say that as long as the work is recognized, this can meet the goals of minimal computing. Finally, the collection is text searchable which is an additional computation, but this layer of computing provides new avenues for discoverability, which I think is a reasonable tradeoff and is part of what makes this a digital collection.","title":"Minimal Computing"},{"location":"DH-Minimal-Computing/#minimal-computing","text":"I am evaluating Princeton Library's Blue Mountain Project. The project digitized issues of important periodicals of the European avant-garde and made them fully text searchable. The collection contains 3,747 issues comprising 80,953 pages and 93,148 articles. URL: https://bluemountain.princeton.edu/bluemtn/ When I open the page, there is a website that appears, so at a minimum there is HTML and CSS in use here for this page. At the bottom right of the page I notice the words \"Powered by Veridian\". Following the link to Veridian shows that they are a software company that creates content management systems designed for digital humanities, specifically for digitized archives and collections. Looking again at the main homepage, and clicking inspect-element on the search bar, I find that the search bar is a HTML form field with a submit button. This means that when I click the button, I am directed to a new page with my query in the HTTP request to load the new page. This means that the website is doing some preprocessing on the server-side to load the page before handing it off, but we can keep looking. I switched the developer tools bar to the network tab so that my browser would track all of the requests that the website would send. Once there, I searched for \"death\" in the form. This lead to a new page in the URL (based on the HTML form GET submit button). The page had some parameters in the URL that specified the search criteria. URL: https://bluemountain.princeton.edu/bluemtn/?a=q&hs=1&r=1&results=1&txf=txIN&txq=death&e=-------en-20--1--txt-txIN------- has the txq parameter equal to \"death\" which is what I searched for. Additionally in the network tab there is loading of CSS, JavaScript, and Image, but there are no other API calls that occur, which further suggests that this is a server compiling the HTML that I am viewing (as opposed to a static HTML site or a static site with JavaScript to load and create content). Looking at the \"Veridian-Core.JS\" shows that the underlying JavaScript is not minified and is written with JQuery, which makes usage of any JS framework unlikely. All in all, this is a moderately small webpage at 542kB and does not require the end user render the full webpage. Looking at the September 1916 issue of Der Sturm, the size of the network traffic quickly reached 1MB, which for most people is not an unreasonable size. Broadband minimal speeds of 25 Mbps can still easily get these archive materials, and even average DSL speeds of 10 Mbps are still happy with retrieving and displaying this content. From a digital accessibility standpoint, I think this site meets the goals of minimal computing since the end-user requirements are limited. From a maintenance perspective, this webpage looks like it requires funding from Princeton's library to maintain their contract with Veridian's content management system. Additionally they would also need to pay technical people to host the CMS internally or pay an external service to manage all of the changes. This does not meet some people's definition of \"minimal computing\" since it requires continued maintenance of servers and code. I would say that as long as the work is recognized, this can meet the goals of minimal computing. Finally, the collection is text searchable which is an additional computation, but this layer of computing provides new avenues for discoverability, which I think is a reasonable tradeoff and is part of what makes this a digital collection.","title":"Minimal Computing"},{"location":"DH-Network-Analysis/","text":"Network Analysis I first tried out Gephi Lite and it took a bit to get used to with how to calculate the metrics and use the interface. I feel like with a bit more control over the initial data this would be a very useful tool. I attempted to see if I could gather any insights with the Java graph, but even with filtering and stats, the data was highly connected and difficult to interpret. I then switched to the Les Miserables graph, which was very interesting. None of the layout algroithms really got down to what I was looking for. But by filtering to nodes with degrees greater than 10, I had a smaller number of people that I was able to manually cluster. I was also able to change the edge to the color of the target node, which helps with differentiating the paths between characters. Then switching on over to Palladio, I used the test dataset there to see what was doable. I feel like this has much more potential as a mapping tool, but the graph capabilities were interesting. Gephi was definitely better in terms of features, but Palladio was able to plot the data in a meaningful way. Here I looked at the occupation and location of those in this graph. It showed primary cities for centrality and key job classes for what I were artists. There were also interested separated graphs.","title":"Network Analysis"},{"location":"DH-Network-Analysis/#network-analysis","text":"I first tried out Gephi Lite and it took a bit to get used to with how to calculate the metrics and use the interface. I feel like with a bit more control over the initial data this would be a very useful tool. I attempted to see if I could gather any insights with the Java graph, but even with filtering and stats, the data was highly connected and difficult to interpret. I then switched to the Les Miserables graph, which was very interesting. None of the layout algroithms really got down to what I was looking for. But by filtering to nodes with degrees greater than 10, I had a smaller number of people that I was able to manually cluster. I was also able to change the edge to the color of the target node, which helps with differentiating the paths between characters. Then switching on over to Palladio, I used the test dataset there to see what was doable. I feel like this has much more potential as a mapping tool, but the graph capabilities were interesting. Gephi was definitely better in terms of features, but Palladio was able to plot the data in a meaningful way. Here I looked at the occupation and location of those in this graph. It showed primary cities for centrality and key job classes for what I were artists. There were also interested separated graphs.","title":"Network Analysis"},{"location":"DH-Static-Website/","text":"Static Websites I decided to use MKdocs since I am more familiar with Python and this is what we use at work to create internal wikis. I have never set up a project for this or managed the configuration, so this will be a fun learning experience. https://www.mkdocs.org/getting-started/ I already have python installed so I can easily add this package: python -m pip install mkdocs","title":"Static Website"},{"location":"DH-Static-Website/#static-websites","text":"I decided to use MKdocs since I am more familiar with Python and this is what we use at work to create internal wikis. I have never set up a project for this or managed the configuration, so this will be a fun learning experience. https://www.mkdocs.org/getting-started/ I already have python installed so I can easily add this package: python -m pip install mkdocs","title":"Static Websites"},{"location":"DH-Text-Analysis/","text":"Text Analysis I am responding to the first prompt for the assignment. Introduction Looking at usages of DH tools for text analysis I found two articles that use these tools for literary research. The first article I found was, \"Text-Mining Short Fiction by Zora Neale Hurston and Richard Wright using Voyant Tools\" by K. Rambsy in CLA Journal v59(3), pp. 251 - 258. The second article I found was, \"Close- and Distant-Reading Modernism\" by J. Drouin in The Journal of Modern Periodical Studies v5(1), pp. 110-135. 1) \"Text-Mining Short Fiction by Zora Neale Hurston and Richard Wright using Voyant Tools\" Hurston and Wright are both prominent African American writers and this article wanted to look at using Voyant as a way to compare their short stories and quantify usage of African American Vernacular English (AAVE). The passages in Standard English and AAVE were found to variety in length and context, and specific words and phrases were found to be heavily reused by Wright. In some sections of the works, Wright reused 23 AAVE phrases 20 times, which was much more than the 6 phrases 20 times in other areas. These two authors also had critiques of one another's representation, Rambsy wrote \"Wright found fault with Hurston for seemingly playing into a folk romanticism that was stereotypical and one that white reading audiences enjoyed. Hurston, on the other hand, critiqued Wright for his portrayals of the South, linking his overwhelming use of violence to a hyper-masculine narrative\" (p. 256). Based on the background of critique between these two authors, Rambsy suggests that the differences in AAVE usage to represent African American culture is possibly based on their personal ideologies. This tool helped provide a new look and framework to analyze these readings. 2) \"Close- and Distant-Reading Modernism\" Drouin performed a close and distant reading of the September 1918 issue of The Little Review which stood due to its seemingly uniqueness of dealing with death and WWI. The close reading of this issue follows tranditional literary techniques, but it is also combined with the distant reading of The Little Review issues from 1910 to 1922. This magazine also had a TEI edition that was able to be used in Voyant for word analysis. They also collected data in graduate student workshops to create a dataset based on a literary magazine's publications to link authors, works, genres, and topics. This dataset was used in Gephi to provide networking context to certain authors and topics. Some of the analysis showed that topics of death were associated to poetry instead of the short story genre, and that the short story group had a pronounced isolation in topics. Digital Humanities Trends One trend I see with textual analysis and how digital humanities tools being used is that this \"distant-reading\" techniques can be used to help guide or focus more traditional works. Viewing language and literary work in new light or under a different lens helped the second author consider themes of death in poems and how that occurred prior to the issue that was singled out for mentioning WWI and death. In the same vein, another trend is the use of textual analysis as textual discovery. When following close reading, pulling out themes, words, and other topics is a goal, but sometimes there are topics and associations missed. In the first paper, the author notes that they singled out passages where \"gun\" was used since that was the second most frequently occurring word; this lead to a close reading around that word and its usage even though as a fairly basic noun in the short story it might not have been identified otherwise. Links to Articles: \"Text-Mining Short Fiction by Zora Neale Hurston and Richard Wright using Voyant Tools\" \"Close- and Distant-Reading Modernism\"","title":"Text Analysis"},{"location":"DH-Text-Analysis/#text-analysis","text":"I am responding to the first prompt for the assignment.","title":"Text Analysis"},{"location":"DH-Text-Analysis/#introduction","text":"Looking at usages of DH tools for text analysis I found two articles that use these tools for literary research. The first article I found was, \"Text-Mining Short Fiction by Zora Neale Hurston and Richard Wright using Voyant Tools\" by K. Rambsy in CLA Journal v59(3), pp. 251 - 258. The second article I found was, \"Close- and Distant-Reading Modernism\" by J. Drouin in The Journal of Modern Periodical Studies v5(1), pp. 110-135.","title":"Introduction"},{"location":"DH-Text-Analysis/#1-text-mining-short-fiction-by-zora-neale-hurston-and-richard-wright-using-voyant-tools","text":"Hurston and Wright are both prominent African American writers and this article wanted to look at using Voyant as a way to compare their short stories and quantify usage of African American Vernacular English (AAVE). The passages in Standard English and AAVE were found to variety in length and context, and specific words and phrases were found to be heavily reused by Wright. In some sections of the works, Wright reused 23 AAVE phrases 20 times, which was much more than the 6 phrases 20 times in other areas. These two authors also had critiques of one another's representation, Rambsy wrote \"Wright found fault with Hurston for seemingly playing into a folk romanticism that was stereotypical and one that white reading audiences enjoyed. Hurston, on the other hand, critiqued Wright for his portrayals of the South, linking his overwhelming use of violence to a hyper-masculine narrative\" (p. 256). Based on the background of critique between these two authors, Rambsy suggests that the differences in AAVE usage to represent African American culture is possibly based on their personal ideologies. This tool helped provide a new look and framework to analyze these readings.","title":"1) \"Text-Mining Short Fiction by Zora Neale Hurston and Richard Wright using Voyant Tools\""},{"location":"DH-Text-Analysis/#2-close-and-distant-reading-modernism","text":"Drouin performed a close and distant reading of the September 1918 issue of The Little Review which stood due to its seemingly uniqueness of dealing with death and WWI. The close reading of this issue follows tranditional literary techniques, but it is also combined with the distant reading of The Little Review issues from 1910 to 1922. This magazine also had a TEI edition that was able to be used in Voyant for word analysis. They also collected data in graduate student workshops to create a dataset based on a literary magazine's publications to link authors, works, genres, and topics. This dataset was used in Gephi to provide networking context to certain authors and topics. Some of the analysis showed that topics of death were associated to poetry instead of the short story genre, and that the short story group had a pronounced isolation in topics.","title":"2) \"Close- and Distant-Reading Modernism\""},{"location":"DH-Text-Analysis/#digital-humanities-trends","text":"One trend I see with textual analysis and how digital humanities tools being used is that this \"distant-reading\" techniques can be used to help guide or focus more traditional works. Viewing language and literary work in new light or under a different lens helped the second author consider themes of death in poems and how that occurred prior to the issue that was singled out for mentioning WWI and death. In the same vein, another trend is the use of textual analysis as textual discovery. When following close reading, pulling out themes, words, and other topics is a goal, but sometimes there are topics and associations missed. In the first paper, the author notes that they singled out passages where \"gun\" was used since that was the second most frequently occurring word; this lead to a close reading around that word and its usage even though as a fairly basic noun in the short story it might not have been identified otherwise.","title":"Digital Humanities Trends"},{"location":"DH-Text-Analysis/#links-to-articles","text":"\"Text-Mining Short Fiction by Zora Neale Hurston and Richard Wright using Voyant Tools\" \"Close- and Distant-Reading Modernism\"","title":"Links to Articles:"},{"location":"DH-Tool/","text":"DigHum Tool: Palladio Palladio is a network analysis and visualization to designed to graph relationships and geography as well as perform statistical calculations about the relationships. The tool was built to generalize and extend the work done for the Mapping the Republic of Letters project which looked at the scientific correspondance archives at a large scale. The project felt that the letters were looked at in pieces are their respective archives or for the creation of critical editions of key subsets. The tool helped to map who corresponded with whom, and thus start to analyze how information could have travelled. I consider this a DH tool because it takes analysis that would be done in small, laborous chunks over time by many researchers to tease apart correspondance groupings. So this is then technology that extends that framework to look at more information in similar ways. Instead of a network analysis of researchers at Oxford 1630-1650 (significantly limiting scope), the tool helps answer questions about how the communities were connected and identifies potential areas for future research. So someone could isolate a different communal subset, timeperiod, or information to create a new set of critical editions to better understand the history. I have no experience using this tool, but I have done similar network analysis and graph theory statistics.","title":"Palladio Tool"},{"location":"DH-Tool/#dighum-tool-palladio","text":"Palladio is a network analysis and visualization to designed to graph relationships and geography as well as perform statistical calculations about the relationships. The tool was built to generalize and extend the work done for the Mapping the Republic of Letters project which looked at the scientific correspondance archives at a large scale. The project felt that the letters were looked at in pieces are their respective archives or for the creation of critical editions of key subsets. The tool helped to map who corresponded with whom, and thus start to analyze how information could have travelled. I consider this a DH tool because it takes analysis that would be done in small, laborous chunks over time by many researchers to tease apart correspondance groupings. So this is then technology that extends that framework to look at more information in similar ways. Instead of a network analysis of researchers at Oxford 1630-1650 (significantly limiting scope), the tool helps answer questions about how the communities were connected and identifies potential areas for future research. So someone could isolate a different communal subset, timeperiod, or information to create a new set of critical editions to better understand the history. I have no experience using this tool, but I have done similar network analysis and graph theory statistics.","title":"DigHum Tool: Palladio"},{"location":"DH-Tools-Info/","text":"Digital Humanities Tools Database Information Origin Data was compiled from this gitlab discussion on DH tools: https://github.com/ZoeLeBlanc/is578-intro-dh/discussions/2 Contents Tool Name (str): Name of the tool described Description (str): Short description of the tool's usage Tool Category (str): Main category of tool's usage Abstract Name (str): Title of linked research that uses this tool Link to Abstract (URL): Link to research that uses this tool Link to Post (URL): Link to the submitted GitLab markdown file describing the tool Link to Tool (URL): Link to tool's website Link to Contributor (str): GitLab User ID Experience (str): Level of experience the contributor has with the tool (None, Some, or Yes)","title":"DigHum Tools"},{"location":"DH-Tools-Info/#digital-humanities-tools-database-information","text":"","title":"Digital Humanities Tools Database Information"},{"location":"DH-Tools-Info/#origin","text":"Data was compiled from this gitlab discussion on DH tools: https://github.com/ZoeLeBlanc/is578-intro-dh/discussions/2","title":"Origin"},{"location":"DH-Tools-Info/#contents","text":"Tool Name (str): Name of the tool described Description (str): Short description of the tool's usage Tool Category (str): Main category of tool's usage Abstract Name (str): Title of linked research that uses this tool Link to Abstract (URL): Link to research that uses this tool Link to Post (URL): Link to the submitted GitLab markdown file describing the tool Link to Tool (URL): Link to tool's website Link to Contributor (str): GitLab User ID Experience (str): Level of experience the contributor has with the tool (None, Some, or Yes)","title":"Contents"}]}