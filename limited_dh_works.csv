work_id,conference_label,conference_short_title,conference_theme_title,conference_year,conference_organizers,conference_series,conference_hosting_institutions,conference_city,conference_state,conference_country,conference_url,work_title,work_url,work_authors,work_type,full_text,full_text_type,full_text_license,parent_work_id,keywords,languages,topics
113,2001 - New York University,New York University,,2001,ACH;ALLC,ACH/ALLC;ACH/ICCH;ALLC/EADH,New York University,New York,NY,United States,https://web.archive.org/web/20011127030143/http://www.nyu.edu/its/humanities/ach_allc2001/,Building spell-checking facilities for ancient Spanish,,Alejandro Bia;Manuel Sanchez-Quero,paper,"The huge development of information technology has motivated the appearance of this new type of libraries, called digital libraries (Arms, 2000). The Miguel de Cervantes Digital Library (http://cervantesvirtual.com) is one of the most ambitious projects of its kind ever to have been undertaken in the Spanish-speaking world with more that 4000 digital books at present. This enormous amount of digitised works are mostly Hispanic classics from the 12th up to the 20th century. The development of these digital books require a lot of care from the point of view of correction and editing, but can be processed in a massive uniform way afterwards to produce the different publications formats and services offered to the readers.

Concerning human resources involved in the project, the biggest group by far corresponds to correction and markup people (Bia and Pedreño, 2000), who are in charge of the hardest-to-automate part of the production process, which involves reading and correcting digitisation errors, structurally marking up the texts, and taking important editing decisions that involve both rendering and functionality of the hypertext documents to be published. These humanists are highly skilled people with at least a bachelor degree in philology, or other humanistic disciplines. We want them to devote their time to higher intellectual tasks like taking editing or markup decisions, or preparing the texts for interesting Internet services (like text analysis or concordance queries), than to spend their energies in the tedious mechanical task of correction, the main bottleneck in our production workflow, and by far the most time-consuming task.

In the case of contemporary works, spell-checkers turned out to be a useful aid to the correction process, but for literary works written in ancient Spanish, commercially available modern spell-checkers may produce more mistakes than they can prevent. The reason for this is that spell-checker-dictionaries include only modern uses of the language, and when they are applied to old texts, the result is that they take correct ancient uses of words for mistakes and try to correct them. Unable to use spell-checking as an aid, correctors have to do a side by side comparison of the original and the digitised texts to detect the errors.

Being aware of the usefulness of spell-checkers on the correction of modern works, and lacking this facility for ancient texts, we decided to build dictionaries for ancient Spanish. These decision led to new problems and new questions. As there is no such thing as ancient Spanish, but instead a dynamically evolving language that changes through the centuries, how many old-Spanish dictionaries should we build? Should we set arbitrary chronological limits?

Taking advantage of the 4000 books already digitised and corrected at the Miguel de Cervantes Digital Library, as a corpus covering several centuries of Spanish writings, we’ve built a time-aware system of dictionaries that takes into account the temporal dynamics of language, to help solve the problem of ancient Spanish spell-checking.

In this paper we present the problems we have found, the decisions we have made and the conclusions and results we arrived at. We have also been able to extract statistical information on the evolution of the Spanish language through time. The final section of the paper deals with the technical details of this project and the innovative application of digital methods like the use of TEI ans XML markup.

References

William Arms 
Digital Libraries 
MIT Press, 2000, Cambridge, Massachusetts, ISBN 0-262-01880-8.

C.M. Sperberg-McQueen and Lou Burnard, editors. 
Guidelines for Electronic Text Encoding and Interchange (Text Encoding Initiative P3), Revised Reprint, Oxford, May 1999.
TEI P3 Text Encoding Initiative, Chicago - Oxford, May 1994.

Alejandro Bia and Andrés Pedreño. 
The Miguel de Cervantes Digital Library: The Hispanic Voice on the WEB. 
LLC (Literary and Linguistic Computing) journal, Oxford University Press, (to be published soon) 2000. 
Presented at ALLC/ACH 2000, The Joint International Conference of the Association for Literary and Linguistic Computing and the Association for Computers and the humanities, 21/25 July 2000, University of Glasgow.

David Hunter, Curt Cagle, Dave Gibbons, Nikola Ozu, Jon Pinnock, and Paul Spencer. 
Beginning XML. 
Programmer to Programmer. Wrox Press, 1102 Warwick Road, Acocks Green, Birmingham, B27 6BH, UK, 1st edition, 2000.

Alejandro Bia 
Automating the Workflow of the Miguel de Cervantes Digital Library 
Poster at the ACM-DL'2000 Digital Libraries conference, June 2000, Menger Hotel, San Antonio, Texas, USA.

Olumide Owolabi 
Efficient pattern searching over large dictionaries 
Information Processing Letters, v.47, n.1, 17-21, August 1993.

Real Academia Española 
Diccionario de la lengua española 
Espasa Calpe, 1992, Madrid.",txt,This text is republished here with permission from the original rights holder.,,automatic learning;dictionaries;digital libraries;nlp resources;spell-checking,English,
1690,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Expanding and connecting the annotation tool ELAN,,Han Sloetjes;Aarthy Somasundaram;Sebastian Drude;Herman Stehouwer;Kees Jan van de Looij,"paper, specified ""short paper""","Abstract
The annotation tool ELAN allows for adding time-linked textual annotations to digital audio and video recordings. It is applied in various disciplines within the humanities, with linguistics, sign language and gesture research represented most prominently in its user base. This paper highlights new developments in ELAN with an emphasis on those features that introduced new technological and methodological approaches to analysing both audio/video and derived textual data. 1. Introduction
Annotation of audio and video recordings, be it manual or (semi-)automatic, is a crucial step in many areas of research within the humanities. ELAN [1] , developed at The Language Archive (TLA) [2] /Max Planck Institute for Psycholinguistics, is a tool for manual annotation that is already available for more than a decade and that is applied in various types of projects: language documentation, sign language and gesture studies, psychological and educational behaviour studies etc. ELAN enables users to create multi-levelled, multi-participant, time-linked annotations to one or more media streams, including timeseries streams. Both qualitative and quantitative research is supported; arguably the qualitative oriented use is predominant but the quantitative application is gaining popularity. In this paper we focus on recent developments that improve the workflow of researchers by introducing task oriented modes, expand the scope of the program by implementing a framework for computational annotation creation modules and by connecting to web services that, in a similar way, apply computational techniques to create annotations. ELAN is free and open source software and runs on Windows, MacOS and Linux.

2. The Interlinearization mode and text processing modules in Lexan
One of the recent and still ongoing developments concerns the introduction of the Interlinearization mode. This mode, on the one hand, provides a user interface optimized for the task of adding linguistically relevant layers to an orthographical transcription of the media. Layers for morphological break down, part of speech tags and glossing are part of the common repertoire of documentary linguists (Bow 2003). On the other hand this mode is the hub to Lexan, the extensible framework for annotation and text processing modules. Such modules can perform a variety of tasks, from simple to complex, from word segmentation to interlinearization based on machine learning algorithms. Some modules are expected to produce multiple suggestions for new annotation layers and to improve their suggestions based on interactive user feedback accommodated by the user interface of, primarily, the interlinearization mode. The name ""Lexan"" indicates that this framework interconnects ELAN with the TLA multimedia lexicon tool LEXUS [3] . This architecture allows to build and enrich a lexicon while annotating and at the same time to use information in the lexicon in the annotation suggestions process. This combination of NLP (Natural Language Processing) techniques with manual media annotation marks a new line of development in ELAN and brings together technologies that usually seem to develop apart.

For this sort of work other tools are and have been around for a long time and providing interoperability with these tools (often implemented on the level of file format conversion) is highly important for many users.

3. Interoperability with FLEx
The FieldWorks Language Explorer [4] is a prominent example of such tools, therefore import and export facilities for the FLEx format have been implemented and revised with the goal to make repeated transfer of data (""round-tripping"") between the tools as seamless as possible. Importing FLEx files was possible since ELAN version 3.8 (2009) but because the FLEx format at that time did not support time alignment and speaker information, an export function was not implemented simultaneously. That has been added recently, after the introduction of the ""begin-time-offset"", ""end-time-offset"" and ""speaker"" attributes at the phrase level of the flextext format (2012, FLEx 7). The import has been updated such that per speaker a group of tiers is created. Additionally efforts are made to retain punctuation and font information where possible. Punctuation elements are on import linked to an ISOcatdata category so that on export these elements receive the correct attribute again. Exporting an ELAN document that is the result of a FLEx import is fairly straightforward. Exporting just any ELAN document to FLEx remains a challenge; where ELAN is very flexible and allows to have any number of tiers without predefined content designation, there is FLEx much more rigid, providing a fixed set of layers of known categories. Resolving the mapping from one to the other is not (always) possible without user intervention.

4. Connecting to web services and online resources
ELAN is a standalone desktop application that in principle works with locally available (local hard drive, local network) resources. Audio and video files are (more and more) often very large, up to several Gb. per file, and high accuracy annotation is still problematic when using media streaming, even in situations with high speed internet connections. For the vast majority of features of ELAN an internet connection is not required, but recently several options have been added that allow the user to connect to online services and resources. In 2008 association of tiers and annotations with ISOcat [5] data categories was introduced and this feature has recently been improved and made more relevant. By default tiers are generic annotation ""containers"", oblivious of the type of content of the annotations; there are no predefined tiers, e.g. ""translation tier"" or ""gesture phases tier"". By associating a tier with a concept registered and described in ISOcat it acquires an explicit content designation.

In the CLARIN-NL SignLin [6] project support for external controlled vocabularies was added, enabling collaborators to share vocabularies over the network (Crasborn and Sloetjes 2010). This feature improves consistency within a team and prevents team members from making (unchecked) changes to the vocabularies. In the context of several CLARIN [7] projects and of the AVATecH [8] project extensions were developed that call web services which produce segmentations and/or annotation content taking audio, video or text, or a combination thereof, as input. The WebLicht [9] tool chaining framework is a core service in CLARIN-D [10] (Hinrichs, et al. 2010) and preliminary support for calling services registered with this framework is now available to users of ELAN. Tiers can be uploaded (in the required XML format, TCF) in order to be processed by well known parsers and taggers; the results are added as new tiers and thus enrich the annotation document.

No matter how useful these web services are or will become, for many field linguists, and other researchers who are working offline a lot, these provisions will not be available. Therefore the core functions will always be independent of online services. For some services, like ISOcat, it is possible to work with a local cache; a selection of categories is stored on the local machine for use while offline.


Figure 1
Preparing a web service call

Most parsers and taggers are only available for a small number of major languages, linguists who study lesser described, let alone, endangered languages usually don't have similar mature, well tested and well trained systems at their disposal. The Lexan approach, stepwise building up ""personalized"" computational assistance based on the input and feedback of the user, can come to their rescue.

5. Local corpus enrichment and exploration
Though ELAN has been a multiple document application almost from the start, most functions of ELAN allow the user to interact with one, the current active, document. But in recent years more and more functions have been introduced that operate on multiple files e.g. on an entire local corpus. The urge for such functions emerged with the growing number of recordings and transcriptions research teams nowadays are working on (Johnston 2010).

A shortlist of multiple files functions contains creation of transcriptions for a set of recordings, editing the collection of tiers in transcriptions, creating annotations by applying logical operations (AND, OR, XOR) on annotations of selected tiers (Lausberg and Sloetjes 2009), extracting information by executing search queries, generating simple statistics, converting multiple files to and from specific formats etc.

For some types of research assessing the quality of the annotations and the skills of the annotators is crucial. How the inter-annotator reliability best is assessed is still under discussion (Gut 2004; Holle and Rein 2012; Lücking 2011) and the best approach can differ depending on the properties of the data and the focus of the research. A few algorithms for calculating the inter-annotator agreement have been implemented in ELAN and are available for application on multiple files. Especially concerning time-alignment (the segmentation step of the annotation process) there seems to be no generally accepted algorithm for assessing agreement. By offering several alternatives, the choice remains to the user while some of the hassle of exporting data to other tools is taken away from her/him.

6. Conclusion
In this paper we show how researchers working with digital audio/video materials across disciplines can apply new technologies as a result of connections established between ELAN and local or online modules and services. Features that allow to enrich and explore a local corpus are introduced and briefly discussed.

References
Bow, C., B. Hughes, and S. Bird (2003). Towards a general model of interlinear text. In Proceedings of EMELD Workshop 2003: Digitizing and Annotating Texts and Field Recordings. Lansing: MI
Crasborn, O., and H. Sloetjes (2010). Using ELAN for annotating sign language corpora in a team setting. In Proceedings of the 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies. Paris: ELRA, 137-142.
Gut, U., and P. Bayerl (2004). Measuring the Reliability of Manual Annotations of Speech Corpora. In Proceedings of Speech Prosody, Nara.
Hinrichs, M., T. Zastrow, and E. Hinrichs (2010). WebLicht: Web-based LRT Services in a Distributed eScience Infrastructure. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10). Valletta, Malta.
Holle, H., R. Rein Assessing interrater agreement of movement annotations. In Lausberg, H. (ed.), Neuroges: The Neuropsychological Gesture Coding System. Berlin: Peter Lang.
Johnston, T. (2010). Adding value to, and extracting of value from, a signed language corpus through secondary processing: implications for annotation schemas and corpus creation. In Proceedings of the 4th Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies. Paris: ELRA, 137-142.
Lausberg, H., and H. Sloetjes (2009). Coding gestural behavior with the NEUROGES-ELAN system. Behavior Research Methods, Instruments, & Computers 41(3):841-849.
Lücking, A., Ptock, S., and Bergmann, K. (2011). Staccato: Segmentation Agreement Calculator. Proc. of the 9th International Gesture Workshop held 25-27 May in Athens, Greece.
Notes
http://tla.mpi.nl/tools/tla-tools/elan
http://tla.mpi.nl
http://tla.mpi.nl/tools/tla-tools/lexus
http://fieldworks.sil.org/flex
http://www.isocat.org
http://www.ru.nl/sign-lang/@673229/pagina/
http://www.clarin.eu
http://www.mpi.nl/avatech
http://www.clarin-d.de/language-resources/weblicht-en.html
http://www.clarin-d.de",txt,This text is republished here with permission from the original rights holder.,,annotation;interlinear text;multimedia;nlp;web services,English,"audio, video, multimedia;content analysis;corpora and corpus activities;linking and annotation;natural language processing;programming;software design and development;spatio-temporal modeling, analysis and visualisation;standards and interoperability"
1766,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Reading the Visual Page of Victorian Poetry,,Natalie M Houston;Michael Neal Audenaert,"paper, specified ""long paper""","The digitization of nineteenth-century texts offers us the opportunity of asking new research questions that could transform our historical understanding of Victorian culture. Digital access to the breadth of nineteenth-century print culture, which included books, periodicals, and newspapers published for an ever-increasing reading audience, puts pressure on traditional configurations of the literary canon, which examines only a limited number of authors and texts. As Dan Cohen asks, ‘Should we be worrying that our scholarship might be anecdotally correct but comprehensively wrong? Is 1 or 10 or 100 or 1000 books an adequate sample to know the Victorians?’ (Cohen). In developing VisualPage, a software application for the large-scale identification and analysis of the graphical elements of digitized printed books, we will enable researchers to identify unique or representative examples across very large data sets of digitized texts. Such computational analysis will reveal new ways of thinking about both the printed book and its digitized forms. This paper presents the current development of this proof-of-concept software (funded in 2012 by a Level II NEH Digital Humanities Start-Up Grant) and some findings from the analysis of our initial data set.
Large Scale Analysis and Victorian Books
As Franco Moretti notes, the traditional Victorian canon includes only a ‘minimal fraction of the literary field’ constituted by all the texts published in the period. He calls for new methods of analysis because ‘a field this large cannot be understood by stitching together separate bits of knowledge about individual cases, because it isn't a sum of individual cases: it's a collective system’ (3-4). To understand the literary field as a system requires examining what Pierre Bourdieu calls ‘position-takings’ — all of the actions, persons, and objects that produce a work of art and its cultural value, including those ‘which, because they were part of the self-evident givens of the situation, remain unremarked and are therefore unlikely to be mentioned in contemporary accounts’ (30-31). The structures of cultural value that surrounded Alfred Tennyson's In Memoriam, Christina Rossetti's Goblin Market and Other Poems, or any other volume of Victorian poetry were partly created by all the other books of poetry, including what Bourdieu calls the ‘unremarked’ ones that were overlooked by both Victorian critics and scholars in our own day.
Digitization offers us the possibility of expanding our study of Victorian literature to include previously ‘unremarked’ texts. However, most tools for large-scale research focus on the linguistic content of texts, through either syntactic or semantic analysis. But printed texts simultaneously convey meaning through both linguistic and graphic signs. As Jerome McGann suggests, ‘text documents, while coded bibliographically and semantically, are all marked graphically’ and a ‘page of printed or scripted text should thus be understood as a certain kind of graphic interface’ (138, 199). We’ve taken poetry as our starting point for VisualPage because the visual appearance of the printed page contributes to the reader’s understanding of the poem’s form and meaning through the conventions of line capitalization, punctuation, and indentation.
Printed poems are typically framed by the white space created by line endings, creating a distinctive visual signal of the genre on the printed page. Experienced readers evaluate the graphical codes of printed texts quickly, often subconsciously; as Johanna Drucker suggests, ‘we see before we read and the recognition thus produced predisposes us to reading according to specific graphic codes before we engage with the language of the text’ (242). In Victorian books of poetry, for example, rhymed lines were frequently indented the same distance from the left margin to visually indicate the poem’s form and structure. Rhyme is thus simultaneously a linguistic, poetic, and graphic feature of many Victorian books.
Scholars have long realized that ‘Typographic transcriptions . . . abstract texts from the artifacts in which they are versioned and embodied’ (Viscomi 29). Although full bibliographical analysis of a book is not available from a digital surrogate, digital images of a book’s pages offer researchers more information about ‘the interaction of its physical characteristics with its signifying strategies’ than can text alone (Hayles 103). Accordingly, most scholarly digital archive projects today recognize the value of this graphical meaning and provide users access to both digitized page images and plain text versions. But until now, researchers have been limited to only what their human eyes can see or recognize in those page images. Developing tools for the large scale graphical analysis of digitized books will contribute to a broader understanding of literature’s circulation, consumption, and function within Victorian culture.
Overview of the VisualPage Application
In order to make the visual structure of document images explicit and available to both computational processing and interactive human analysis, the VisualPage application is designed around three inter-related tasks. The Feature Extraction module analyzes the digitized page images in order to translate pixels into the language of visual features used to design and analyze page layout: typeface size; margin size; width, height and spacing of text lines; and more. These features are then organized in relation to bibliographic categories, such as volumes, poems, and pages, in order to enable questions such as ‘how much variability is there in the length of lines in poems from two different publishers?’ or ‘how does the visual density of a page change for this publisher over time?’ VisualPage is designed so that the specific set of features extracted from a collection can be changed in response to new analytical needs and new technical capabilities.
Once these features have been extracted and stored in an attribute-relation file (ARFF), the next task is to discover relationships within the data. This is the responsibility of the Pattern Recognition module. The pattern recognition module will support basic queries such as ‘find all poems that use dropped capital letters’ or ‘find poems whose line length is in the bottom 25% of poems from this publisher.’ It will also enable more sophisticated data mining based on machine-learning techniques. Simple examples include the ability to cluster documents based on a set of features such as margin width and line height or to find documents that are ‘visually similar’ to a set of known documents.
Finally, the Analysis module presents data visualization and exploration interfaces. This is the outward-facing portion of the application that allows scholars to interact with the documents and to harness the pattern recognition tools in order to pose new questions and discover new relationships within the collection.
During the start-up phase of this project, we are focusing software development work toward two main objectives. First, we are designing the main structure of each module and implementing an initial set of features that can be extended and enhanced through future work. Second, we are performing an initial proof-of-concept prototyping for the more technically complex components of the system. Notably, this includes:
•        recognition of the low-level image features
•        understanding the higher-level structure in terms of both poetry (e.g., titles, epigraphs, stanzas as they are found within a single page and across multiple pages) and page layout (margins, running heads, page numbers, footnotes, etc.)
•        analyzing these structures using pattern recognition and machine learning techniques
This proof-of-concept work addresses questions about which approaches hold the most promise for scholarly research in addition to demonstrating the technical feasibility of our approach. In order to ensure that the techniques we develop are appropriate to collections beyond that which can be easily analyzed and comprehended by a single scholar, our initial data set consists of 300 single-author books of poetry published between 1860-1880, or approximately 60,000 page images.
Initial Research Findings
In presenting research findings from our initial data set of single-author books of poetry, we will focus on three main areas of research:
•        identifying historical changes correlated with particular publishers in the printing of poetry during the period 1860-1880
•        analyzing line indenting and line length to understand Victorian rhyme and poetic form across a varied set of authors and texts
•        identifying computationally significant patterns or trends in the graphical design of Victorian books
Our VisualPage software enables researchers to move beyond our human capacity to view, compare, and understand only a limited number of texts at one time. Large-scale analysis of the graphical dimensions of previously ‘unremarked’ books offers us the possibility of understanding the cultural field of Victorian poetry in all its historical complexity.
Funding
This work was supported by the National Endowment for the Humanities [HD5156012].
References
Bourdieu, P. (1993). The Field of Cultural Production, or: The Economic World Reversed. In Johnson, R. (ed). The Field of Cultural Production: Essays on Art and Literature. New York: Columbia University Press.
Cohen, D. (2010). Searching for the Victorians. Dan Cohen, http://www.dancohen.org/2010/10/04/searching-for-the-victorians (accessed 4 October 2010).
Drucker, J. (2009). SpecLab: Digital Aesthetics and Projects in Speculative Computing. Chicago: University of Chicago Press.
Hayles, N. K. (2005). My Mother was a Computer: Digital Subjects and Literary Texts. Chicago: University of Chicago Press.
McGann, J. (2001). Radiant Textuality: Literature after the World Wide Web. New York: Palgrave Macmillan.
Moretti, F. (2005). Graphs, Maps, Trees: Abstract Models for a Literary History. London: Verso.
Viscomi, J. (2002). Digital Facsimiles: Reading the William Blake Archive. Computers and the Humanities 36(1). 27-48.",txt,This text is republished here with permission from the original rights holder.,,machine learning;ocr;poetry;textuality,English,bibliographic methods / textual studies;data mining / text mining;image processing;literary studies;software design and development;text analysis
1803,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Visualizing Uncertainty: How to Use the Fuzzy Data of 550 Medieval Texts?,,Stefan Jänicke;David Joseph Wrisley,"paper, specified ""short paper""","Our research project Visualizing Medieval Places brings together a computer scientist and a literary historian. We use the web-based tool GeoTemCo (Jänicke, 2012) to visualize thousands of place names against a focusable timeline. The resulting geospatial-temporal visualization is a way for the researcher to analyze space and time in a historical corpus of literature. The ideal user interface will allow manipulation of the visualization by (1) dynamically changing the thresholds for both visualizing and suppressing given types of uncertainty in the geospatial and temporal dimensions, and (2) adding or removing facets (e.g. particular genres, time ranges) to broaden or constrain the amount of data to be displayed. This interactivity will hopefully allow for controlled visualization of literary data, and will facilitate the formation of nuanced, supportable hypotheses about time and space in literature. Figure 1 illustrates the current user interface.
 
Figure 1:
The current GeoTemCo user interface, showing 636 data points from the Franco-Italian dataset. NB: The time line at the bottom does not represent the real temporal data of the project.
The data set is being built using a corpus of nearly 550 medieval French texts. Unlike English or Classics, scholars of medieval French have few electronic texts at their disposal. Furthermore, spelling variance of toponyms in medieval vernacular texts poses a significant challenge for any semi-automatic extraction. We are considering combining our geospatial data of French places with Latin place names to build a bilingual gazetteer for use by the digital medievalist community in the future. For now, the toponyms (and their variants) are being manually harvested from a canonical reference work, the Table des noms propres (Flutre, 1962); they are subsequently disambiguated and geocoded. Unlike narratologically-inspired digital literary geographies such as The Literary Atlas of Europe (Hurni, Piatti et al., 2012) constructed on close readings of fictional and vernacular spaces, the data model for Visualizing Medieval Places includes only real geographical place names. It shares more with the GIS-based analysis of unstructured texts found in the Lancaster University initiative entitled Spatial Humanities (Gregory et al., 2012). Since Flutre’s work does not fully represent the variety of textual communities and genres of medieval French, we are also extracting place names from name indices in selected critical editions. The first subset of data points from Franco-Italian literature is virtually complete. The project uses the crowd-sourced Archives de littérature du moyen âge (Brun, 2012) to enrich the metadata about the texts.
Using the data has proved problematic since so many aspects of it are uncertain. Situating the composition of medieval texts in a specific time and place can be at best speculative. Date formats of traditional scholarship have been represented in idiosyncratic ways (e.g. between 1095-1291, first half of the 14th century, before 1453). Likewise, the toponyms found in these works are difficult for various reasons: they are unmappable, they can refer to multiple places, or they designate ancient Greco-Latin or medieval geographical zones no longer found on the contemporary map.
The visualization of uncertainty is a hot topic in the visualization community. Despite a broad set of applications in this field, there are still no straightforward solutions for displaying multiple, overlapping kinds of uncertainty within one set of visual interfaces. Drawing upon a long list of uncertainty types (Griethe et al., 2006), a data item within our project might be said to embody two basic kinds of uncertainty. The first uncertainty is one of “lineage,” by which we mean the reliability of the text source. Certainty values for lineage can simultaneously affect the representation of data items in both dimensions, the geospatial and temporal. The second uncertainty is one of “accuracy,” referring to the granularity of place or time, that is, to the distinctly sized intervals in which a value can lie. Again, granularity impacts both dimensions, the geospatial (with units such as landmarks, localities, regions, countries, continents) and the temporal(years, eras, as well as upper- or lower-bounded time declarations). Unlike Rees, who primarily uses transparency to depict uncertain information (Rees, 2012), we need to investigate multiple visual metaphors that represent several dimensions of uncertainty in a clear way.
Visualizing distinct, overlapping geographic entities with different certainty values represents a major challenge for the project. Inspired by MacEachren’s overview of existing methods for the geospatial (MacEachren et al., 2005), we suggest testing pairwise mixtures of color hue, texture, saturation and transparency, as well as other features such as pop-up text, backgrounds and overlapping/non-overlapping shapes to encode lineage and accuracy uncertainties. Figure 2 demonstrates how we use different shapes to encode objects with distinct geospatial accuracies.
 
Figure 2:
A visualization of 636 points from the Franco-Italian dataset, with non-overlapping shapes denoting different toponym types.
Although some work addresses the problem of temporal uncertainty for small datasets (Zuk et al., 2005), sufficient research on large-scale temporal uncertainty is not available. Expecting thousands of overlapping temporal values of variable granularity on the timeline, we need to create novel visualization approaches. Figure 3 illustrates one solution for dealing with aggregated temporal uncertainty where increased saturation designates a higher degree of certainty. In our short presentation, we will demonstrate the tool and some strategies for simultaneous visualization of various aspects of the data.
 
Figure 3:
: A timeline represented as a stacked graph with multiple aggregated uncertain temporal values (increased saturation designates increased certainty in lineage value). NB: The time line below does not represent the real temporal data of this project.
The project hopes to bring attention back to hundreds of unread works of the period (Moretti, 2005), perhaps even spawning new close readings of them based on their “interspatiality” — the common spaces that texts reference — but also to encourage students and scholars to experiment with visualizing spatial clusters and patterns
References
Brun, L. (2012). Archives de littérature du moyen âge. University of Ottawa http://www.arlima.net (accessed 28 October 2012).
Flutre, L.-F. (1962). Table des noms propres avec toutes leurs variantes figurant dans les romans du moyen âge écrits en français ou en provençal et actuellement publiés ou analysés.
Gregory, I. (2012). Spatial Humanities: Texts, Geographic Information Systems and Places.http://www.lancs.ac.uk/spatialhum/ (accessed 15 February 2013)
Griethe, H., H. Schumann (2006). The Visualization of Uncertain Data: Methods and Problems. In Proceedings of SimVis'06, 143-156.
Harris, R. L. (1999). Information Graphics: A Comprehensive Illustrated Reference.
Hurni, L., B. Piatti, et al. (2013). A Literary Atlas of Europe — Ein Literarischer Atlas Europas.http://www.literaturatlas.eu/ [accessed 1 March 2013].
Jänicke, S. (2012) GeoTemCo: Comparative Visualization of Geospatial-Temporal Data http://www.informatik.uni-leipzig.de/geotemco (accessed 29 October 2012)
MacEachren, A., A. Robinson, S. Hopper, R.M. Gardner, M. Gahegan, and E. Hetzler (2005). Visualizing Geospatial Information Uncertainty: What We Know and What We Need to Know. In Cartography and Geographic Information Science 32.3 139-160.
Moretti, F. (2005). Graphs Maps Trees: Alternative Models for a Literary History.
Rees, G. P. (2012). Uncertain Date, Uncertain Place: Interpreting the History of Jewish Communities in the Byzantine Empire using GIS. Abstract DH2012, Hamburg. http://www.dh2012.uni-hamburg.de/conference/programme/abstracts/uncertain-date-uncertain-place-interpreting-the-history-of-jewish-communities-in-the-byzantine-empire-using-gis/ (accessed 14 March 2013).
Zuk, T., S. Carpendale, and W. Glanzman (2005). Visualizing Temporal Uncertainty in 3D Virtual Reconstructions. In Proceedings of the 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage (VAST'05). 99-106.",txt,This text is republished here with permission from the original rights holder.,,geocritique;uncertainty,English,"french studies;interdisciplinary collaboration;medieval studies;spatio-temporal modeling, analysis and visualisation;visualization"
1824,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,A Network Analysis Approach of the Venetian Incanto System,,Yannick Rochat;Mélanie Fournier;Andrea Mazzei;Frédéric Kaplan,"paper, specified ""long paper""","The Venetian maritime empire is the subject of numerous works and monographs (e.g. Ercole 20061, Lane 19732, Luzzatto 19413). This paper focuses on the period between the end of the 13th century and the fall of Constantinople in 1453. During that period the Venetian state set up seven regular shipping lanes, linking the Republic of Venice with the oriental and the occidental Mediterranean basins,  the Black Sea, England and Flanders. Special warships—called galleys—were readapted to perform commercial duties during peacetime on these shipping lanes. Every year, the Venetian Republic organized an auction system—the Incanto—to assign the commercial space on these ships. Subsequently the Senate was in charge of determining the mandatory stopovers, duration of the call, date of departure and date of return to Venice. All of this precise information was recorded in the Venetian official administrative documents.
Several authors have tried to reconstruct the Incanto system from the highly detailed information contained in these administrative documents. In 1961, Tenenti and Vivanti produced a series of chronological maps showing the evolution of the lanes year by year. Unfortunately, their model of the archives is not available for further investigation. More recently, Doris Stöckly extracted from the Venetian state archives—and other sources—a detailed list of all the information related to the ships on a year by year basis. She published her analysis in a monography (Stöckly 19954). The compiled tables appear as appendices to her Ph.D thesis; and are only available in printed form (see figure 1).
For this work, we take these printed tables, digitize, automatically transcribe and structure them. We perform new analyses of the structure and evolution of the Incanto system. Our ambition is to go beyond the textual narrative or even cartographic representation to perform a network analysis which potentially offers a new perspective on this maritime system.
Method

Step 1 : From Printed Tables to Structured Data

The first step of our project was the transformation of the appendices into structured data ready for analysis. We scanned these documents and processed each page using a specifically designed pre-processing pipeline, aimed at improving the quality and highlighting the structure of the scanned images. The pre-processing step included several computer vision-based procedures, serving two main purposes: the adjustment of moderate rotations introduced by the scanning process and the removal of noisy components that may disturb the recognition process. To explicit the structure of the table, we elaborated a method based on horizontal and vertical projection profile that automatically fit rows and columns of the document table. This grid was then used in conjunction with Optical Character Recognition Software (ABBYY Fine reader). We extracted 1480 lines of data. Each line matches a galley and includes the following information: name of the line, year, number of ships, stopovers, and optionally duration of stay.

Fig. 1: Excerpt of the extracted data from Doris Stöckly Ph. D thesis appendix.
 
Step 2 : From Structured Data to Networks

We transformed the resulting table into a network. First, we applied a set of rules in order to clean the data. Then, we removed the stops marked as “facultative”. The stops mentioned without any temporal detail were considered as equal to one day—the shortest unit of time. Names of places and geolocations were standardised using a spatial database of Ancient Ports and Harbours based on Harvard’s DARMC5 and the Pleiades data6. We grouped the stopovers under two generic labels for Crete and for Cyprus.

Fig. 2: The 170 years of the Incanto system visualised as a network.
We decomposed—using an R script—the structured table into individual segments made of paired consecutive stopovers. By connecting these directed segments, we created a global directed network encoding 170 years of navigation (see figure 2). The vertices of this network represent all the ports and places mentioned for this period. The size of the nodes is proportional to the sum of in- and out-degree measures of the node. The arcs represent maritime traffic. Two attributes are associated to each arc: one for the year of the trip and another one reporting the number of ships in each convoy.
From the global network, we produced separated subnetworks corresponding to each year of navigation. These subnetworks inherit their attributes from the main network: the number of ships and days. In figure 3, we illustrate evolution and dynamics of the Venetian maritime routes for the three years before and the three years after the Chioggia war (1351-1354) between Venice and Genoa.

Fig. 3: Network visualization of six years of maritime routes before and after Chioggia war (1377-1381)
Network Analysis: Crete vs. Cyprus

We focused our investigation on two particular islands located in the oriental basin of the Mediterranean Sea: Crete and Cyprus. After its acquisition by the Venetian empire and for 460 years, Crete was a fundamental naval base in terms of localisation, logistics and safety (Dudan 2006, Major 1989). Cyprus had a similar strategic position; it was an intermediary stop and became part of the Venetian empire in 1489.
Based on the network extracted from the Incanto dataset, we computed a measure of commercial betweenness of the islands of Crete and Cyprus. In figure 4, we show its time evolution in the period comprised between 1283 and 1453. We highlight three patterns emerging from the computation of this measure and interpret them using three events in the maritime history of Crete and Cyprus.
The first time histogram contains a blue box encapsulating that measure on Crete between 1344 and 1377. During that period, the maritime traffic density increased because of the reopening of the Alexandria lane, as Crete was the last stopover for all the convoys heading to Egypt. It is interesting to compare this change with the increase of commercial betweenness, as highlighted in the figure 4.
In the second time histogram, two red boxes highlight two historical events related to Cyprus maritime traffic. The first one reflects the betweenness of Cyprus as an important stopover on the way to Armenia (1283 - 1338) (Balard 1987). During this period the measure of betweenness naturally skyrockets, as the island had acquired a strategic position as a maritime hub. On the contrary, the second box shows very low measures of betweenness; corresponding to moderate maritime traffic. This was due to the fact that the Senate of Venice reorganised the commercial exchanges by opening a new lane towards Beirut. During this period (1375 - 1444), Cyprus lost its strategic position for maritime activity directed towards Syria and Egypt.
One can notice that the re-opening of Alexandria as destination for Venetian navigation (1344) had the opposite impact on the maritime traffic passing through Cyprus and Crete.

Fig. 4: Betweenness of Crete and Cyprus with respect to the maritime traffic (1283 - 1453)
Conclusions and Future Work

It sounds like a commonplace to describe the Mediterranean Sea, geographically and historically, as an area of intense exchanges and communications; however the fact is that any visualisations up to this point, when they exist, never went beyond the narration and failed to give a concrete idea of the pace imposed by Venetian navigation over a period of 170 years.
With this work, we go beyond that common way of visualising maritime historical data. First, we have designed processing procedures to automatically digitise data present only on paper documents. Second, based on this digitised data, we modelled the Venetian maritime connections over 170 years as a network. Third, we magnified the network over Cyprus and Crete and extracted a measure of betweenness for these two islands.
From a qualitative analysis point of view, we showed the consequences of three historical events with respect to the Incanto system. We are confident that we can apply this methodology to better explain historical events and quantify their influence on the global maritime network.
References

1. Ercole, G. (2006). Duri i banchi!: le navi della Serenissima, 421-1797. Gruppo Modellistico Trentino di Studio e Ricerca Storica.
2. Lane, F.C. (1973). Venice, A Maritime Republic. ACLS Humanities E-Book. Johns Hopkins University Press.
3. Luzzatto, G., Padovan G. (1941). Navigazione Di Linea e Navigazione Libera, Nelle Grandi Città Marinare Del Medioevo. Popoli 1: 389–391.
4. Stöckly, D. (1995). Le système de l’Incanto des galées du marché à Venise: fin XIIIe - milieu XVe siècle. BRILL.
5. Digital Atlas of Roman and Medieval Civilization. darmc.harvard.edu/icb/icb.do (accessed on 30.10.2013)
6. Bagnall, R., Talbert R. J. A., Elliott T., Twele R., Becker. J., Gillies S., Horne R., McCormick M., Rabinowitz A., and Turner B.. (2006). Pleiades: A Community-built Gazetteer and Graph of Ancient Places. Collection. pleiades.stoa.org
7. Dudan, B., (1938), Il Dominio veneziano di Levante. Nicola Zanichelli.
8. Major A. (1989), Les colonies continentales de Venise en Grece meridionale, XIV-XV siecle, Doctorat Nouveau Régime, Histoire, A.N.R.T..
9. Balard, Michel. (1987), Les Vénitiens En Chypre Dans Les Années 1300. Byzantinische Forschungen 12: 580–606.",txt,This text is republished here with permission from the original rights holder.,,maritime routes;networks;ocr;venice,English,"content analysis;data mining / text mining;digitisation, resource creation, and discovery;historical studies;information retrieval;medieval studies;networks, relationships, graphs"
1858,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Book History and Software Tools: Examining Typefaces for OCR Training in eMOP,,Todd Samuelson;Matthew J Christy;Katayoun Torabi;Bryan Tarpley;Elizabeth Grumbach,"paper, specified ""short paper""","In 1936, the notable English bibliographer A. W. Pollard admitted in his Preface to Frank Isaac’s English Printers’ Types of the Sixteenth Century that “[he] had a very poor eye for distinguishing types and a very poor head for remembering them.”1 Pollard is hardly alone among experts in the history of printing in this shortcoming.  Even among scholars with decades of experience in scrutinizing features of the printed book, the ability to distinguish and identify typefaces is a notorious challenge.  The literature about early type designs and designers (known as punchcutters) is partial and contradictory; the variations in typefaces are subtle and, at times, inconclusive; and the ability to make differentiations has been considered less a matter of regimented principle than of elusive skill.  As Harry Carter suggested, “it is evident that in considering the face of a fount of type we are in a world of art, . . . not a mechanical proceeding or anything susceptible of scientific treatment.""2
However, it is precisely the consideration of “founts of type” that is currently engaging a majority of the Early Modern OCR Project (eMOP) team. eMOP, a 2-year Mellon Foundation-funded grant project underway at the Initiative for Digital Humanities, Media, and Culture (IDHMC) at Texas A&M University, aims to OCR the documents that comprise the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) collections. As a project that involves collecting and aggregating huge amounts of data, OCR’ing 45 million page images on a high-performance computing cluster, and the development of several software tools and services, eMOP is technology-laden. But at its heart eMOP is a Humanities project, conceived  by Humanists, driven by the needs of Humanities scholars, and supported throughout by book history and an understanding of the development of print type in the 15th-18th Centuries.
For eMOP’s book historian, Dr. Todd Samuelson, one of the difficulties in conceptualizing the scope of eMOP has centered in a potential conflict between DH methodology (as encompassed by “big data”) and the traditional means of approaching type identification: as Carter noted, it is an art steeped in years of hard-won practice rather than a science with predictable and reproducible models. While DH is focused on humanities questions and methodologies, it does employ scientific principles as well, especially when dealing with a very large set of documents, and conflict can arise by trying to synthesize a skill set based on minutiae with an extremely large data set. By contrast, even when big data projects incorporate crowdsourcing and the oversight of human experts, they require the ability to find readily transferrable commonalities, rather than to establish proficiency in a small number of experts. In the course of the eMOP project, we have found that the development or adoption of specific software tools has helped to ameliorate this conflict and incorporate type history scholarship into the training of OCR engines.
One of the ideas driving eMOP work is that, by training OCR engines to recognize specific early modern fonts, we can increase the accuracy of those engines when used to OCR documents printed in those fonts. To accomplish this, the eMOP team has spent most of the last year investigating font history, creating a database of early modern printers and the fonts they used, and developing and testing tools and techniques to train Tesseract (an open-source OCR engine) to recognize these fonts. The ability to distinguish between different, but sometimes closely related, fonts, and to train Tesseract to recognize these distinctions has been a central focus. For example, the general classification of different families of typefaces has been attempted by book historians, including Adrian Weiss, who categorized unknown English typefaces of a certain period as either “S-face” or “Y-face”.3 So, though the source of the typeface may not be ascertainable, certain characteristics can be defined which allow scholars (and potentially OCR engines) to identify and group the typefaces more accurately.
As has already been noted, identifying examples of S- and Y-face characters and distinguishing between them, especially when both can be present in one document, is a difficult enough task for an expert. Trying to find all instances of the lower-case letter ‘w’ in a document, as an example, and then deciding which exemplars match some specified “ideal” is difficult and time consuming. Fortunately, eMOP has software tools that can drastically simplify this task, and even allow non-experts to do some of the work. Those tools were originally developed to create training for Tesseract to recognize early modern typefaces, but can also be applied to support research into the typefaces themselves.
To create specific font training for the Tesseract OCR engine, a team of undergraduate student workers, lead by IDHMC graduate student Katayoun Torabi, first process the available page images using Aletheia Desktop. (Aletheia was developed by the Pattern Recognition and Image Analysis (PRImA) Research Laboratory at the University of Salford. Apostolos Antonacopoulos, IMPACT Work Package leader for PRImA, University of Salford, has made Aletheia and other tools available at http://www.primaresearch.org/tools.php.) Aletheia Desktop includes several semi-automated tools that identify and define layout regions, lines, words, and individual characters (glyphs) within documents. Aletheia reads the text in the page image (using Tesseract) and assigns a Unicode value for each letter, number, and punctuation mark.

Fig. 1: Aletheia Desktop with identified glyphs and some of their associated Unicode values.
As output, Aletheia creates an XML file that contains a set of XY coordinates, along with the associated Unicode value, for each identified glyph. The data contained in this XML file is then ingested or imported into a tool created by IDHMC graduate student Bryan Tarpley called Franken+. Franken+ uses a MySQL database to associate each glyph image with its corresponding Unicode character. The user can then select any glyph from a drop down menu to see every instance of that character in a window (Fig. 2). With every instance of a particular glyph (for example all the ‘a’s) from a document available in one window, the user can quickly identify mislabeled glyphs and choose the best exemplar (or exemplars) for each glyph in that font set (Fig. 2). Once the user has isolated the best instance(s) of each character, Franken+ uses a standard text document to produce a set of synthetic TIFF images and XML files, producing a “Franken-text” with only these ideal characters. This Franken-text matches the characteristics of Tesseract’s expected training file and so can be used to train Tesseract to recognize the typeface being processed.

Fig. 2: Some images of the Frank+ user interface.
eMOP’s book history team immediately realized that the capabilities of Aletheia and Franken+ would tremendously benefit their research into the S-face vs Y-face font question. The ability of Franken+ to display all instances of a given letter from a set of page images in one window dramatically simplifies the task of identifying all examples of any letter in a set of pages. And, being able to examine all these examples alongside each other makes comparing similarities or differences much easier and faster (Fig. 3). After a quick installation of Franken+ and less than an hour of training, the book history team was able to commence work on their research question in earnest. Since Franken+ was introduced at the 2013 Doc Eng Conference,4 the eMOP team has been contacted by several international scholars interested in learning more about Franken+ for use in their research on typefaces.

Fig. 3: An image from Franken+ of a set of exemplars of the “a” glyph from one document.
The study of early modern fonts is a road less traveled in the landscape of Humanities research. Based as it is on minutiae and requiring incredible attention to detail, this work traditionally has been left to a handful of individual scholars. However, the development of Franken+ for eMOP, when used in conjunction with Aletheia, promises to open up this field of study to scholars who may have been interested in it, but found the challenges too daunting. This paper will describe aspects of the eMOP work being done in the field of early modern type research, and will introduce Franken+ as a valuable new tool in this research. The creation of tools like Franken+ have the potential to increase attention and alter research methodologies for this field.
References

1. Pollard, A. W. (1936) Preface. Isaac, Frank. English Printers’ Types of the Sixteenth Century. Oxford: Oxford UP. (v-vii).
2. Carter, Harry Graham (1969). A View of Early Typography Up to About 1600. Oxford: Clarendon.
3. Weiss, Adrian. (1990) Font Analysis as a Bibliographical Method: the Elizabethan Play-Quarto Printers and Compositors. Studies in Bibliography 43: 95-164.
4. Torabi, Katayoun, Jessica Durgan, and Bryan Tarpley (2013). Early Modern OCR Project (eMOP) at Texas A&M University: Using Aletheia to Train Tesseract. ACM Press. 23. doi:10.1145/2494266.2494304. Web. 31 Oct. 2013.",txt,This text is republished here with permission from the original rights holder.,,history;ocr;software;tools;typeface,English,digitisation - theory and practice;historical studies;other;software design and development
1901,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Diagnosing Page Image Problems with Post-OCR Triage for eMOP,,Matthew Christy;Loretta Auvil;Ricardo Gutierrez-Osuna;Boris Capitanu;Anshul Gupta;Elizabeth Grumbach,"paper, specified ""short paper""","The Early Modern OCR Project (eMOP), currently underway at the Initiative for Digital Humanities, Media, and Culture (IDHMC) at Texas A&M University, is a Mellon Foundation-funded endeavor tasked with improving, or creating, OCR (optical character recognition) for the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) collections. The basic premise of eMOP is to 1) use book history to identify the fonts represented in the collections and the printers that used them; 2) train open source OCR engines on those fonts; and 3) OCR documents using an engine trained on the font specific to each documents. In addition, as a Mellon Fountation-funded project eMOP is tasked with using open-source solutions and producing open-source tools, workflows, and processes that are reproducible and which can be implemented by other scholars in their own digitization projects. One of eMOP’s end products will be an open-source workflow of our entire process using Taverna.
As eMOP enters its second year, intensive work on developing and testing training for the Tesseract OCR engine has demonstrated a failing in the three-fold basic premise. Many of the page images which we are trying to OCR are of such poor quality that no amount of training will produce OCR results that meet the standards we have set for the grant outcome.1 These images are already binarized, low-quality, low-resolution, digitized images of microfilm, converted from photographs—4 decades and 3 media generations removed from the originals.Typical problems include noisiness, bleedthrough, skewing, and warping, but there are many more. There already exist many algorithms that can fix most of the problems extant in our collection of page images.23 Applied during a pre-processing stage, these algorithms have the potential to improve page image quality to the point that they can yield excellent OCR results. But with approximately 45 million pages in eMOP’s data set, determining which pages need which kind of pre-processing proved problematic at best. 

Fig. 1: A sample of part of a page image from the eMOP collection showing skew, noise, bleedthrough, over-inking, and an image.
To this end, the eMOP management team, along with our collaborators, Loretta Auvil and Boris Capitanu at SEASR (Software Environment for the Advancement of Scholarly Research, at the University of Illinois, Urbana-Champaign), and Dr. Ricardo Gutierrez-Osuna and graduate student Anshul Gupta of Texas A&M University, decided to focus our proposed post-processing triage workflow on the problems that exist in our page image inputs. Originally stated, our triage process would examine OCR results and decide whether the documents would be routed to different tools being built for eMOP to perform automatic word correction, crowd-sourced line segmentation correction, by-hand font identification, or automated re-OCRing with different font training. However, the presence of so many low quality page images in our input required a more robust system for handling the output. What we needed was a triage process that would allow us to programmatically diagnose our input documents based on the output of our OCR system.
The open-source Tesseract OCR engine is capable of producing both plain text files and files in an XML-like format called hOCR. hOCR files contain wrappers around each found word, line, paragraph, and region, and these wrappers contain bounding box coordinates for each entity (Fig. 2). A close examination of the text and hOCR results for nearly 600 poor quality page images revealed certain patterns, or ‘cues’, which could be used, singly or in combination, to uniquely predict individual problems that exist in the original page images.

Fig. 2: Bounding boxes for lines (red) and words (blue) drawn on a page images based on hOCR output.
For example, documents printed in a blackletter or gothic font, but OCR’d with Tesseract trained for a roman font produce a text file with a character frequency distribution different from that expected of English language documents. Basically, if Tesseract is trained with a roman font, characters printed in a blackletter font look predominantly like m, n, u, and l. Similarly, documents containing a lot of noise (e.g. numerous spots and blotches on the page) typically produce “words” found in areas of the page outside of the main text area, have word bounding boxes of widely varying heights, and have line bounding boxes that overlap. Page images that exhibit heavy skewing (the text lines are tilted at an angle from the horizontal) also pose problems for Tesseract, as it will often begin reading one line and then at some point jump to the line above or below (depending on the direction of the skew) to finish reading the ""line."" In these cases the hOCR again contains overlapping line bounding boxes, but also has word bounding boxes that don’t have contiguous coordinates, i.e. it is finding words out of the reading order as they appear on the page to a human reader. These are just a few examples that demonstrate the problems we’ve encountered and the cues we’ve discovered to identify them, which we will identify in this paper.
Cues like these and others have provided us with the mechanism we were looking for to identify page image problems based on OCR output. In order to take full advantage of this information however, we are also developing a full post-processing workflow. Beginning with OCR results, the output of this workflow will be either 95%, or better, corrected text or a per-page indicator describing what kind of pre-processing should be performed before each page is re-OCR’d.
We are also working with our collaborators on developing a mechanism to assess the quality of our OCR output. We have combined different analysis techniques developed by collaborators at SEASR and Texas A&M University, to examine text data (examining character unigram frequency distributions and word lengths), page data, (determining the main text area of the page and looking for outliers), and hOCR bounding boxes (calculating box heights and widths). Applying these mechanisms to the results of each page will yield a score that constitutes a prediction of how the document would compare to a ground-truth transcription. Test results show a strong correlation between these predicted scores and actual scores produced on documents that do have ground-truth available.
Page results receiving a high enough score can then be sent for further text analysis, including dictionary look-ups, to correct as much of the OCR output as possible. Those pages that receive scores below the threshold undergo an iterative process of looking for different cues in order to identify the likely reason the OCR process failed for each page.

Fig. 3: Proposed eMOP post-processing workflow.
Much work has already been done with regard to OCR post-processing, but it has concentrated on questions of identifying and correcting bad OCR.45 In this paper we will report on the development of an OCR post-processing workflow that can evaluate and identify a broad range of defects common to page images of early modern printed documents. The result of this workflow can then be funneled into a pre-processing and re-OCR’ing process later. We plan, by grant-end, to release an open-source workflow and code that can be used by other groups or individuals engaging in large-scale OCR projects. Given the inherent problems that these documents pose for OCR engines, we view this kind of analysis as a vital step forward in the comprehensive understanding and digitization of large collections of early modern printed documents.
References

1. Singh, Chandan, Nitin Bhatia, and Amandeep Kaur. Hough Transform Based Fast Skew Detection and Accurate Skew Correction Methods. Pattern Recognition 41.12 (2008): 3528–3546. CrossRef. Web. 30 Oct. 2013.
2. Subramaniam, L. Venkata et al. A Survey of Types of Text Noise and Techniques to Handle Noisy Text. ACM Press, 2009. 115. doi:10.1016/j.patcog.2008.06.002. Web. 30 Oct. 2013.
3. Taghva, Kazem, Thomas Nartker, and Julie Borsack. Information Access in the Presence of OCR Errors. ACM Press, 2004. 1–8. doi:10.1145/1031442.1031443. Web. 30 Oct. 2013.
4. Wudtke, Richard, Christoph Ringlstetter, and Klaus U. Schulz. Recognizing Garbage in OCR Output on Historical Documents. ACM Press, 2011. 1. doi:10.1145/2034617.2034626. Web. 30 Oct. 2013.
5. Borovikov, Eugene, Ilya Zavorin, and Mark Turner. A Filter Based post-OCR Accuracy Boost System. ACM Press, 2004. 23–28. doi:10.1145/1031442.1031446. Web. 30 Oct. 2013.",txt,This text is republished here with permission from the original rights holder.,,diagnosis;ocr;page images;post-processing;pre-processing,English,"digitisation, resource creation, and discovery;digitisation - theory and practice;english studies;image processing;programming;software design and development;text analysis"
1918,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Digitization of Hmong Sacred Texts,,Mitchell Paul Ogden,poster / demo / art installation,"A remote religious community, called Ee Nbee Mee Noo (Ib Npis Mis Nus), in northern Thailand has recently shared their extensive collection of hand-written religious manuscripts with researchers following extensive collaboration and community work. Interested in making these foundational sacred texts more accessible across the global Hmong diaspora, the community leaders sought the cooperation of scholars to digitize the texts in preparation for eventual publication. Rich in historical and religious content, these nine 200-page volumes are written in the community’s own sacred and obscure Puaj Txwm alphabet of the Hmong language.

The scope of our project includes the development of a set of digital tools that will perform OCR (built on the Tesseract engine), error-check based on phonological rules of the Hmong language, and encode the text (according to TEI guidelines) to make the text searchable and indexable for future distribution as electronic texts. In addition, the project will create a transliteration tool that will enable Hmong texts to be transliterated across the dozen or more orthographies used throughout the diaspora, thus facilitating the exchange and investigation of Hmong language texts across diasporic histories and geographies.

This poster will feature the project’s tools and process, emphasizing the solutions to obstacles including a hand-written manuscript and the complicated politics of ethnic minority religious movements in Thailand. The poster will provide context for these sacred texts and their obscure alphabet, overview the digitization process, and provide examples of the work and the innovative solutions developed by our research team.",txt,This text is republished here with permission from the original rights holder.,,digitization;hmong;ocr;sacred;transliteration,English,"asian studies;digitisation, resource creation, and discovery;translation studies"
1924,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,"Distributed ""Forms of Attention"": eMOP and the Cobre Tool",,Anton Raymund duPlessis;Laura Mandell;James Creel;Alexey Maslov,"paper, specified ""long paper""","A recent article by Paul Gooding, Melissa Terras, and Claire Warwick argues that a gap in our knowledge about the impact upon scholars of “large-scale digitized collections” of textual data has spawned “myths” about mass-digitization—those surrounding the distant- vs. close-reading debate, as well as dystopian arguments about digitally disrupted attention spans (Gooding et. al.; Moretti; Trumpener; Guillory; Hayles). “Where understanding lags behind innovation,” Gooding, Terras, and Warwick argue persuasively, “the rhetoric of technological determinism can fill the void” (633). Central to the myth that digital media produce “crowds of quick and sloppy readers” (632) is ignorance: the last decade has witnessed the emergence of a spate of digital editing and annotating tools as well as the emergence of what might be called “network editing.”[2] As has been amply revealed by the Australian Newspaper Digitisation Program, as well as other projects involving the crowd in correcting textual transcriptions such as Transcribe Bentham, people are as much engaged by the task of modifying digital textual archives as they are in using them (Halley, Terras), and textual “modding”[3] by networks of users requires paying close attention to texts, as might networked monasteries of monks in the process of transcribing them. The need for distributed networks of people helping to solve problems endemic to creating large textual corpora, in other words, fosters close attention to text. This fact is demonstrated by the Early Modern OCR Project (eMOP)[4]: the attempt to produce searchable text for 45 million page-images of texts published between 1473 and 1800 would fail were it not for the project’s adaptation of Cobre, a tool originally designed for closely examining various 16th Century Iberoamerican imprints, and Cobre elicits careful scholarly attention from globally distributed experts and citizen scholars wishing to take part in improving the quality and kind of information available on the Internet.

The eMOP project focuses on how to digitize the archive of early modern texts, despite problems entailed. The printing process in the hand-press period (1473-1800), while systematized to a certain extent, nonetheless produced texts with fluctuating baselines, mixed fonts, and varied concentrations of ink (among many other variables). Adding to these factors, the quality of the digital images of these books is very poor: ProQuest’s Early English Books Online dataset (EEBO) and Gale’s Eighteenth-Century Collections Online (ECCO) contain page images that were digitized in the 1990s from microfilm created in the 1970s and 80s. Hand-press printing as well as skewed low-quality images with no gray-scale originals creates a problem for Optical Character Recognition (OCR) software. OCR engines are notoriously bad at translating into texts digital images of early modern texts even under the best of circumstances (Gooding, Terras, and Warwick). That is trying to translate the images of these pages into archiveable, mineable texts.

The Early English Books Online dataset (EEBO) consists of a collection of approximately 15 million digital page images of texts published between 1473 and 1700, and these page images are practically impenetrable to OCR engines. Moreover, metadata for such early texts is notoriously unreliable: according to David Foxon, title pages don't only lie, they sometimes joke, naming the printer typically used by a rival author as a way of implicating that author in the text's composition, or naming a bookseller in an area of London such as the theater district, for satirical purposes. Not only the binding of books, but the re-use of previously printed materials in ""new"" books makes it very difficult to know what is actually proffered by any title--what editions of other works might be included, unacknowledged in the metadata. A consortium of libraries called the Text Creation Partnership (TCP) has decided to key in, type by hand, one instance of each title in the collection, but obviously, in this context, “the same title” rarely indicates what “buried treasures” lie beneath its mark (Jackson). Moreover, it is even the case that individual witnesses of the same edition vary because of stop-press additions and corrections, changes made during the run of a single printing of one edition.

Gibbs muses that “even once we have more reliable OCR technology, it would be nice to have an infrastructure to allow the manuscripts to be viewed together and improved by user expertise” and expresses hope for a transcription editor with an unobtrusive, functional, and intuitive interface, that allows text to be easily (re)configured while displaying variations between versions. Cobre (COmparative Book REader), a suite of image viewers and tools developed to facilitate detailed interaction with the collection of 16th Century New World imprints in the Proyecto losPrimeros Libros de las Américas: Impresos mexicanos y peruanos del Siglo XVI en la bibliotecas del mundo meets those needs.[5] Cobre ingests content from an OAI/PMH enabled digital repository. To populate a Cobre instance with texts for eMOP triage, we first structure the page images and their associated OCR transcriptions in the DSpace Simple Archive Format for ingestion into a DSpace repository, from which they can be imported into Cobre. Intrinsic to Cobre’s functionality is a Detailed View that not only places page images in context, via a filmstrip metaphor, but provides multiple zoom levels and the ability to drag the page in the viewer pane (Liles et al). Cobre’s Comparison View likewise uses a filmstrip view of two or more books together (Liles et al). These filmstrips can be locked, keeping them aligned when any one filmstrip is moved and when a thumbnail in the filmstrip is clicked, a side-by-side view of all the pages appears (Liles et al) in a Quick Comparative View.


Fig. 1: Cobre's Detailed View with imported OCR and pane for text correction


Fig. 2: Cobre's Comparative View of multiples exemplars


Fig. 3: Cobre's Quick Comparative View of page images and OCR output side-by-side.

Though the Cobre tool was built for the purposes of transcription and thus is technically a crowd-sourced transcription tool like the Bentham wiki and the other tools listed by Melissa Terras,[6] it resembles Ben Brumefield’s FromThePage, also mentioned by Terras, in being half transcription tool and half social editor of the sort described by Ray Siemens et. al. Like Bentham, Siemens’s own Devonshire ms. uses Wikimedia so that editing can be discussed as well as implemented. Cobre too allows for transcription, annotation, and—a key attraction for experts—editing and adding information to page images and to the metadata.

We performed user studies on the tool, bringing in book history experts James Raven and Robert D. Hume to test the tool, and we videotaped their eye movements while recording their comments. There are many things that they did not find intuitive which we fixed on our last development sprint. We will be performing another set of user studies when we teach approximately 50 people to use Cobre at a pre-conference workshop at the American Society for Eighteenth-Century Studies, to be held in Williamsburg, VA, 19 March 2014.

Transcribe Bentham and the ANDP have been very successful at recruiting experts and citizen scholars (Causer, et. al., and Holley). While the ANDP required a very light form of attention, transcription, and Transcribe Bentham slightly more – users were asked to encode as well – we will be asking users of Cobre to transcribe and compare transcriptions to multiple pages of various editions. Can a network of “authorized” users who will be carefully comparing pages of editions be generated of sufficient strength? Can there be networked—and so massive—close-reading? We will report on whether and how it is possible to generate distributed forms of attention that are required for careful digitization en masse.

[1] Kermode’s phrase encapsulates the scholarly output over the last two centuries that have produced close readings of the meanings and forms of canonical works of art and literature

[2] Siemens et. al. describe “the social edition,” a procedure that was debated at a recent SSHRC-sponsored conference called “Social, Digital, Scholarly Editing,” hosted by Peter Robinson at the University of Saskatchewan (ocs.usask.ca/conf/index.php/sdse/sdse13). Gibbs calls for a “community transcription tool [that] will reduce significantly the barrier to entry and encourage mark-up of texts,” such as: the CWRC (www.dh2012.uni-hamburg.de/conference/programme/abstracts/cwrc-writer-an-in-browser-xml-editor), FromThePage (beta.fromthepage.com) and “Textual Communities” (www.textualcommunities.usask.ca)

[3] Craig Chappel’s view that game-modding is in decline may portend a trend against participation, but that view is arguable.

[4] emop.tamu.edu

[5] primeroslibros.org, libros.library.tamu.edu

[6] melissaterras.blogspot.com/2010/03/crowdsourcing-manuscript-material.html: Scratch, Remote Writer, and the tools hosted by the Australian National Digitisation Program (ANDP) and BYU Historica Journals.

References
Causer, T., J. Tonra, and V. Wallace (2012). Transcription Maximized; Expense Minimized?: Crowdsourcing and editing The Collected Works of Jeremy Bentham. Literary and Linguistic Computing 27.2, pp. 119-137.

Causer, T., and V. Wallace (2012). Building a Volunteer Community: Results and Findings from Transcribe Bentham. Digital Humanities Quarterly 6.1. www.digitalhumanities.org/dhq/vol/6/2/000125/000125.html. Accessed 8 January 2014

Chapple, Craig (2013). An FPS Insurgency – Breaking into a Crowded Genre. 13 September 2013. www.develop-online.net/interview/an-fps-insurgency-breaking-into-a-crowded-genre/0117719 Accessed 31 October 2013.

Dahlström, Mats. ( 2009). The Compleat Edition. In Text Editing, Print and the Digital World. Eds. Marilyn and Kathryn Sutherland. Farnham, MA: Ashgate. 27-44.

Foxon, David, with James McLaverty. (1991). Pope and the Early Eighteenth-Century Book Trade. Oxford: Clarendon Press.

Gibbs, Frederick W (2011). New Textual Traditions from Community Transcription. Digital Medievalist 7. www.digitalmedievalist.org/journal/7/gibbs

Gooding, Paul, and Melissa Terras, Claire Warwick (2013). The Myth of the New: Mass Digitization, Distant Reading, and the Future of the Book. Literary and Linguistic Computing 28.4: 629-39.

Guillory, John (2010). Close Reading: Prologue and Epilogue, ADE Bulletin 149: 8-14.

Hayles, N. Katherine (2007). Hyper and Deep Attention: The Generational Divide in Cognitive Models, Profession 2007: 187-199.

Holley, Rose. (2009) How Good Can It Get: Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs. D-Lib Magazine 1.3/4.

---. Many Hands Make Light Work. March 2009. National Library of Australia. ISBN 978-0-642-27694-0

Jackson, Millie. (2008) Using Metadata to Discover the Buried Treasure in Google Book Search. Journal of Library Administration 47.1/2: 165-73.

Kermode, Frank (1985). Forms of Attention. Chicago: University of Chicago Press.

Liles, B., Creel, J., Maslov, A., Nuernberg, S., duPlessis, A., Mercer, H., McFarland, M. and Leggett, J. (2012). Cobre: A Comparative Book Reader for Los Primeros Libros. Proceedings of the 45th Hawaii International Conference on System Sciences (HICSS45), Maui, HI, USA. January 4-7, pp. 1707-16. dx.doi.org/10.1109%2fHICSS.2012.155

Moretti, Franco.Conjectures on World Literature, New Left Review 1: 54-68. (2000)

---. Graphs, Maps, and Trees: Abstract Models for a Literary History. New York: Verso, 2005. 

---. More Conjectures, New Left Review 20 (2003): 73-81.

---. Style, Inc. Reflections ‘Relatively Blunt,’ 172-174.

Robinson, Peter.Textual Communities. www.textualcommunities.usask.ca

Siemens, Ray, and Meagan Timney, Cara Leitch, Corina Koolen, Alex Garnet.Toward Modeling the Social Edition: An Approach to Understanding the Electronic Scholarly Edition in the Context of New and Emerging Social Media. Literary and Linguistic Computing 27.4 (2012): 445-461.

Terras, Melissa. (2010) Crowdsourcing Manuscript Material. 2 March 2010. Adventures in Digital Humanities Blog. melissaterras.blogspot.com/2010/03/crowdsourcing-manuscript-material.html. Accessed 8 January 2014

---. Crowdsourcing or crowdsifting? Results and experiences from Transcribe Bentham. Paper given at Social, Digital, Scholarly Editing Conference, University of Saskatchewan, Saskatoon,  Saskatchewan, Canada. 11 July 2013.

Trumpener, Katie. (2009). Critical Response I: Paratext and Genre System, Critical Inquiry 3.1: 159–71.",txt,This text is republished here with permission from the original rights holder.,,comparing_editions;crowdsourcing;document_editing;metadata_correction;ocr,English,bibliographic methods / textual studies;crowdsourcing;digital humanities - nature and significance;encoding - theory and practice;renaissance studies;scholarly editing;text analysis
2017,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,"Navigating the Storm: eMOP, Big DH Projects, and Agile Steering Standards",,Elizabeth Grumbach;Matthew J Christy;Laura Mandell;Clemens Neudecker;Loretta Auvil;Todd Samuelson;Apostolos Antonacopoulos,"paper, specified ""long paper""","Introduction
In 2011, the Comite de Sages presented “The New Renaissance” to the European commision, stating that “digiti[z]ation is more than a technical option, it is a moral obligation” to the public. The report stresses that the initiative’s goal is to ensure that we “experience a digital Renaissance instead of entering into a digital dark age.” If the lack of adequate, searchable early-modern digital resources can be correctly referred to as a “digital dark age,” then we are undoubtedly seeing the emergence of a “digital renaissance.1” Projects like IMPACT (Improving Access to Text2), eMOP (Early Modern OCR Project), TCP (Text Creation Partnership), and others have emerged in recent years to take up the call to arms issued by the Comite de Sages. However, we need more than the digitization of cultural materials; we need responsible digitization alongside a community engaged in the fight for digital visibility of those materials. And, most importantly, large DH projects need effective and responsible management and collaboration standards. The aim is to adapt and adjust to the changing climate, ultimately steering the project safely into the harbor.

Overview
Many OCR (Optical Character Recognition) and cultural preservation projects are underway that need to be able to adapt their project plans to OCR technology and crowd-sourcing breakthroughs as they occur. In Fall 2012, the Initiative for Digital Humanities, Media, and Culture at Texas A&M University received a $734,000 grant from the Mellon foundation for the Early Modern OCR Project (eMOP)3. eMOP’s objective is to make machine readable, or improve the readability for, 45 million pages of text from two major proprietary databases: Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO). Generally, eMOP intends to improve the visibility of early modern texts by making their contents fully searchable. The current paradigm of searching special collections for early modern materials by either metadata alone or “dirty” OCR is inefficient for scholarly research (Mandell, 20134). We intend to publish an open source OCR workflow at grant end in Taverna. This workflow will contain access to an early modern font database, customization guidelines for the Tesseract OCR engine, post-processing and diagnostic algorithms, and crowdsourcing and “scholar-sourcing” (as Brian Geiger has dubbed) correction tools. But the overarching goal of eMOP, a project that blends book history5, digital humanities, textual analysis, and machine learning, is ultimately to foster a community of scholars and institutions interested in the digital preservation of, and access to, these texts. To this end, eMOP has assembled an international team of collaborators from multiple disciplines.

eMOP, however, has faced problems in the implementation of our goals and processes. During Year One, the eMOP team and collaborators quickly realized that the grant document excellently outlined milestones and goals, but it did not provide the level of granularity needed to complete each. We have also realized that progress is continually changing in this field, and if big DH projects do not adjust accordingly, they will end up reinventing the wheel. Active outreach and collaboration with institutions outside the initial grant collaborators proved important. In addition, eMOP is working with proprietary page images and metadata in order to release an open source tool, which has produced its own challenges. In order to succeed in producing a corpus of machine-readable texts and a workflow for future OCR projects, continual outreach and collaboration is needed, yet not always possible due to the restrictions of grant deadlines, funding, and other institutional roadblocks.

Getting Started
This panel considers how big DH projects, with big datasets, big networks of collaborators, and big goals, can adjust and adapt to change. It has long been noted that digital humanities projects lend themselves well to agile6 development models7, specifically the “the philosophy of ‘releasing early and often‘” (Scheinfeldt, 20108). However, these models often break down in the face of multi-institutional and international collaboration, software development, assembling large amounts of data, and what James Smithies and enterprise IT call “transition management,” or planning for “Change” (20119). A digital humanities project, large or small, also “seems to both depend upon collaboration and aim to support it” (Spiro, 200910). Each big DH project must find a practical balance in development management and collaboration methods.

This panel will bring together the eMOP management team at the IDHMC and collaborators from various disciplines and institutions to discuss the reasons why big DH projects need to plan for adaptation, ways in which projects can achieve this flexibility, and how to swiftly change directions.

If eMOP’s goal reflects of the goal of digital humanities at large, i.e. to foster collaboration among various disciplines and cultivate inter-institutional and international relationships that make possible new kinds of humanities research, then this panel provides a microcosm of that endeavor.

Panel Organization
This panel will consist of a brief 5 minute overview of the goals of, methodologies for, and collaborators in the Early Modern OCR Project, and then each speaker will introduce a major directional change or challenge that eMOP has faced, including the resulting solution in 7 minutes or less. Introductions to challenges may include comparisons to other large dh projects (e.g. IMPACT). Discussion of the resulting solution may include a short software/tool demo. The panel organizers will then pose questions to the roundtable to begin an open conversation, leaving the remaining time for discussion amongst panelists and the audience. Discussion will likely focus on how to change directions, rethink decisions, and reconfigure plans when collaborating with multiple institutions and individuals while facing grant deadlines and milestones.

Questions that Panel Organizers may pose:
Discuss future models of big DH project management, especially how essential multi-institution and international collaboration can be.
What can big DH projects learn from the agile vs. traditional software development models?
Discuss best practices in project management, and how they might be modified in order to take in recent technological innovations or respond to challenges.
We know that “failure” (Unsworth, 199711) is important: how can small failures be channeled into big success?
What kinds of cultural practices need to be taken into account when U.S. projects adopt European models, and vice versa?
How can transatlantic collaboration best be orchestrated so that projects benefits from collaborators’ advancements, both technological and social?
Participants
All panelists are committed and eagerly anticipating the discussion of eMOP, large DH projects, and successful and responsible collaboration and development management. 

Apostolos Antonacopoulos is the Director of the Pattern Recognition and Image Analysis (PRImA) research lab at the University of Salford, UK. Dr. Antonacopoulos has been working on issues of pattern recognition, image and document analysis, and historical document digital restoration for many years. In addition to eMOP, he has contributed to the IMPACT and Europeana Newspaper projects, and will discuss how the adoption and customization of software for large cultural preservation projects should be responsive to changing project needs.
Loretta Auvil works at the Illinois Informatics Institute (I3) at the University of Illinois at Urbana Champaign. She has worked with a diverse set of application drivers to integrate machine learning and information visualization techniques to solve the needs of research partners. Prior to working for I3, she spent many years at NCSA on machine learning and information visualization projects and several years creating tools for visualizing performance data of parallel computer programs at Rome Laboratory and Oak Ridge National Laboratory. She will be discussing big DH projects from the perspective of these experiences and her work with eMOP.
Liz Grumbach is Project Manager for the Advanced Research Consortium (ARC) and IDHMC “alt-ac” Research Staff. She is Co-Project Manager for eMOP (Year Two), and will briefly introduce the project (goals and methodologies). She will also end the panel by comparing the current workflow for the eMOP OCRing process with the proposed OCR workflow contained in the grant, summing up the overall changes that each collaborator’s contribution shaped.
Laura Mandell is Professor of English and Director of the IDHMC at Texas A&M University. In addition to being the Lead PI for eMOP, Dr. Mandell previously received a Mellon grant (2010) to investigate how effective the open-source OCR engine Gamera could be trained to read early modern fonts. She will introduce the data management challenges eMOP has faced, demonstrating software and tool solutions created by eMOP graduate students and staff.
Clemens Neudecker serves as Technical Coordinator in the Research section of the Innovation & Development Department of the KB National Library of the Netherlands. He has been working in numerous large-scale national and international digitization / digital humanities projects since the early 2000’s, with a particular focus on OCR (www.impact-project.eu) and scalable workflows (www.scape-project.eu), and will be discussing how this previous knowledge aided the eMOP team.
Todd Samuelson is Assistant Professor at Texas A&M University and the Curator of Rare Books & Manuscripts at Cushing Memorial Library & Archives. Dr. Samuelson is the book history consultant for eMOP. He will discuss font history research roadblocks and demonstrate font creation and identification tools created by eMOP collaborators to solve these issues.
Panel Organizers:
Matthew Christy, Lead Software Applications Developer for the IDHMC and Co-Project Manager for eMOP (Year Two)

Liz Grumbach, IDHMC “alt-ac” Research Staff and Co-Project Manager for eMOP (Year Two)

References
1. European Commission: The Comité des Sages. The New Renaissance: Report of the comité des sages on bringing Europe’s cultural heritage online. By Elizabeth Niggemann, et al. 10 Jan 2011.

2. IMPACT. Annual Report: Project Periodic Report. Netherlands: IMPACT, 2011. Improving Access to Text. 9 Dec 2011. www.impact-project.eu/uploads/media/IMPACT_Annual_report_2011_Publishable_summary_01.pdf. 29 Oct 2013.

3. Mandell, Laura.Mellon Foundation Grant Proposal: ""OCR'ing Early Modern Texts."" Grant Proposal. 30 Jun 2012.

4. Mandell, Laura. (2013) Digitizing the Archive: The Necessity of an 'Early Modern' Period. Journal for Early Modern Cultural Studies 13.2: 83-92.

5. Heil, Jacob and Todd Samuelson. (2013) Book History in the Early Modern OCR Project, or, Bringing Balance to the Force. Journal for Early Modern Cultural Studies 13.4 (2013): 90-103. Web. 30 Oct 2013.

6. Beck, Kent, et al.Manifesto for Agile Software Development. Agile Alliance. 30 Oct 2013.

7. Martin, Robert Cecil (2003). Agile Software Development: Principles, Patterns, and Practices. Saddle River, NJ: Prentice Hall.

8. Scheinfeldt, Tom.Stuff Digital Humanists Like: Defining Digital Humanities by its Values. Found History. 2 Dec 2010. www.foundhistory.org/2010/12/02/stuff-digital-humanists-like . 30 Oct 2013.

9. Smithies, James (2011). A View from IT. Digital Humanities Quarterly 5.3. Web. 30 Oct 2013.

10. Spiro, Lisa. Examples of Collaborative Digital Humanities Projects. Digital Scholarship in the Humanities. 1 Jun 2009. digitalscholarship.wordpress.com/2009/06/01/examples-of-collaborative-digital-humanities-project.. 30 Oct 2013.

11. Unsworth, John.Documenting the Reinvention of Text: The Importance of Failure. The Journal of Electronic Publishing 3.2 (1997). Web. 30 Oct 2013.",txt,This text is republished here with permission from the original rights holder.,,collaboration;cultural preservation;open source;optical character recognition (ocr);project management,English,"archives, repositories, sustainability and preservation;crowdsourcing;cultural infrastructure;digital humanities - institutional support;digitisation, resource creation, and discovery;digitisation - theory and practice;interdisciplinary collaboration;project design, organization, management;renaissance studies;software design and development"
2058,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Sentiment Analysis for the Humanities: the Case of Historical Texts,,Alessandro Marchetti;Rachele Sprugnoli;Sara Tonelli,"paper, specified ""long paper""","1. Introduction
In this paper we investigate the possibility to adapt existing lexical resources and Natural Language Processing (NLP) methodologies related to Sentiment Analysis (SA) to the historical domain.

Sentiment analysis aims at the computational treatment of opinion, sentiment and subjectivity in texts .1

Current research in SA mainly focuses on the identification of sentiment and opinions in areas such as social media 2,  news 3  4, political speeches 5 , customer and movie reviews 6 78. To our knowledge, SA in the context of the humanities has been rarely explored 9 10 11.

Many SA tools often take advantage of polarity lexicons, i.e. a lexicon of positive and negative words and n-grams. In a polarity lexicon, each word is associated with its prior polarity, i.e. the polarity of the word out of the context. A SA system uses these lexicons to evaluate the polarity of a whole text, a sentence or a topic within a text. The availability of a sentiment lexicon is thus a crucial step toward the creation and training of any SA application. Unfortunately, the majority of existing SA lexicons are for English (e.g. Harvard General Inquirer 12) while no lexicon for Italian has been developed yet.

The polarity of a word can however be different according to its context of use. A word can be negated and change its polarity (‘ice-cream is good’ vs ‘ice-cream is not good’) or have different usages (‘they fought a terrific battle""’ vs ‘I loved the film, it was terrific!’). To account for these differences, a system must be able to handle the contextual polarity of a word, i.e. the different  polarity of a word according to its syntactic, semantic or pragmatic context 13 1415 16.

Apart from manual annotation or automatic mapping from English, crowdsourcing methodologies can offer a viable solution to collect a polarity lexicon17 and to annotate a large dataset 18.

The need to explore the application of SA to historical texts has emerged thanks to the collaboration between the authors and the Italian-German Historical Institute (ISIG) in Trento. This collaboration is aimed at developing tools that can help historians access and understand textual data through the adoption of NLP methods. In particular, SA has been identified as notably relevant to quantify the general sentiment of single documents, to track the attitude towards a specific topic or entity over time and across a large collection of texts, and to allow specific search based on sentiment. This is crucial, for instance, to research on the history of ideology, evolution of political thought, etc.

The dataset used for our research is the complete corpus of writings of Alcide De Gasperi, one of the founders of the Italian Republic, made of about 3,000 documents and 3,000,000 words.

Using this corpus as a case study, two experiments have been carried out and are described in this paper. The aim of these experiments is the evaluation of i) how existing lexical resources for SA perform in the historical domain and ii) the feasibility of a sentiment annotation task for historical texts either with expert annotators and crowdsourcing contributors.

2. Prior Polarity Experiment
The first experiment on De Gasperi's corpus has been carried out using two existing polarity lexicons, namely SentiWordNet 19 and WordNet-Affect 20, to calculate the prior polarity of lemmas and measure the general sentiment of each document within the corpus. The goal was to test how resources built on contemporary languages can deal with historical texts.

SentiWordNet and WordNet-Affect have the great advantage of being extensions of a well-known resource called WordNet 21. This allowed us to map the word senses (called synsets) with a positive, negative or neutral polarity in SentiWordNet and WordNet-Affect to the corresponding Italian synsets in MultiWordNet 22, in which Italian synsets are aligned with WordNet ones. At the same time, lemmas were automatically extracted from De Gasperi's corpus using the TextPro tool 23: the total of 70,178 lemmas was reduced to 36,304 after excluding lemmas that can’t have a polarity score (e.g. numbers, articles). Each lemma was then automatically associated with the most frequent synset in MultiWordNet and its polarity score: this association covered 14,874 lemmas (40.97%) among which 9,650 were neutral. This process, followed by a manual check of the scores, produced a list of 5,224 lemmas with a polarity score: 449 with an absolute positive score (e.g. 'giubilo'/rejoicing), 576 with an absolute negative score (e.g. 'affranto'/broken-hearted) and the others with an intermediate score.

The general sentiment of each document in the corpus was finally calculated summing up the polarity scores of the lemmas appearing both in the documents and in our list, and visualized through a gauge diagram in the A.L.C.I.D.E. web platform [dh.fbk.eu/projects/alcide-analysis-language-and-content-digital-environment] (Figure 1).


Fig. 1: Document visualization: sentiment and key-concepts

Historians’ evaluation of the results was positive for most of the documents but a more specific need emerged: historians are indeed more interested in the polarity of a specific topic and in its evolution over time, rather than in the global polarity of a document that can give us indications only about the general sentiment conveyed in it. However, as historical texts are complex documents in which several topics can be identified, the global polarity of the document is not enough to identify the polarity of a single topic.

To address these requirements, we performed the experiment presented in Section 3 aimed at annotating SA at the level of topic in De Gasperi's corpus, following a contextual polarity approach.

3. Crowdsourcing Experiment for Contextual Polarity
In order to perform a pilot experiment, we identified two topics which were relevant in De Gasperi's writings, namely ""sindacato'""(trade union) and ""sindacalismo"" (trade-unionism).

A corpus of 525 sentences was automatically extracted from De Gasperi's corpus, where each sentence contained at least one of the two lemmas “sindacato” and “sindacalismo”. The previous and the following sentence were added as a context as well. Each sentence was annotated by two expert annotators, while a third annotation was collected through the crowdsourcing platform CrowdFlower [www.crowdflower.com] after performing a majority voting over 5 judgements.

The two expert annotators were asked to create gold standard data (GS), i.e. a set of sentences on which both annotators gave the same judgements, from a subset of the corpus (60 sentences, 11% of the whole corpus). Both expert annotators and crowdsourcing contributors were then asked to annotate the contextual polarity of the two topics in the sentences with one of the four possible judegments (i.e. positive, negative, neutral, unknown) given a simple set of instructions and some annotation example. 

In addition to the manual annotation, we also calculated the prior polarity for each sentence using the same algorithm applied to the documents and described in Section 2.

1. The feasibility of this task was then evaluated calculating:

2. the accuracy of the crowdsourced annotation over GS (figure 2), i.e. how well non-expert contributors performed the task;

3. the accuracy of the prior polarity for each sentence over GS (figure 2), i.e. how well the Italian prior polarity lexicon performed on the sentences in comparison to the contextual polarity approach;

the inter-annotator agreement (IAA) with the Fleiss's kappa measure (figure 3)24 , i.e. the level of consensus between the annotators. 


Fig. 2: Accuracy scores


Fig. 3: IAA results

The overall accuracy score for the crowd-collected judgements in Figure 2 (68.3%) indicates the general complexity of the task. In particular negative and positive polarities are more difficult to identify (55.5% and 46.6%) than neutral polarity (80%).

Considering the prior polarity scores in Figure 2, we observe that accuracy is always lower than in the crowd annotation setting, except for the positive judgements (86%).

The IAA agreement in Figure 3 confirms that SA is a a challenging task 25 . The highest kappa-score is found if we consider the two expert annotators (0.46), but it is not much higher than the situation in which we consider 3 annotators (0.39) or one of the two experts and the crowd judgement (0.35). In general, the type of documents have a great influence on the agreement scores: past works report that news stories can achieve an agreement of 0.81 26, whereas social media (tweets) can be as low as 0.321 27.

4. Conclusions and Future Works
This paper presented two experiments related to SA and involving a corpus of historical texts. In the first one we created a new Italian lexical resource for sentiment analysis starting from two existing lexicons for English and we applied it to measure the polarity of an entire document using a prior polarity approach. In the second experiment, the use of crowdsourced annotation to obtain contextual polarity of a specific topic was exploited.

The long term goal of our ongoing research is to create a system to support historical studies, which is able to analyze the sentiment in historical texts and to discover the opinion about a topic and its change over time.

In the near future we plan to perform domain adaptation of existing annotation schemes developed for SA 28 29 and of the Italian lexical resource we created. Particular attention will be devoted to a step-by-step evaluation by historians in order to tailor the results of our work to their needs.

References
Pang, B. and Lee, L. (2008). Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieval 2 (1-2) , 1-135.

Basile, V. and Nissim, M. (2013). Sentiment analysis on Italian tweets. Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pp. 100–107, Atlanta, United States.

Wiebe, J., Wilson, T., and Cardie, C. (2005). Annotating Expressions of Opinions and Emotions in Language, Language Resources and Evaluation 39 (2/3) , 164-210.

Amer-Yahia, S., Anjum, S., Ghenai, A., Siddique, A., Abbar, S., Madden, S., Marcus, A. and El-Haddad, M. (2012). MAQSA: a system for social analytics on news., in K. Selçuk Candan; Yi Chen 0001; Richard T. Snodgrass; Luis Gravano and Ariel Fuxman, ed., 'SIGMOD Conference' , ACM, , pp. 653-656.

Somasundaran, S. and Wiebe, J. (2010). Recognizing Stances in Ideological On-line Debates, in Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pp. 116-124, Los Angeles, CA. Association for Computational Linguistics.

Hu, M. and Liu, B. (2004). Mining and Summarizing Customer Reviews, in Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 168-177 .

Pang, B. and Lee, L. (2004). A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, in Proceedings of the 42nd annual meeting on Association for Computational Linguistics, Barcelona, ES , pp. 271-278 .

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. (2011). Learning word vectors for sentiment analysis, in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 142-150.

Cooper, D. and Gregory, I. N. (2011), Mapping the English Lake District: a literary GIS. Transactions of the Institute of British Geographers, 36: 89–108.

Kakkonen, T. and Kakkonen, G.G. (2011). SentiProfiler: Creating Comparable Visual Profiles of Sentimental Content in Texts, in Proceedings of the Workshop on Language Technologies for Digital Humanities and Cultural Heritage, pp. 62–69, Hissar, Bulgaria. www.aclweb.org/anthology/W11-4110.

Mohammad, S. (2011). From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels and Fairy Tales, in Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 105–114, Portland, OR, USA. Association for Computational Linguistics. www.aclweb.org/anthology/W11-1514.

Stone, P. (1997). Thematic text analysis: new agendas for analyzing text content, in Carl Roberts, ed., Text Analysis for the Social Sciences, Lawerence Erlbaum Associates, Mahwah, NJ .

Kim, S. M., and Hovy, E. (2004). Determining the sentiment of opinions, in Proceedings of the 20th international conference on Computational Linguistics.

Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis, in Proceedings of the conference on human language technology and empirical methods in natural language processing, pp. 347-354.

Nasukawa, T. and Yi, J. (2003). Sentiment Analysis: Capturing Favorability Using Natural Language Processing, in Proceedings of the Conference on Knowledge Capture (K-CAP).

Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment treebank, in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1631-1642.

Mohammad, S. and Turney, P. D. (2013). Crowdsourcing a Word-Emotion Association Lexicon. Computational Intelligence 29 (3) , 436-465.

Pang, B. and Lee, L. (2004). A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, in Proceedings of the 42nd annual meeting on Association for Computational Linguistics, Barcelona, ES , pp. 271-278 .

Baccianella, A. E. S. and Sebastiani, F. (2010). SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining, in Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10).

Strapparava, C. and Valitutti, A. (2004). WordNet-Affect: An affective extension of WordNet, in Proceedings of the 4th International Conference on Language Resources and Evaluation, pp. 1083-1086.

Fellbaum, C., ed. (1998). Wordnet, an Electronic Lexical Database, MIT Press.

Pianta, E., Bentivogli, L. and Girardi, C. (2002). MultiWordNet: developing an aligned multilingual database, in Proceedings of the First International Conference on Global WordNet.

Pianta, E., Girardi, C. and Zanoli, R. (2008). The TextPro Tool Suite, in Proceedings of the 5th Conference on Language Resources and Evaluation (LREC'06).

Artstein, R., and Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4), 555-596.

Pang, B. and Lee, L. (2008). Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieval 2 (1-2) , 1-135.

Balahur, A., and Steinberger, R. (2009). Rethinking Sentiment Analysis in the News: from Theory to Practice and back. Proceeding of WOMSA.

Basile, V. and Nissim, M. (2013). Sentiment analysis on Italian tweets. Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pp. 100–107, Atlanta, United States.

Wiebe, J., Wilson, T. and Cardie, C. (2005), Annotating Expressions of Opinions and Emotions in Language, Language Resources and Evaluation 39 (2/3) , 164-210.

Di Bari, M., Sharoff, S., and Thomas, M. (2013). SentiML: functional annotation for multilingual sentiment analysis. In Proceedings of the 1st International Workshop on Collaborative Annotations in Shared Environment: metadata, vocabularies and techniques in the Digital Humanities.",txt,This text is republished here with permission from the original rights holder.,,historical texts;nlp;sentiment analysis,English,content analysis;corpora and corpus activities;crowdsourcing;historical studies;linking and annotation;natural language processing
2099,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,The Early Modern OCR Project (eMOP): Fostering Access to Early Modern Cultural Materials,,Elizabeth Grumbach;Laura Mandell;Matthew J Christy,poster / demo / art installation,"1. Introduction
The Initiative for Digital Humanities, Media, and Culture at Texas A&M University received a $734,000 grant from the Andrew W. Mellon Foundation in 2012 to make machine readable 45 million pages of data1. By partnering with Gale and Proquest, eMOP will combine open source OCR (Optical Character Recognition) software and book history in order to improve the accuracy of

OCR for early modern (1473-1800) texts2. The Early Modern OCR Project (eMOP) aims to publish an open source OCR workflow, improve the visibility of early modern texts by making them fully searchable3, and form a community of scholars and institutions interested in the digital preservation of these texts4. Our goal is to foster collaboration among various disciplines, and, in doing so, cultivate inter-institutional and international relationships that make possible new kinds of humanities research.

2. Poster Description
Our workflow (see images below for two early drafts of our workflow, subject to change) blends the disciplines of book history, digital humanities, textual analysis, and machine learning in order to create a corpus of keyed texts that are far more correct than is now possible with the current set of tools. These keyed texts will improve access to early modern texts that are currently only searchable through “dirty” OCR or metadata alone. The open source OCR workflow will contain, among other things, access to an early modern font database, customization guidelines for the Tesseract OCR engine, post-processing and diagnostic algorithms, and crowdsourcing correction tools.


Fig. 1: Two versions of the eMOP workflow from October 2012 and February 2014, respectively.

In addition to presenting a detailed and accurate representation of our OCR workflow for early modern texts, we intend to present the following aspects of eMOP: 

Information on how to obtain the open source code for all of the tools, software, and workflows that eMOP has produced. 
How our tools and software can be used by individual scholars, instructors, and institutions in the classroom, for an OCR project, or for personal research. 
3. Demonstration Description
We intend to go beyond presenting an overview of the project; instead our poster and demonstration will communicate the concrete solutions found and software available to address the ""OCR problem"" from a Digital Humanities perspective. To this end, we will demo our our five crowdsourcing and scholar-sourcing correction tools for conference attendees. These demonstrations of the tools will be operating in production with our eMOP OCR output of the EEBO ECCO datasets (45 million pages).

The Franken+ tool, developed by Bryan Tarpley at Texas A&M, enables the creation of an “ideal” typeface using glyphs identified in scanned images of documents from the early modern period. Franken+ also exports these typefaces to a training library for the open-source OCR engine Tesseract5.
Aletheia Layout Editor (ALE), developed by PRImA at the University of Salford, is a crowd-sourced correction tool for re-drawing regions on problematic OCR’d pages, such as Title pages, multi-columned texts, image-heavy documents, and more.
The TypeWright software, developed by Performant Software Solutions and 18thConnect, enables users to correct the “dirty” OCR of an entire early modern document, and our partnership with ECCO allows 18thConnect to release fully corrected documents to their scholar-editors in plain text and TEI-A formats.
The Cobre tool, developed by Dr. Anton DuPlessis and Cushing Memorial Library at Texas A&M, enables scholar-experts to compare, re-order pages, and annotate the metadata for multiple printings of documents in the eMOP dataset. 
The Anachronaut tool, developed by a team of undergraduates and Dr. Ricardo Gutierrez-Osuna at Texas A&M, is a Facebook game that uses the power of Facebook (and many layers of user confidence testing) to correct single words and phrases.
References
1. Mandell, Laura. Mellon Foundation Grant Proposal: ""OCR'ing Early Modern Texts."" Grant Proposal. 30 Jun 2012.

2. Heil, Jacob and Todd Samuelson.Book History in the Early Modern OCR Project, or, Bringing Balance to the Force. Journal for Early Modern Cultural Studies 13.4 (2013): 90-103. Web. 30 Oct 2013.

3. Mandell, Laura.Digitizing the Archive: The Necessity of an 'Early Modern' Period. Journal for Early Modern Cultural Studies 13.2 (2013): 83-92.

4. European Commission: The Comité des Sages.The New Renaissance: Report of the comité des sages on bringing Eu rope’s cultural heritage online. By Elizabeth Niggemann, et al. 10 Jan 2011.

5. Katayoun, Torabi, Jessica Durgan and Bryan Tarpley.Early modern OCR project (eMOP) at Texas A&M University: using Aletheia to train Tesseract. Proceedings of the 2013 ACM symposium on Document Engineering. New York: ACM, 2013.",txt,This text is republished here with permission from the original rights holder.,,cultural preservation;open source;optical character recognition (ocr);tools,English,"archives, repositories, sustainability and preservation;crowdsourcing;cultural infrastructure;digitisation, resource creation, and discovery;english studies;other;software design and development"
2105,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,The Open Philology Project at the University of Leipzig,,Monica Berti;Frederik Baumgardt;Giuseppe Celano;Gregory R. Crane;Stella Dee;Simon Frazier;Maryam Foradi;Emily Franzini;Greta Franzini;Simona Stoyanova,poster / demo / art installation,"The Open Philology Project (OPP) at the University of Leipzig aspires to re-assert the value of philology in its broadest sense and has been designed with the hope that it can contribute to any historical language that survives within the human record. It includes three different yet interdependent tasks:

(1) Open Greek and Latin Project (OGL): OGL is currently collecting and scanning editions of classical texts in an effort to build the largest and most comprehensive open-source library of classical philology to date, concurrently contributing to the expansion of Google Books. Where existing corpora of Greek and Latin have generally included one edition of a work, the OGL corpus is designed to manage multiple, copyright-free editions and translations.

The digitization workflow involves OCR, correction and encoding in EpiDoc-compliant XML. The large volume of data we aim to generate requires significant computational power and task management, thus entreating a partnership with two Data Entry companies who carry out each operation under the supervision of the Leipzig team. While performed by our contractors, OCR correction is facilitated and partly automated thanks to a proofreading tool jointly developed by Leipzig, Mount Allison University and the CNR (Bruce Robertson of Mount Allison University, Canada, and Federico Boschetti of the CNR, Italy) Works currently under conversion include, amongst others, the Patrologia Latina, the Patrologia Graeca, the Commentaria in Aristotelem Graeca.

Moreover, Leipzig has established international collaborations aiming at creating open-source, curated collections and electronic editions of Greek and Latin literature. Editorial projects include the Digital Fragmenta Historicorum Graecorumproject, Digital Athenaeus, and Bibliotheca Aeschylea. Furthermore, collaborations with Croatia, Bulgaria and Georgia will yield machine-actionable versions of translations of classical literature in these languages, thus opening-up research into less-explored textual heritage.

(2) Historical Languages e-Learning Project (eLP): the development of dynamic textbooks that use richly annotated corpora to teach the vocabulary and grammar of texts that learners have chosen to read, and at the same time engage users in collaboratively producing new annotated data. eLPis developing computationally customized learning materials for historical languages, beginning with Ancient Greek. The text selected for the pilot is the Pentecontaetia, part of Thucydides' History of the Peloponnesian War. Users learn through active engagement with the text and through the contribution of their own annotations. Future work will extend the system to accommodate other corpora.

At the core of eLP lies increasing the accessibility and enjoyability of the morphosyntactic and semantic annotation of text (e.g. treebanking), including that deriving from the OGL corpus. The creation of such a richly annotated and searchable text repository will serve a variety of purposes, including research in philology, Natural Language Processing (NLP), historical linguistics, and second language acquisition (SLA).

The production of automated queries to support this dynamic, customized, and localized interface relies upon the backend storage of complex textual data. The chosen graph model meets the broad requirements of the e-Learning application while retaining features of the real world objects represented by the data. The absence of schemas within graph databases enables extensibility, while maintaining a stable experience for users through the use of REST APIs.3 The web interface takes the data and adapts its presentation to individual needs and access devices. HTML5, CSS3, and responsive technologies provide an appropriate experience to users regardless of how they access the system, while templating systems allow for resources that are structurally accessible via any first language.

(3) Open Publications and Data Revenue Models: OPP is establishing a new model of scholarly publication in a born digital environment. Such a task is accomplished through Perseids, which is a collaborative platform for annotating TEI XML documents in Classics, including inscriptions and manuscripts. The main publication model within the OPP is the Leipzig Open Fragmentary Texts Series, whose goal is to establish open editions of ancient works that survive through quotations and re-uses in later texts. Such editions are fundamentally hypertexts and the effort is to produce a dynamic infrastructure for a full representation of relationships between sources, quotations, and annotations about them.

With open data meaning by definition free access for all users, the OPP team has already begun thinking of ways for it to be financially sustainable for years to come. The team intends to devise business models to sustain and maintain distributed open source learning and discourse. The core principle is to move away from charging for monopoly access to data, to charging instead for services that allow users to identify, analyze and then contribute to increasingly complex open data, with services for faculties, students and for the interested public set at recognized and affordable price points.

References
Bruce Robertson of Mount Allison University, Canada, and Federico Boschetti of the CNR, Italy.

Digital Fragmenta Historicorum Graecorum (DFHG): www.dh.uni-leipzig.de/wo/open-philology-project/the-leipzig-open-fragmentary-texts-series-lofts/digital-fragmenta-historicorum-graecorum-dfhg-project; Digital Athenaeus (with the University of Nebraska and the Perseus Digital Library), Bibliotheca Aeschylea (with researchers based in Leipzig and in Italy).

For more information about REST APIs, see: en.wikipedia.org/wiki/Representational_state_transfer (Accessed: 4 March 2014).",txt,This text is republished here with permission from the original rights holder.,,collaborative platform;e-learning;epidoc;linguistic annotation;ocr;open data;tei xml,English,classical studies;digital humanities - pedagogy and curriculum;digitisation - theory and practice;encoding - theory and practice;hypertext;information retrieval;interdisciplinary collaboration;linking and annotation;philology;semantic web;xml
2125,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Tracking Semantic Drift in Ancient Languages: The Bible as Exemplar and Test Case,,Matt Munson,"paper, specified ""long paper""","Language changes. On this everyone would agree. But how can we track this ever-changing phenomenon? If we focus on modern languages, the task is easier since we have native speakers whom we can ask, “How is this usage different than this other one?”  But in the case of historical languages, and especially those spoken and written millennia ago, this task becomes much more difficult. How is it possible for us to create, or at least simulate, in ourselves the language proficiency of a society that has been dead for hundreds or thousands of years? And if we cannot rely on native proficiency, how can we track systematic language change and, thus, come to a better understanding of the language and texts of any particular period. The pioneering work most closely associated with John Sinclair gives us our best answer: Trust the Text!1  We have millions of words of, e.g., Greek, ranging over a time-span of 3000 years from Homer to the present day.2 What we need are methods that can help us to harness this huge amount of information. David Bamman and Gregory Crane have already begun working in the field of historical word-sense variation in Latin.3  Relying on translation equivalents, they were able to successfully track word sense variation in Latin in a 389-million word corpus. By their own admission, however, this method has the drawback of requiring “large amounts of parallel text data”4 in translation. In contrast, the method proposed here, comparison of co-occurrence patterns in two or more corpora, which has been applied in many other fields (see below), only requires simple, plain-text input in a single language.
The first theoretical foray into computational analysis of co-occurrence patterns came in 1955 with Warren Weaver’s article “Translation.”5 Starting from the recognition that the sense of any word is ambiguous if examined in isolation, he asserts, “But if one lengthens the slit in the opaque mask, until one can see not only the central word in question, but also say N words on either side, then if N is large enough one can unambiguously decide the meaning of the central word.”6  The necessary corpora and computational power to realize Weaver’s theory, however, only came much later.  Computational analysis of co-occurrence patterns on large-scale corpora began with the COBUILD project, which set out to build “the very first dictionaries to be based completely on corpus data” and, in doing so, systematically tracked collocations, defined as “the high-frequency words native English speakers naturally use with the target word.”7  Since then, co-occurrence analysis has been used in several fields in which word-sense disambiguation is necessary, such as speech recognition,8 machine translation,9 and topic modeling,10 and it is the basis for the field of distributional semantics.11
In this paper, I will present my application of co-occurrence analysis to the problem of historical word-sense variation, what I call in my title “semantic drift.”  I have chosen to carry out these experiments using as my two corpora the Greek Old Testament (the Septuagint) and the Greek New Testament for several reasons: the texts are easily available in digital form, have been deeply researched and, thus, deeply annotated, exist in multiple translations that can be used to benchmark methods and to test results, are of great interest to millions of people around the world, and, finally, because they are the most influential texts in the history of western civilization, the research can be easily extended to other corpora and, with more difficulty, even into other contemporary languages such as Latin, Hebrew, Aramaic, and Coptic, to name just a few.  The presentation will have two primary foci: the method and exemplary results. 
The method consists of the following steps.  First, I tokenized the texts and calculated co-occurrence counts for every word in an 8-word window.12  Using these co-occurrence tables, I calculated the statistical significance of each collocate word to each node word using the log-likelihood measure as described by Manning and Schütze.13 Log-likelihood was chosen primarily because it deals very well with sparse data and can be easily interpreted without recourse to, e.g., chi-squared tables.14  The former is important because most data in language is quite sparse and I was reluctant to eliminate a large amount of my data simply because the chosen method could not deal well with it.15  Ease of interpretation was important because, instead of using the measure as a means of hypothesis testing, in which I would expect to get a yes or no answer, I used it as hypothesis weighting, i.e., to measure how much more likely one thing is than another.  My purpose is not to decide if two words certainly form a set collocation but, instead, to measure the strength of collocation, ranging from strong repulsion to strong attraction, and compare this range with the ranges of other node words to find relationships.  Having calculated the statistical significance of these relationships, I used the cosine similarity16 measure to determine the strength of relationship, first, of every word in the Old Testament with its counterpart in the New Testament (e.g., θεός (God) in the Old Testament to θεός in the New Testament) and, second, of every word in the Old Testament with every other word in the Old Testament and the same for the New Testament.  These results allow me to discover which words’ senses have changed the most (comparison of Old Testament to New Testament) and how they have changed (comparison of the words most similar to, e.g., θεός in the Old Testament with those most similar to θεός in the New Testament).

Fig. 1: Results based on the differences in cosine similarity measure between θεός (God) and the list words. Those on the left are nearer to θεός in the OT, on the right to θεός in the NT.
After relying purely on computational methods to this point, the final results of my research come through qualitative analysis of the comparisons described in the previous paragraph.  The two tables above show the 20 words most closely associated with θεός (God) in the Old Testament and the New Testament based on the differences between the cosine similarity scores in each testament between θεός (God) and the words in the list.  The colors have been added by me to highlight what I see to be related words in each list.  What we see on the left is that God in the Old Testament is more closely related to words concerning ruling (in yellow: “Solomon”, “command”, “anointed”, “Benjamin”), violence (red: “destroy utterly”, “to lay hold of”, “to drive away”, “to strike”), agriculture (brown: “field”, “cattle”), and the Exodus (green: “captivity”, “foreign”).  While in the New Testament, God is more closely related to (evil) rulers (yellow: “Satan”, “Pharisees”, “centurion”, “Pilate”), servants of God (dark purple: “Peter”, “Christ”, “apostle”, “Paul”, “disciple”), and words that relate the servants to God (light purple: “to believe”, “faith”, “gospel”, “grace”, “love”). So, by classifying the words most closely related with θεός (God) in each of the testaments, we are able to determine not only that the portrayal of God had changed from the Old Testament to the New Testament, but also to see how it changed (move from a ruler who leads and makes war to a patron who offers to and receives favors from clients) and to guess at the probable historical cause (change from an independent monarchy to a Roman province).  In the second part of this paper, salient examples, such as that described above for God, will be used to demonstrate the effectiveness of this method.
The final section of the paper will be a look forward at how this method could be extended to other corpora and even other languages, allowing us to tell the stories of language development with more precision and so, ultimately to understand historical texts better.
References

1. John McHardy Sinclair (2004). Trust the text : language, corpus and discourse. London: Routledge.
2. The Thesaurus Lingua Graece collection claims to contain 105 million words of Greek “from Homer (8 c. B.C.) to the fall of Byzantium in AD 1453 and beyond.” Thesaurus Lingua Graece. n.d. 1 November 2013. www.tlg.uci.edu.
3. David Bamman and Gregory Crane (2011). Measuring Historical Word Sense Variation.www.perseus.tufts.edu/publications/bamman-11.pdf. 30 October 2013.
4. Bamman and Crane, 1.
5. Warren Weaver (1955). Translation.www.mt-archive.info/Weaver-1949.pdf. 30 October 2013.
6. Weaver, 8.
7. www.mycobuild.com/about-cobuild.aspx
8. www.mycobuild.com/about-cobuild.aspx
9. en.wikipedia.org/wiki/Machine_translation#Disambiguation
10. Mark Steyvers and Tom Griffiths (2007). Probabilistic Topic Models.psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf. 30 October 2013.
11. This field was pioneered especially by J.R. Firth and Zellig Harris in the 1950s. See especially Zellig S. Harris, „How Words Carry Meaning.“ 1986. Language and Information: The Bampton Lectures, Columbia University, 1986. Lecture. 1. November 2013. www.ircs.upenn.edu/zellig/3_2.mp3 and John Rupert Firth. ""A synopsis of linguistic theory 1930-1955."" Selected Papers of J.R. Firth, 1952-1959. Ed. F.R. Palmer. Harlow: Longmans, 1968. 168-205.
12. Léon, p. 14, footnote 15.
13. Christopher Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language Processing. Cambridge, Massachusetts: MIT Press. 172-175.
Manning and Schütze, 172. On the topic of log-likelihood and spare data, see Ted Dunning. „Accurate Methods for the Statistics of Surprise and Coincidence.“ March 1993. ACL Anthology: A Digital Archive of Research Papers in Computational Linguistics. 1. November 2013. acl.ldc.upenn.edu/J/J93/J93-1003.pdf
15. Dunning, 61.
16. Manning and Schütze, 299-303.
",txt,This text is republished here with permission from the original rights holder.,,co-occurrence;historical languages;nlp;semantic drift,English,classical studies;corpora and corpus activities;data mining / text mining;historical studies;natural language processing;text analysis;theology
2185,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,Automatic semantic tagging of Leo Tolstoys works,,Daniil Skorinkin;Anastasia Bonch-Osmolovskaya,"paper, specified ""short paper""",,,Creative Commons Attribution 4.0 International,,fact extraction;information extraction;named entity recognition;nlp;ontology engineering;rdf;tei;text mining,English,"data mining / text mining;digitisation, resource creation, and discovery;english;linking and annotation;literary studies;natural language processing;ontologies;semantic analysis;text analysis;xml"
2221,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,Digital Democracy Project: Making Government More Transparent one Video at a Time,https://github.com/ADHO/dh2015/blob/master/xml/DEKHTYAR_Alex_Digital_Democracy_Project__Making_Governm.xml,Sam Blakeslee;Alex Dekhtyar;Foaad Khosmood;Franz Kurfess;Toshihiro Kuboi;Hans Poshcman;Giovanni Prinzivalli;Christine Roberston;Skylar Durst,poster / demo / art installation,"<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Digital Democracy Project: Making Government More Transparent one Video at a Time</title>
                <author>
                    <persName>
                        <surname>Blakeslee</surname>
                        <forename>Sam</forename>
                    </persName>
                    <affiliation>Institute for Advanced Technology and Public Policy, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>sblakesl@calpoly.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Dekhtyar</surname>
                        <forename>Alex</forename>
                    </persName>
                    <affiliation>Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>dekhtyar@calpoly.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Khosmood</surname>
                        <forename>Foaad</forename>
                    </persName>
                    <affiliation>Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>foaad@calpoly.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Kurfess</surname>
                        <forename>Franz</forename>
                    </persName>
                    <affiliation>Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>fkurfess@calpoly.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Kuboi</surname>
                        <forename>Toshihiro</forename>
                    </persName>
                    <affiliation>Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>tkuboi@calpoly.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Poshcman</surname>
                        <forename>Hans</forename>
                    </persName>
                    <affiliation>Institute for Advanced Technology and Public Policy, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>hposchma@calpoly.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Prinzivalli</surname>
                        <forename>Giovanni</forename>
                    </persName>
                    <affiliation>Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>gprinziv@gmail.com</email>
                </author>
                <author>
                    <persName>
                        <surname>Roberston</surname>
                        <forename>Christine</forename>
                    </persName>
                    <affiliation>Institute for Advanced Technology and Public Policy, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>crober22@calpoly.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Durst</surname>
                        <forename>Skylar</forename>
                    </persName>
                    <affiliation>Department of Computer Science, California Polythechnic State University, San Luis Obispo, United States of America</affiliation>
                    <email>cdurst@calpoly.edu</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Long Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>e-government</term>
                    <term>digital democracy</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>databases &amp; dbms</term>
                    <term>audio</term>
                    <term>video</term>
                    <term>multimedia</term>
                    <term>publishing and delivery systems</term>
                    <term>digitisation</term>
                    <term>resource creation</term>
                    <term>and discovery</term>
                    <term>software design and development</term>
                    <term>speech processing</term>
                    <term>information architecture</term>
                    <term>interdisciplinary collaboration</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>One would think in one of the most advanced industrialized countries in the world, in the state most renowned for technological innovation, an average citizen can easily find out what was said in a public session by his or her elected representative. Surprisingly this is not the case in California in 2014. In the California State Legislature, important legislative decisions on pending bills occur in committee hearings. The videos of these hearings are publicly available,
                <hi rend=""superscript"">1</hi> but California is one of the many US states that do not transcribe or close-caption them. The committee hearings contain a treasure trove of information,
                <hi rend=""superscript"">2</hi> but it takes extraordinary time and effort to find relevant information in the hours upon hours of raw, untranscribed, and unindexed footage. The ability to access and search transcripts of legislative committee hearings is a great resource for political insiders, media, government watchdog organizations, and interested citizens. It is one way to enable public accountability of elected officials (as well as government officials and lobbyists participating in the hearings) for the things they said at these hearings, and, more importantly, for the effects of the hearings on the state’s legislation. 
            </p>
            <p>We present our contribution to government transparency: the Digital Democracy (DD) platform. The goal of the project is to provide automated, inexpensive, timely, accurate, and informative knowledge extracted from legislative hearing sessions. </p>
            <p>The Digital Democracy System Design </p>
            <p>The Digital Democracy platform obtains data about the legislative committee hearings: the video archives, the information about the state legislature, and so on. Figure 1 shows the design of the DD system. The main source of information for the DD platform is the Cal Channel
                <hi rend=""superscript"">3</hi> video archive of legislative sessions, a service provided courtesy of cable TV companies that operate in California. In addition, the DD platform obtains information on the nature of legislative sessions (bills, legislators, committees), political contributions, registered lobbyists, and financial disclosures from a variety of existing online databases provided by the state of California or by good government organizations. The information is stored in the Digital Democracy database (DDDB), from which it is delivered to the end users and used for analytical tasks.
            </p>
            <figure>
                <graphic n=""1001"" width=""13.837708333333333cm"" height=""5.3445833333333335cm"" url=""Pictures/image1.jpeg"" rend=""block""/>
            </figure>
            <p>Figure 1. Digital Democracy system architecture. </p>
            <p>Access to the information and the knowledge stored in the DD database is provided through a variety of means. The beta version of the DD platform uses a web portal
                <hi rend=""superscript"">4</hi> as the means of accessing the data and interacting with the system. Mobile applications and social media plugins, complete with alerting mechanisms, are planned for the future. 
            </p>
            <p>
                <hi rend=""bold"">Users and Use Cases </hi>
            </p>
            <p>We consider three core categories of users of the platform: (a) individual citizens, interested in legislative affairs; (b) media and watchdog organizations observing and covering legislative sessions; and (c) legislators and their staff. For the first category of users, the key use case is the ability to find all committee hearings associated with a specific bill, watch the hearings and read transcripts, and search for the things a specific lawmaker (e.g., representing their district) said in those hearings. The second group of users is interested in searching the database for specific topics and being able to listen to and watch the portions of committee hearings related to the topics of their interest. Finally, the third category of users wants to use the DD platform to find specific moments during committee hearings where important decisions were made or important words were said. </p>
            <p>The Beta Site</p>
            <p>In 2014 we piloted the beta site
                <hi rend=""superscript"">5</hi> for the project. The site provided the basic navigation and search functionality for a collection of California legislative committee hearings from the 2013–2014 session. Users could search hearing transcripts, browse hearings and bills discussed at them, and look up information about the hearing participants. Screenshots of the beta site depicting results of the hearing transcript search and a hearing page are shown in Figure 2. 
            </p>
            <p>Evaluation</p>
            <p>We conducted a beta test of the Digital Democracy site from March to July of 2014. The beta testers (public officials, their staffers, journalists, political activists) had access to about 30 committee hearings, primarily devoted to the discussion of California’s state budget. Tables 1, 2, and 3 and Figure 3 summarize the results of the beta test.</p>
            <p>Conclusion</p>
            <p>The Digital Democracy platform is a research and reporting tool providing popular access to an aspect of the political discourse that was hitherto not available. We find that Digital Democracy is well received and useful, but has the potential to be much more. As we continue building the new features for our upcoming releases, we take into consideration the additional recommendations and feedback we receive from our beta testers.</p>
            <table rend=""rules"">
                <row>
                    <cell rend=""DH-Default""/>
                    <cell rend=""DH-Default"">Do you feel that the ability to search legislative hearing videos using the Digital Democracy tool would be useful?</cell>
                    <cell rend=""DH-Default"">If you were able to search and clip a video to share on social media, would you find this feature useful?</cell>
                    <cell rend=""DH-Default"">If you were able to run analytics to identify trends in speaker statements, speaker interactions, voting records, and donor data, would you find this feature useful?</cell>
                    <cell rend=""DH-Default"">During the beta test, did you find the Digital Democracy site easy to navigate?</cell>
                    <cell rend=""DH-Default"">During the beta test, did you ever have problems with the video not playing correctly?</cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">YES</cell>
                    <cell rend=""DH-Default"">27</cell>
                    <cell rend=""DH-Default"">23</cell>
                    <cell rend=""DH-Default"">24</cell>
                    <cell rend=""DH-Default"">23</cell>
                    <cell rend=""DH-Default"">3</cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">NO</cell>
                    <cell rend=""DH-Default"">1</cell>
                    <cell rend=""DH-Default"">6</cell>
                    <cell rend=""DH-Default"">3</cell>
                    <cell rend=""DH-Default"">5</cell>
                    <cell rend=""DH-Default"">23</cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">NO RESPONSE</cell>
                    <cell rend=""DH-Default"">1</cell>
                    <cell rend=""DH-Default"">0</cell>
                    <cell rend=""DH-Default"">2</cell>
                    <cell rend=""DH-Default"">1</cell>
                    <cell rend=""DH-Default"">3</cell>
                </row>
            </table>
            <p>Table 1. Tester feedback on usefulness of Digital Democracy features and potential features.</p>
            <figure>
                <graphic n=""1002"" width=""6.82625cm"" height=""5.4504166666666665cm"" url=""Pictures/image2.png"" rend=""inline""/>
            </figure>
            <figure>
                <graphic n=""1003"" width=""8.269111111111112cm"" height=""6.291791666666667cm"" url=""Pictures/image3.png"" rend=""inline""/>
            </figure>
            <p>Figure 2. Screenshots of the Digital Democracy beta site.</p>
            <table rend=""rules"">
                <row>
                    <cell rend=""DH-Default""/>
                    <cell rend=""DH-Default"">Searching Keywords in Transcripts</cell>
                    <cell rend=""DH-Default"">Searching Speakers</cell>
                    <cell rend=""DH-Default"">Searching Issues/Topics</cell>
                    <cell rend=""DH-Default"">More Complex Analytical Searches</cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">EXTREMELY USEFUL</cell>
                    <cell rend=""DH-Default"">22</cell>
                    <cell rend=""DH-Default"">20</cell>
                    <cell rend=""DH-Default"">19</cell>
                    <cell rend=""DH-Default"">13</cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">USEFUL</cell>
                    <cell rend=""DH-Default"">5</cell>
                    <cell rend=""DH-Default"">7</cell>
                    <cell rend=""DH-Default"">6</cell>
                    <cell rend=""DH-Default"">11</cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">NOT USEFUL</cell>
                    <cell rend=""DH-Default"">2</cell>
                    <cell rend=""DH-Default"">2</cell>
                    <cell rend=""DH-Default"">2</cell>
                    <cell rend=""DH-Default"">3</cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">NO RESPONSE</cell>
                    <cell rend=""DH-Default"">0</cell>
                    <cell rend=""DH-Default"">0</cell>
                    <cell rend=""DH-Default"">2</cell>
                    <cell rend=""DH-Default"">2</cell>
                </row>
            </table>
            <p>Table 2. Tester feedback on usefulness of search functionality.</p>
            <figure>
                <graphic n=""1004"" width=""11.281833333333333cm"" height=""6.261805555555555cm"" url=""Pictures/image4.png"" rend=""inline""/>
            </figure>
            <p>Figure 3. How users would use Digital Democracy data.</p>
            <table rend=""rules"">
                <row>
                    <cell rend=""DH-Default"">
                        <hi rend=""bold"">In Free-Form Feedback</hi>
                    </cell>
                    <cell rend=""DH-Default"">number</cell>
                    <cell rend=""DH-Default"">example</cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">Thought Very Useful</cell>
                    <cell rend=""DH-Default"">8</cell>
                    <cell rend=""DH-Default"">
                        <hi rend=""background(white)"">‘This looks to be an extremely useful tool. I thought the site was easy to navigate and the links to video and text were seamless and really helpful’</hi>.
                    </cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">Suggested an Improvement</cell>
                    <cell rend=""DH-Default"">7</cell>
                    <cell rend=""DH-Default"">‘
                        <hi rend=""background(white)"">Creating an alphabet list, at the top of the “browse speakers” feature in addition to the already present features, would allow for ease if searching by speaker’</hi>.
                    </cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">Wanted to See More</cell>
                    <cell rend=""DH-Default"">7</cell>
                    <cell rend=""DH-Default"">‘I 
                        <hi rend=""background(white)"">think this is a great start for a test. Look forward to seeing it grow with the inclusion of other subject areas (ie. energy, transportation, justice system, etc).’</hi>
                    </cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">Had Difficulties</cell>
                    <cell rend=""DH-Default"">2</cell>
                    <cell rend=""DH-Default"">‘
                        <hi rend=""background(white)"">Actually, the site never loaded at all. I could not give any real assessment as a result. But I would very much be interested in this kind of material being available and searchable’</hi>.
                    </cell>
                </row>
                <row>
                    <cell rend=""DH-Default"">Wanted to Collaborate</cell>
                    <cell rend=""DH-Default"">1</cell>
                    <cell rend=""DH-Default"">‘
                        <hi rend=""background(white)"">Would be interested in discussing how we integrate our services’</hi>.
                    </cell>
                </row>
            </table>
            <p>Table 3. Types of user feedback in free-form responses. </p>
            <p>Notes</p>
            <p>. http://www.calchannel.com/recentarchive/.</p>
            <p>2. As evidenced by the firsthand experience of the first author, who served in both the California State Assembly and the California State Senate.</p>
            <p>3. http://www.calchannel.com.</p>
            <p>4. http://www.digitaldemocracy.org.</p>
            <p>5. http://www.digitaldemocracy.org.</p>
        </body>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,digital democracy;e-government,English,"audio, video, multimedia;databases & dbms;digitisation, resource creation, and discovery;english;information architecture;interdisciplinary collaboration;publishing and delivery systems;software design and development;speech processing"
2226,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,Digital Media Literacy in Indonesian Youth: Building Sustainable Democratic Institutions and Practices,https://github.com/ADHO/dh2015/blob/master/xml/SUWANA_Fiona_Digital_Media_Literacy_in_Indonesian_Youth.xml,Fiona Suwana,poster / demo / art installation,"<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Digital Media Literacy in Indonesian Youth: Building Sustainable Democratic Institutions and Practices</title>
                <author>
                    <persName>
                        <surname>Suwana</surname>
                        <forename>Fiona</forename>
                    </persName>
                    <affiliation>Queensland University of Technology, Australia</affiliation>
                    <email>f.suwana@qut.edu.au</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Short Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>digital media literacy</term>
                    <term>democray</term>
                    <term>digital democracy</term>
                    <term>youth civic engagement</term>
                    <term>Indonesian youth</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>digital humanities - nature and significance</term>
                    <term>asian studies</term>
                    <term>media studies</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>As the world’s third-largest democracy, Indonesia has experienced fundamental changes since the country progressed from a repressive authoritarian to a democratic regime in 1998. Competitive, direct elections have been held four times; freedom of speech and assembly are legally protected; and citizens have a lively and outspoken public sphere. Freedom House, the U.S.-based non-governmental organization, labels Indonesia as a free and democratic country (Antlov and Wetterbeg, 2011, 3). Indonesia’s Ministry of Communication and Informatics reports that there were 82 million Internet users in 2014, and Indonesia has the eighth-highest number of Internet users in the world (Kominfo, 2014). The Internet is perceived as a media that can potentially promote greater democracy due to it being a freer and more open form of communication compared to traditional media, therefore opening more opportunities for freedom of speech and other communicative functions than traditional media communication.</p>
            <p>Bennett (2008, 9) argues that the future of democracy is in young people’s hands. It is crucial for young people to learn how to use digital media in order to develop civic and political actions. Democracy and participation in public culture require not just a willingness to consume information but a willingness to create, share, and use information. Internet use has increased rapidly in Indonesia, particularly among Indonesian youth (Kominfo, 2014). This provides an opportunity to use Internet and social media to enhance democracy in Indonesia. Nugroho, Putri, and Laksmi (2012, 7) found that the Internet has encouraged space for citizens to communicate without restriction. Also, Indonesian citizens can create their own public sphere and engage freely with others by blogs, social media, and micro-blogging. Digital media literacy is an important life skill that is increasingly necessary to develop democracy in Indonesia; therefore, this research aims to investigate whether young Indonesian people have been able to understand and meaningfully use digital media to support democratic institutions and practices.</p>
            <p>There will be two stages in this research. In the first phase, the focus group method will be suitable for researching Indonesian youth’s digital media literacy and capacity to use digital media to support democracy. I will conduct two focus group discussions to identify and compare their digital media practices. The first focus group will be conducted with young Indonesian urban men and women, middle class, ages 18 to 25 (as university/college students), and Internet heavy users (defined as using the Internet a minimum of once a day (ACMA, 2009a, 6). The second focus group will be conducted with a group of the same background as Group 1 but with a different Internet habit, namely Internet medium users, defined as using the Internet one to seven times a week (ACMA 2009a, 6). Both focus groups will be recruited from faculties of politics/communication in the top 10 university/colleges in Jakarta, and respondents will be drawn from leadership positions in campus organisations (student union/guild). Focus group discussion topics will include how the student makes responsible choices and accesses online information, and also how the student analyses and creates information in a variety of digital media forms. The second phase, semi-structured interviews, will be conducted relating to the digital media practices of young Indonesian urban men and women, middle class, ages 18 to 30, active members in non-profit organizations or the community as a leader or participating on a committee, Internet heavy user and living in Indonesia’s capital, Jakarta. The findings will measure the role of digital media literacy to enhance democracy in Indonesia.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">ACMA (Australian Communication Media Authority).</hi> (2009a). Australia in the Digital Economy Report 2: Online Participation. http://www.acma.gov.au/webwr/aba/about/recruitment/online_participation_aust_in_digital_economy.pdf.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ACMA (Australian Communication Media Authority).</hi> (2009b). Digital Media Literacy in Australia: Key Indicators and Research Sources. http://www.acma.gov.au/theACMA/the-acma-digital-media-literacy-resources.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Antlov, H. and Wetterbeg, A.</hi> (2011). Citizen Engagement, Deliberative Spaces, and the Consolidation of a Post-Authoritarian Democracy: The Case of Indonesia. http://www.icld.se/pdf/ICLD_wp8_printerfriendly.pdf.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bennett, L.</hi> (2008). Changing Citizenship in the Digital Age. In Bennett, W. L. (ed.), 
                        <hi rend=""italic"">Civic Life Online: Learning How Digital Media Can Engage Youth</hi>, 1-24, doi: 10.1177/1748048514524119.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kominfo.</hi> (2014). Kemkominfo: Pengguna Internet di Indonesia Capai 82 Juta (Ministry of Communication and Information: Internet Users in Indonesia Reached 82 Million). http://kominfo.go.id/index.php/content/detail/3980/Kemkominfo%3A+Pengguna+Internet+di+Indonesia+Capai+82+Juta/0/berita_satker#.VDSDcBB8C3p (accessed 1 September 2014).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Nugroho, Y., Laksmi, S., Amalia, M., Santi Widyartini, M., Cox, D. and Miles, I.</hi> (2011). Citizens in @ction: Collaboration, Participatory Democracy, and Freedom of Information: Mapping Contemporary Civic Activism and the Use of New Social Media in Indonesia. http://www.cdi.manchester.ac.uk/newsandevents/documents/Citizensinaction-MIOIR-HIVOSFinal_Report.pdf.
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(222222)"">Nugroho, Y., Putri, D. and Laksmi, S.</hi>
                        <hi rend=""color(222222)"">(2012).</hi>
                        <hi rend=""apple-converted-space"">
                            <hi rend=""color(222222)""> </hi>
                        </hi>
                        <hi rend=""color(222222)"">Mapping the Landscape of the Media Industry in Contemporary Indonesia. Report Series: Engaging Media, Empowering Society: Assessing Media Policy and Governance in Indonesia through the Lens of Citizens’ Rights. http://www.cipg.or.id/uploads/books/D02-MediaIndustry-CIPG-Hivos-MAN_FULL_FINAL_rev.pdf.</hi>
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,democray;digital democracy;digital media literacy;indonesian youth;youth civic engagement,English,asian studies;digital humanities - nature and significance;english;media studies
2392,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,TILT 2: Text to Image Linking Tool,https://github.com/ADHO/dh2015/blob/master/xml/SCHMIDT_Desmond_TILT_2__Text_to_Image_Linking_Tool.xml,Desmond Allan Schmidt,"paper, specified ""short paper""","<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>TILT 2: Text to Image Linking Tool</title>
                <author>
                    <persName>
                        <surname>Schmidt</surname>
                        <forename>Desmond</forename>
                    </persName>
                    <affiliation>Queensland University of Technology, Australia</affiliation>
                    <email>desmond.allan.schmidt@gmail.com</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Short Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>OCR</term>
                    <term>text-to-image linking</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>image processing</term>
                    <term>audio</term>
                    <term>video</term>
                    <term>multimedia</term>
                    <term>digitisation</term>
                    <term>resource creation</term>
                    <term>and discovery</term>
                    <term>software design and development</term>
                    <term>programming</term>
                    <term>visualisation</term>
                    <term>linking and annotation</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p rend=""CM4"">TILT is a Web-service and graphical user interface designed to make it easier to study printed books or handwritten manuscripts. TILT recognises word-shapes in page images, then links them to words in an already existing transcription. Unlike a traditional OCR program, no actual characters are recognised. Instead, the links are used to provide visual cues to the user by highlighting corresponding parts of the image and the text as the user moves the mouse over the words or taps them.</p>
            <p>Text-to-image linking was ﬁrst explored by Seales et al. (2000) in electronic editions of old English texts at the University of Kentucky. Their technique was to manually draw mostly rectangles around areas of manuscripts and then link them to the text by means of embedded markup in the transcription (Dekhtyar et al., 2005), developing eventually the commercial EPT tool. Other examples include the British Museum’s electronic 
                <hi rend=""italic"">Codex Sinaticus</hi> (BM, 2009) and, using a different display technique, the ‘zoom topographic’ view of the Samuel Beckett digital manuscript project (Hulle et al., 2011), the TILE project (Reside et al., 2011), and the TextGrid text-to-image link editor (Al-Hajj and Küster, 2013). However, all of these methods were predominantly manual. Rather than automatically processing the text and images, they invite the user to manually draw shapes (rectangles, ovals, and polygons) around words, and to link them one at a time to words in the text. This is very tedious, and the linking methods also lead to problems of overlapping markup that can only be resolved by creating two separate transcriptions: one to contain the text-to-image links and one the formatted text (Middell and Wissenbach, 2011). 
            </p>
            <p rend=""CM5"">The earlier TILT tool (Schmidt, 2013) attempted to address the automation of the text-to-image links by allowing words to be outlined via a single click, and for entire pages to have their shapes recognised and 
                <hi rend=""italic"">linked</hi> to the corresponding words of the transcription largely automatically. But TILT 1 proved to be limited in the way it could deliver a user interface. The key problem was that in order to automate the linking, it would be necessary to have access to a fully featured suite of image-processing tools. For this, the Java class library was chosen. However, the only way to mediate the automation process was to present the tool as a Java applet, which is notoriously difficult to run in a web context.
            </p>
            <p rend=""CM5"">TILT 2 attempts to address this problem by separating the front-end GUI (link editing and management of the recognition process) from the back-end (page recognition) part. The back-end is accessible via a test interface (Schmidt, 2014), and is designed to act as a faceless service usable by anyone who has a page image and a transcription, and wants to link the two. The front-end GUI is currently under development and contains tools for revising the automatically generated word-shapes and links using standard web technologies.</p>
            <figure>
                <graphic n=""1001"" width=""13.230930555555556cm"" height=""9.768416666666667cm"" url=""Pictures/image1.png"" rend=""block""/>
            </figure>
            <p>Figure 1. Text-to-image links and polygons as invisible GeoJSON overlay.</p>
            <p rend=""CM14"">Rather than use embedded markup in the transcriptions to link word-shapes with the corresponding words in the text, TILT uses GeoJSON (Butler et al., 2014) to describe the page and its recognised shapes, and annotates it with the positions of the words in the text. As shown in Figure 1, both the text-to-image links and the polygons that overlay the image are wholly contained in an invisible GeoJSON document, which can be loaded from a database or generated if required. The transcription can be in HTML or plain text formats. </p>
            <p rend=""CM16"">Page Recognition </p>
            <p>Given the immense variety of manuscript types, the TILT service is divided into clear stages. Each stage feeds the result into later stages, ending in the generation of the word-level links. The stages are:</p>
            <p rend=""List""> 1. Conversion to greyscale. </p>
            <p rend=""List""> 2. Conversion to two-tone. </p>
            <p rend=""List""> 3. Removal of borders. </p>
            <p rend=""List""> 4. Line recognition. </p>
            <p rend=""List""> 5. Word recognition. </p>
            <p rend=""List""> 6. Linking word-shapes to actual words. </p>
            <p rend=""CM14"">Of these, the first three conform closely to the standard processing steps of OCR and use techniques in the Java class library or the Ocropus toolset (Google, 2014).</p>
            <p>The fourth step, however, represents a departure from standard OCR techniques. In order to recognise words, they must first be organised into lines. In printed books, recognition of lines is relatively easy. OCR programs use heuristics to determine text blocks and assume even line spacing. Skewed or warped lines must first be deskewed or dewarped. Since lines may curve or tilt naturally in manuscripts, a general line-recognition algorithm is needed that can cope with both printed and handwritten material. TILT’s approach is to first divide the image into narrow vertical strips. The strips are then reduced to a single average column of pixels, which is smoothed via a moving average algorithm. The peaks in the black pixels of each strip then are taken to correspond to lines. Finally, the peaks are linked up across the page using a greedy approach, by linking the closest points in each pair of adjacent columns first, then progressively more distant points, so long as this does not cause the lines to cross (Figure 2) (BL, 2014).</p>
            <figure>
                <graphic n=""1002"" width=""14.179902777777778cm"" height=""10.216444444444445cm"" url=""Pictures/image2.jpeg"" rend=""block""/>
            </figure>
            <p rend=""CM14"">Figure 2. Line-recognition (Court of Oyer and Terminer for Trial of Attained Traitors record book [1796] from Grenada). </p>
            <p>Once lines have been recognised they can be subdivided into words. (This assumes that texts 
                <hi rend=""italic"">have</hi> word divisions, which may not be the case in certain languages.) The current method uses the two-tone image to identify ‘blobs’ of connected pixels in an image, then surrounds them with a polygon, and finally measures the gaps between polygons (Stamatopoulos et al., 2010). So on a page with 230 words on 30 lines, the number of inter-word gaps to be expected would be 229−29=200. So, assuming that the 200 largest gaps are inter-word gaps, polygons not separated in this way can be merged to form word-shapes. Although this works well for printed texts, with success rates often as high as 98 to 100%, variation in the use of spacing in manuscripts limits its success. An alternative technique (Manmatha and Srimal, 1999) tries to determine an appropriate Gaussian blur to join up blobs into the expected number of words, and then split words on that basis, achieving 86% average word recognition on the test documents. Application of this technique to TILT2 is planned but not yet implemented.
            </p>
            <figure>
                <graphic n=""1003"" width=""13.006916666666667cm"" height=""6.168319444444444cm"" url=""Pictures/image3.png"" rend=""block""/>
            </figure>
            <p rend=""CM14"">Figure 3. Alignment of word-widths in text and image.</p>
            <p>The final linking step is actually the most reliable part of TILT. Since the numbers of word-shapes and words are now known, the approximate width of a single letter can be calculated. This allows the estimation of the pixel-widths of words in the text. Since the sequence of word-shapes in the image must match the sequence of word-shapes in the transcription, the two can be aligned by modifying a classic diff algorithm, such as Ukkonen’s (Ukkonen, 1985). But instead of matching letters in two texts, TILT matches word-widths and allows either several word-shapes in the page image to match one word of the transcription or several words in the transcription to match one word-shape in the image, as shown in Figure 
                <ref target=""REF BMalignment \h \* MERGEFORMAT "" rend=""ref mergeformat"">3</ref>. This can result in polygons being split or merged to correct the alignment. 
            </p>
            <p rend=""heading 3"">Editing Interface</p>
            <p>The editing interface aims to visualise the automatically generated text-to-image links and allow the user to adjust them manually. This uses a number of semi-automatic techniques to speed up the editing process (Figure 
                <ref target=""REF BMsemiautomatic \h \* MERGEFORMAT "" rend=""ref mergeformat"">4</ref>): 
            </p>
            <p rend=""List""> 1. Redistributing words among the word-shapes between any two manually specified anchor points. </p>
            <p rend=""List""> 2. Merging two selected polygons when a word-shape has been incorrectly split by dragging across the shapes. </p>
            <p rend=""List""> 3. Splitting of polygonal shapes by dragging a line across merged words. </p>
            <p>In addition to these techniques, manually adding and deleting points, and adjusting shapes provide a fallback when automatic methods fail. </p>
            <p>Although currently incomplete, the editing interface is anticipated to be ready for demonstration at the conference (Schmidt, 2015).</p>
            <figure>
                <graphic n=""1004"" width=""13.3985cm"" height=""8.038041666666667cm"" url=""Pictures/image4.png"" rend=""block""/>
                <graphic n=""1005"" width=""8.79298611111111cm"" height=""1.2841111111111112cm"" url=""Pictures/image5.png"" rend=""block""/>
                <graphic n=""1006"" width=""6.057194444444445cm"" height=""2.428875cm"" url=""Pictures/image6.png"" rend=""block""/>
                <head>Figure 
                    <anchor xml:id=""BMsemiautomatic""/>4: Semi-automatic editing tools: realignment with anchors (top), merging a split word (middle), splitting two merged words (bottom).
                </head>
            </figure>
            <p>Acknowledgements</p>
            <p>The TILT2 tool was one of two winners of the British Library Labs Competition 2014. The Center for Biodiversity Informatics at the Missouri Botanical Garden is currently supporting further development of the tool.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Al-Hajj, Y. A. A. and Kuster, M. W. </hi>(2013). The Text-Image-Link-Editor: A Tool for Linking Facsimiles and Transcriptions, and Image Annotations. 
                        <hi rend=""italic"">Literary and Linguistic Computing,</hi>
                        <hi rend=""bold"">28</hi>(2): 190–98. 
                    </bibl>
                    <bibl rend=""CM15"">
                        <hi rend=""bold"">BL.</hi> (2014). Endangered Archives: EAP295/2: Collection of Court Records Held by the Grenada Supreme Court Registry [1765–1797]. http://eap.bl.uk/database/overview_item.a4d?catId=140831;r=21726.
                    </bibl>
                    <bibl rend=""CM4"">
                        <hi rend=""bold"">BM.</hi> (2009). Codex Sinaticus. http://www.codexsinaiticus.org/en/manuscript.aspx.
                    </bibl>
                    <bibl rend=""CM12"">
                        <hi rend=""bold"">Butler, H., Daly, M., Doyle, A., Gillies, S., Schaub, T. and Schmidt, C.</hi> (2014). Geojson. http://geojson.org/.
                    </bibl>
                    <bibl rend=""CM15"">
                        <hi rend=""bold"">Dekhtyar, A., Iacob, I. E., Jaromczyk, J. W., Kiernan, K., Moore, N. and Porter, D. C.</hi> (2005). Support for XML Markup of Image-Based Electronic Editions. 
                        <hi rend=""italic"">International Journal on Digital Libraries, </hi>
                        <hi rend=""bold"">6</hi>(1) (February): 55–69
                        <hi rend=""italic"">.</hi>
                    </bibl>
                    <bibl rend=""CM12"">
                        <hi rend=""bold"">Google.</hi> (2014). The OCRopus(tm) Open Source Document Analysis and OCR System. https://code.google.com/p/ocropus/.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Manmatha, R. and Srimal, N.</hi> (1999). Scale Space Technique for Word Segmentation in Handwritten Documents. In Nielsen, M., Johansen, P., Olsen, O. and Wieckert, J. (eds), 
                        <hi rend=""italic"">Scale-Space Theories in Computer Vision</hi>. 
                        <hi rend=""italic"">Lecture Notes in Computer Science</hi>. Vol. 1682. Springer Berlin Heidelberg, pp. 22–33.
                    </bibl>
                    <bibl rend=""CM15"">
                        <hi rend=""bold"">Middell, G. and Wissenbach, M.</hi> (2011). Faust: Multiple Encodings and Diplomatic Transcript Layout. In 
                        <hi rend=""italic"">Philology in the Digital Age Annual TEI Conference</hi>, Wurzburg, 10–16 October 2011, http://www.tei-c.org/Vault/MembersMeetings/2011/ﬁleadmin /04100700/_temp_/TEI_Book_of_Abstracts.pdf, p. 29.
                    </bibl>
                    <bibl rend=""CM12"">
                        <hi rend=""bold"">Reside, D., Lester, D., Porter, D. and Walsh, J.</hi> (2011). Tile–Text Image Linking Environment. http://mith.umd.edu/tile/.
                    </bibl>
                    <bibl rend=""CM15"">
                        <hi rend=""bold"">Schmidt, D. </hi>(2013). Text to Image Linking Tool (TILT). In 
                        <hi rend=""italic"">Digital Humanities 2013 Proceedings</hi>, University of Nebraska, Lincoln, 16–19 July 2013, http://dh2013.unl.edu/abstracts/ab-112.html.
                    </bibl>
                    <bibl rend=""CM6"">
                        <hi rend=""bold"">Schmidt, D.</hi> (2014). TILT2 Test Interface. http://ecdosis.net/tilt/test/post.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Schmidt, D.</hi> (2015). TILT2. http://www.github.com/AustESE-Infrastructure/TILT2.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Seales, W. B., Griﬃoen, J., Kiernan, K., Yuan, C. J. and Cantara, L. </hi>(2000). The Digital Atheneum: New Techniques for Restoring and Preserving Old Documents. 
                        <hi rend=""italic"">Computers in Libraries,</hi>
                        <hi rend=""bold"">20</hi>(2) (February): 26–31. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Stamatopoulos, N., Louloudis, G. and Gatos, B.</hi> (2010). Efficient Transcript Mapping to Ease the Creation of Document Image Segmentation Ground Truth with Text-Image Alignment. In 
                        <hi rend=""italic"">Proceedings of 12th International Conference on Frontiers in Handwriting Recognition, </hi>Kolkata, 16–18 November 2010. IEEE, pp. 226–31.
                    </bibl>
                    <bibl rend=""CM12"">
                        <hi rend=""bold"">Ukkonen, Esko.</hi> 1985. Algorithms for Approximate String Matching. 
                        <hi rend=""italic"">Information and Control,</hi>
                        <hi rend=""bold"">64</hi>: 100–118. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Van Hulle, D., Nixon, M. and Neyt, V.</hi> (2011). Samuel Beckett Digital Manuscript Project. http://www.beckettarchive.org/demo/.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,ocr;text-to-image linking,English,"audio, video, multimedia;digitisation, resource creation, and discovery;english;image processing;linking and annotation;programming;software design and development;visualization"
2429,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Diplomatic History by Data: Understanding Cold War Foreign Policy Ideology using Networks and NLP,,Eun Seo Jo,"paper, specified ""short paper""","<text>
        <body>
            <p>What is diplomatic culture or “rhetoric” and can we measure it?</p>
            <p>This project is an attempt to understand quantitatively the language and structure of U.S. diplomacy as a bureaucratic institution through analysis of a large corpus of diplomatic papers. Since the “cultural turn” of History in the late twentieth century historians have produced cultural interpretations of American diplomacy that highlight gender and racial influences and justifications in diplomatic decision making but there have not been attempts to measure longue duree changes or to quantify them by a standardized measure. 1 This work hopes to fill this gap by introducing a new method of analyzing time-dependent bodies of text. I then apply these methods to a corpus of diplomatic papers to systematically chart changes in concepts and ideology detectable in diplomatic language.</p>
            <p>The project has two aims. First, I am designing a method of measuring how a concept, as a fixed variable, evolves through a temporal corpus. Tools such as GloVe and Dynamic Topic Modeling are two existing approaches that can be used to understanding the linguistic shift and topic distributions over time. 2 I argue, however, that these tools are not adequate yet for time-sensitive tasks such as tracing concepts over time and attempt to design a new method optimized for this task. Second, mindful of the particular characteristics of this corpus as a set of diplomatic papers I want to apply the most appropriate methods of analysis and engage with the existing historiography on the Cold War to engage with claims historians have made about Cold War ideology. I am trying to answer: what is ""Cold War rhetoric"" in high level diplomacy? how did it evolve over the decades? and how did it become propagated within the diplomatic institution? I approach these questions from two methodological angles - networks and NLP. </p>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Dataset</head>
                <p>The dataset for this analysis is the entire text corpus of the Foreign Relations of the United States (FRUS, 1861-1980), a collection of published declassified diplomatic papers. The FRUS collection is hand-curated by librarians at the Office of the Historian to be a representative and comprehensive sample of American diplomatic history and has been a dependable primary source base for historians and social scientists for decades. The document types include telegrams, airgrams, notes, and memoranda among others. Most documents have accompanying meta-data such as the name of the sender, name of the recipient, location of the sender, and date when applicable. So far, I have focused on a subset of this corpus, consisting of all papers from 1948 to 1980, to analyze the high Cold War era. This subset is also what is available in xml format online with hand-tagged meta-data and reliably edited text. About 92,000 documents were available for analysis. 3 A notable caveat of this work is that the corpus, while presumably a representative sample, is still a sample of non-random selection and could carry both deliberate and unintended biases of the librarians. The FRUS is a corpus that becomes publically accessible upon publication and has a fixed audience of social scholars. There is an inevitable feedback loop where the curators respond to the requests of the prime users of the collection such as adjusting the proportion of the most „useful“ types of documents. For instance, as the Vietnam War is a highly contested field of scholarship curators may include a higher ratio of papers surrounding the Vietnam War thus distorting the overall representation of topics by exaggerating the war’s significance. This possible limitation is something to keep in mind throughout the analysis of this corpus.</p>
                <figure>
                    <graphic url=""374/image1.png"" rend=""inline""/>
                    <head>Image A: Example of FRUS document; Stuart as the Ambassador in China to the Secretary of State sent from Nanking on April 25, 1949</head>
                </figure>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Analysis </head>
                <p>Descriptive summaries of the meta-data from the corpus shows several clear distributive patterns. From the total set of all documents 42,000 were correspondences (have to and from agents) from which I parsed all available meta-data (name of sender, name of recipient, location of sender, date) and made inferences on missing data based on historical knowledge. The results show that the Department of State (DOS) as a bureaucratic institution is highly centralized around key actors. Not surprisingly, the prime location of correspondence origin is Washington and its overwhelming predominance indicates that the DOS correspondence system was used for sharing information from the center to the peripheries. Similarly, the top senders of the correspondents were concentrated in high administrative positions – the Secretary of the State (SS), Department of State (DOS) administrative center, and the National Security Advisor (NSA). The individuals that have the highest correspondence authorship are therefore those who have held SS or NSA positions such as Dean Acheson (SS), John Foster Dulles (SS), Henry Kissinger (SS, NSA), and Walt Rostow (NSA). The top recipients of correspondences are also SS and DOS confirming a much bilateral relation between central and peripheral offices. </p>
                <figure>
                    <graphic url=""374/image2.png"" rend=""inline""/>
                    <head>Graph 1A: Location of Correspondence Origin</head>
                </figure>
                <figure>
                    <graphic url=""374/image3.png"" rend=""inline""/>
                    <head>Graph 1B: Top Correspondences sent from</head>
                </figure>
                <figure>
                    <graphic url=""374/image4.png"" rend=""inline""/>
                    <head>Graph 1C: Persons that sent the most correspondences</head>
                </figure>
                <figure>
                    <graphic url=""374/image5.png"" rend=""inline""/>
                    <head>Graph 1D: Top recipients of correspondences</head>
                </figure>
                <p>I then mapped the network structure of correspondences to make the problem of bureacratic “culture” more concrete and visual. In this abstract, I have included images of correspondence networks from two discrete periods – when Kissinger served as Secretary of State from 1973-1977 and when Rostow served as National Security Adviser from 1966-1969. From the maps we can see the overall design of flow of information and transfer of knowledge based on the counts of correspondences and their directions. The maps confirm that indeed the bulk of the correspondence happens bilaterally between top administrative posts and peripheral agents. For instance, the DOS acts as the center point of correspondence for all embassies placed abroad and NSA as that for Washington based lower ranking administrative posts, such as the Under Secretaries of State. Further, two distinct communities emerge within the U.S. diplomatic institution. In both images, one can discern that the DOS and NSA act as distinct and separate focal points while the SS connects the two camps of correspondence and acts as a bridging agent of the two communities. </p>
                <figure>
                    <graphic url=""374/image6.png"" rend=""inline""/>
                    <head>Graph 2A: Map of correspondence networks during the years Kissinger served as Secretary of State (1973-1977)</head>
                </figure>
                <figure>
                    <graphic url=""374/image7.jpeg"" rend=""inline""/>
                    <head>Graph 2B: Map of correspondence networks during the years Rostow served as National Security Advisor (1966-1969)</head>
                </figure>
                <figure>
                    <graphic url=""374/image8.png"" rend=""inline""/>
                    <head>Chart A: Sample of select words’ change of top 10 GloVe neighbors from 1860s and 1950s (‘economy’, ‘empire’, ‘freedom’, ‘european’)</head>
                </figure>
                <p>With this structural framework, I am using a combination of NLP methods to trace given “concepts” to see how they have changed in meaning over time. I identify concepts as individual terms, such as „liberty,“ or as a collection of related terms (topics). Word vectors are the most intuitive method of tracing changes in word meanings. Global vectors (GloVe) and other word vector models suppose a time stagnant corpus so I divided my corpus into decade-long chunks and worked with the assumption that language would not change in usage and meaning significantly enough to matter within ten-year time spans. My results comparing the nearing neighbors by Euclidean Distance of GloVe outputs of select concepts show there is a qualitative difference in conceptual meaning in the 1860s and 1950s. For instance, the concept „freedom“ in the nineteenth century was associated with more poetic and romanticized terms such as „triumph“ and „humanity“ whereas a century later it came to be linked with legal and defensive terms such as „right“ and „safeguard.“ Historians would contextualize this phenomenon in the American Civil War and the Civil Rights respectively. Then a question arises: Were diplomatic agents using terms that reflect the popularized lingo of their time or were their propagating it themselves? Who, in the DOS, initiated the usage of these concepts in such ways? Can we use the networks mapped above for to interpret this? </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Discussion</head>
                <p>This is a work in progress and there is still much work to be done in finding and developing new tools appropriate for time sensitive text data. Given history is a study of change and its significance, it is imperative that we do not assume a static distribution of words across time, eliminating many otherwise useful NLP tools. While Dynamic Topic Modeling considers time as a variable, it constructs a fixed number of topics based on a collection of words making it less favorable for corpora with minimal predictability and pattern as diplomatic papers. Unlike academic journals such as 
                    <hi rend=""italic"">Science</hi> as Blei and Lafferty have applied their modelling on, diplomatic papers are much less consistent in topics. I have also discovered based on my experience of implementing these tools on the FRUS corpus that because in diplomatic papers certain topics predominate discussions at certain dates, I need to be aware of isolating these topics from purely semantic changes. For instance, in the 1940s, “communism” is closest neighbor based on GloVe results to “chinese” or “ccp” because of the Chinese Communist Revolution of 1949, which does not yield any surprising result about the meaning of “communism” in diplomacy. 
                </p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>[1] See how Hoganson introduces a gendered interpretation of U.S. involvement in the Spanish-American and Philippine-American Wars. Hoganson, Kristin L. (2000). Fighting for American Manhood: How Gender Politics Provoked the Spanish-American and Philippine-American Wars (Yale Historical Publications Series). </bibl>
                    <bibl>[2] <hi rend=""bold"">Pennington, J., Socher, R. and Manning, Ch. D.</hi> (2014). GloVe: <hi rend=""italic"">Global Vectors for Word Representation</hi>; David M. Blei and John D. Lafferty (2006) Dynamic Topic Models. </bibl>
                    <bibl>[3] This dataset is available as manually labeled xml formatted files on GitHub 
                        <ref target=""https://github.com/HistoryAtState/frus"">(https://github.com/HistoryAtState/frus</ref>), which makes the corpus more reliable and meta-data accessible than the OCR scanned files of documents from 1861-1947.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,cultural analysis;diplomatic history;linguistic analysis;networks;nlp,English,"historical studies;linguistics;metadata;natural language processing;networks, relationships, graphs;text analysis;xml"
2480,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Crosslingual Textual Emigration Analysis,,Andre Blessing;Jonas Kuhn,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>The presented work describes the adaptation of a Natural Language Processing (NLP) based biographical data exploration system to a new language. We argue that such a transfer step has many characteristic properties of a typical challenge in the Digital Humanities (DH): Resources and tools of different origin and with different accuracy are combined for their use in a multidisciplinary context. Hence, we view the project context as an interesting test-bed for a few methodological considerations.</p>
            <p>In previous work, we developed a web-based application called Textual Emigration Analysis (TEA) (Blessing and Kuhn, 2014). The system consists of two components. The import component automatically extracts emigration paths from a German Wikipedia data set. The user interface component provides several views (interactive map, aggregated tables and underlying textual content) on the extracted data. The whole application was originally designed for German Wikipedia articles and the applied NLP pipeline is based on webservices of the CLARIN-D infrastructure (Mahlow et al., 2014). Later, other German sources of biographical data were included by adapting the import component (ÖBL: Austrian Biographical Dictionary 1815-1950, and NDB: the New German Biography).</p>
            <p>One often requested feature was the adaptation of the system to other languages. In DH research it is important to investigate different sources. As a consequence, textual data may include different languages. In particular, biographical analysis systems benefit if sources of different languages can be analysed. However, the development of such language-sensitive systems still lacks sufficient support. Therefore, the used methods should not require any knowledge of the new target language during development phase. In this work we present the adaptation process for French.</p>
            <p>
                <figure>
                    <graphic url=""480/100000000000049F000000CC98343B24.png""/>
                    <head>
                        <lb/>Figure 1: Parsed French sentence which describes an emigration path.
                    </head>
                </figure>Wikipedia and Wikidata are important resources for the development of language technology tools which also holds for cross-lingual approaches. Wikidata enables a clean mapping between the different language editions of Wikipedia. Following the idea of cross-lingual distant supervision (Blessing 2012), our method consists of three steps. First, we use the results of our TEA tool to find biographical articles that include mentions of emigration paths. Subsequently, Wikidata is used to map those articles to their corresponding French articles, if they exist. Finally, we use anchor points in the text to find comparable sentences.
            </p>
            <p>In most cases, emigration sentences contain geospatial and time expressions (e.g., “He emigrated in 1941 to Switzerland.” ), which can be used to find comparable sentences in the target language. We exploit the hyperlink structure of Wikipedia to recognize geospatial expressions and HeidelTime (Strötgen, 2013) to identify time expressions. The locations can be mapped through Wikidata into the target language and the atomic date representation of HeidelTime enables a simple identification of all matching sentences in the target sentence.</p>
            <p>
                <figure>
                    <graphic url=""480/10000000000005D3000002BC3ED768A8.png""/>
                    <head>
                        <lb/>Figure 2: Screenshot of the TEA web application
                    </head>
                </figure>This results in an annotated corpus in the new target language which can be used as training data for the emigration extraction component . Each sentence of the training corpus is automatically enriched with linguistic annotations (Figure 1) which is necessary to extract features for the emigration extraction component.
            </p>
            <p>Figure 2 depicts our web-based application after integrating the automatically learned French emigration data. Our system can be accessed online: http://clarin01.ims.uni-stuttgart.de/tea</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Blessing, A. </hi>
                        <hi rend=""bold"">and</hi>
                        <hi rend=""bold""> </hi>
                        <hi rend=""bold"">Schüthze,</hi> 
                        <hi rend=""bold"">H.</hi> (2012). Crosslingual Distant Supervision for Extracting Relations of Different Complexity. 
                        <hi rend=""italic"">Proceedings of the </hi>
                        <hi rend=""italic"">twenty-first </hi>
                        <hi rend=""italic"">ACM</hi>
                        <hi rend=""italic""> International Conference on </hi>
                        <hi rend=""italic"">Information and Knowledge Manageme2012</hi>
                        <hi rend=""italic""> (</hi>
                        <hi rend=""italic"">CIKM-12</hi>
                        <hi rend=""italic"">), </hi>ACM, New York, NY, USA.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Blessing, A. </hi>
                        <hi rend=""bold"">and</hi>
                        <hi rend=""bold""> Kuhn, J.</hi> (2014). Textual Emigration Analysis (TEA). 
                        <hi rend=""italic"">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14) European Language Resources Association (ELRA)</hi>, Reykjavik, Iceland.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Mahlow, C., Eckart, K., Stegmann, J., Blessing, A., Thiele, G., Gärtner, M. and Kuhn, J.</hi> (2014). Resources, Tools and Applications at the CLARIN Center Stuttgart<hi rend=""italic"">. Proceedings of the 12th Konferenz zur Verarbeitung natürlicher Sprache (KONVENS 2014).</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Strötgen, J. and Gertz, M.</hi> (2013). Multilingual and Cross-domain Temporal Tagging. 
                        <hi rend=""italic"">Language Resources and Evaluation</hi>. Springer <hi rend=""bold"">47</hi>(2): 269-98.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,biographical data;crosslingual;information extraction;nlp,English,bibliographic methods / textual studies;content analysis;data mining / text mining;linking and annotation;natural language processing;text analysis
2528,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,WebSty - an Open Web-based System for Exploring Stylometric Structures in Document Collections,,Maciej Piasecki;Tomasz Walkowiak;Maciej Eder,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Introduction</head>
                <p>Computer-assisted text analysis is now witnessing the phenomenon of ever-growing computer power and, more importantly, an unprecedented aggregation of textual data. Certainly, it gives us an unique opportunity to see more than our predecessors, but at the same time it presents non-trivial challenges. To name but a few, these include information retrieval, data analysis, classification, genre recognition, sentiment analysis, and many others. It can be said that, after centuries of producing textual data and decades of digitisation them, the scholars now face another great challenge - that of beginning to make good use of this treasure.</p>
                <p>Generally speaking, the problem of large amounts of textual data can be perceived from at least three different perspectives. Firstly, there is a need of asking new research questions that would take advantage of thousands of texts that can be compared. Secondly, one has to introduce and evaluate statistical techniques to deal with vast amounts of data. Thirdly, there is a need of new computational algorithms that would be able to handle enormous corpora, e.g. containing billions of tokens, in a reasonable amount of time. The present study addresses the third of the aforementioned issues.</p>
                <p>Stylometric techniques are known for their high accuracy of text classification, but at the same time they are usually quite difficult to be used by, say, an average literary scholar. In this paper we present a general idea, followed by a fully functional prototype of an open stylometric system that facilitates its wide use with respect to two aspects: technical and research flexibility. The system relies on a server installation combined with a web-based user interface. This frees the user from the necessity of installing any additional software. Moreover, we plan to enlarge the set of standard stylometric features with style-markers referring to various levels of the natural language description and based on NLP methods.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Multi-aspectual Document Representation</head>
                <p>Computing word frequencies is simple for English, but relatively complicated for highly inflected languages, e.g. Polish, with many word forms, resulting in data sparseness. Thus, it might be better first to map the inflected forms onto 
                    <hi rend=""italic"">lemmas</hi> (i.e. basic morphological forms) with the help of a morpho-syntactic tagger, and next to calculate the lemma frequencies.
                </p>
                <p>Most frequent words or lemmas as descriptive features proved to be useful in authorship attribution. However, for some text types or genres they do not provide sufficient information to tell the authors apart, e.g. see (Rygl, 2014). Moreover, in other types of classification tasks, where the goal is to trace signals of individual style, literary style or gender, it usually turns out that they appear on different levels of the linguistic structures. Thus, one needs to enhance text description.</p>
                <p>In practice, every language tool introduces errors. However, if the error level is relatively small and the errors are not systematic (i.e. their distribution is not strongly biased), than the results of such a tool can be still valuable for stylometric analysis. Bearing this in mind, we have evaluated a number of language tools for Polish, and selected types of features to be implemented:</p>
                <list type=""unordered"">
                    <item>length of: documents, paragraphs or sentences (a segmentation tool),</item>
                    <item>morphological features
                        <list type=""unordered"">
                            <item>word forms or tokens and punctuation marks,</item>
                            <item>pseudo-suffixes (last several letters),</item>
                            <item>lemmas (from WCRFT2 morpho-syntactic tagger for Polish (Radziszewski, 2013))</item>
                        </list>
                    </item>
                    <item>grammatical classes
                        <list type=""unordered"">
                            <item>35 grammatical classes defined in the tagset (Szałkiewicz and Przepiórkowski, 2012) of the Polish National Corpus (Przepiórkowski et al., 2012), e.g. pseudo-past participle, non-past form, ad-adjectival adjective; recognised by WCRFT2,</item>
                            <item>parts of speech (by grouping grammatical classes),</item>
                            <item>grammatical categories, e.g. gender, number, person, etc.; (WCRFT2),</item>
                        </list>
                    </item>
                    <item>sequences
                        <list type=""unordered"">
                            <item>lemma sequences (e.g. potential collocations),</item>
                            <item>sequences of grammatical classes (bigrams or trigrams - hints about the grammatical structures),</item>
                        </list>
                    </item>
                    <item>semantic features
                        <list type=""unordered"">
                            <item>semantic 
                                <hi rend=""italic"">Proper Name classes</hi> – recognised by a Named Entity Recogniser Liner2 (Marcińczuk, 2013),
                            </item>
                            <item>
                                <hi rend=""italic"">lexical meanings</hi> – represented by synsets in plWordNet (the Polish wordnet); assigned by Word Sense Disambiguation tool WoSeDon (Kędzia, et al., 2015)
                            </item>
                            <item>generalised lexical meanings – more general meanings from plWordNet, e.g. 
                                <hi rend=""italic""> an animal</hi> instead of 
                                <hi rend=""italic"">a cheetah</hi>,
                            </item>
                            <item>formalised concepts from a formal ontology SUMO that plWordNet is mapped to,</item>
                            <item>lexicographic domains from wordnet.</item>
                        </list>
                    </item>
                </list>
                <p>Semantic features go beyond a typical stylometric description, but allow for crossing borders between the style and the content description.</p>
                <p>There are no features overtly describing the syntactic structure, as the available parsers for Polish express too high level of errors. The set of features can be further expanded by user-defined patterns expressed in the WCCL language (Radziszewski et al., 2011) that can be used to recognise binary relations between words and their combinations.</p>
                <p>WebSty allows for testing the performance of the aforementioned features in different stylometric tasks, several case-studies will be presented on a set of Polish novels.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Processing and Results</head>
                <p>The proposed system follows a typical stylometric workflow which was adapted to the chosen language tools and other components of the architecture (see  Section 4).</p>
                <list type=""ordered"">
                    <item>Uploading a corpus of documents together with meta-data in CMDI format (Broeder et al., 2012) from the CLARIN infrastructure.</item>
                    <item>Choosing the features for the description of documents – done by the users (see Fig. 1).</item>
                    <item>Setting up the parameters for processing (users).</item>
                    <item>Pre-processing texts with the help of language tools.</item>
                    <item>Extracting the features from the pre-processed texts.</item>
                    <item>Calculating feature values.</item>
                    <item>Filtering and/or transforming the original feature values.</item>
                    <item>Clustering the feature vectors representing documents.</item>
                    <item>Extracting features that are characteristic for different clusters.</item>
                    <item>Presenting the results: visualisation or export of data.</item>
                </list>
                <figure>
                    <graphic url=""586/image1.png"" rend=""inline""/>
                    <head>Fig.1 Choice of features GUI</head>
                </figure>
                <p>The step 5 can be performed as: simple counting of words or lemmas, processing and counting annotations matching some patterns or running patterns for every position in a document. This is why a dedicated feature extraction module comes into stage, namely 
                    <hi rend=""italic"">Fextor</hi> (Broda et al., 2013).
                </p>
                <p>Filtering and transformation functions can be built into the clustering packages (see below) or implemented by the dedicated systems, e.g. 
                    <hi rend=""italic"">SuperMatrix</hi> system (Broda and Piasecki, 2013).
                </p>
                <p>The WebSty architecture can be relatively easy adapted to the use of any clustering package. The prototype is integrated with 
                    <hi rend=""italic"">Stylo</hi> – an elaborated clustering package dedicated to stylometric analysis, and 
                    <hi rend=""italic"">Cluto</hi> – a general clustering package (Zhao and Karypis, 2005). Stylo offers very good visualisation functionality, while 
                    <hi rend=""italic"">Cluto</hi> delivers richer possibilities for formal analysis of the generated clusters. The obtained results are displayed by the web browser (see Fig. 2). Users can also download log files with formalised description of the clustering results.
                </p>
                <figure>
                    <graphic url=""586/image2.png"" rend=""inline""/>
                    <head>Fig.2 Stylometric results</head>
                </figure>
                <p>Moreover, features that are characteristic for the description of individual clusters or differentiation between clusters can be identified.  Ten different functions (implemented in Weka
                    <note place=""foot"" xml:id=""ftn1"" n=""1"">
                        <p rend=""footnote text""> http://www.cs.waikato.ac.nz/ml/weka/</p>
                    </note> (Witten et al., 2011) and SciPy packages
                    <note place=""foot"" xml:id=""ftn2"" n=""2"">
                        <p rend=""footnote text""> http://www.scipy.org/</p>
                    </note>), based on mathematical statistics and information theory, are offered. The ranking lists of features are presented on the screen for interactive browsing (Fig. 3) and can be downloaded.
                </p>
                <p>The system has a web-based user interface that allows for uploading input documents from a local machine or from a public repository and enables selecting a feature set, as well as options for a clustering algorithm.</p>
                <figure>
                    <graphic url=""586/image3.png"" rend=""inline""/>
                    <head>Fig.3 Browsing significant features identified for the extracted clusters</head>
                </figure>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Technical Architecture</head>
                <p>Application of language tools is inevitably more complex than calculating statistics on the level of words or letters. Moreover, processing of Polish is mostly far more complex than English (in terms of the processing time and memory used). Thus, higher computing power and bigger resources are required. In order to cope with this, the entire analysis in WebSty is performed on a computing cluster. Users do not need to install any software - which might be non-trivial particularly in the case of the language tools. The system processes the documents using a parallel and distributed architecture (Walkowiak, 2015). </p>
                <p>The workflow is as follows. Input documents are processed in parallel. The uploaded documents are first converted to an uniform text format. Next, each text is analysed by a part-of-speech tagger (we use WCRFT2 for Polish (Radziszewski, 2013)) and then it is piped to a name entity recognizer - Liner2 (Marcińczuk et al., 2013) in our case. After the annotation phase is completed for all texts, the feature extraction module comes into stage, i.e. Fextor (Broda et al., 2013).  Clustering tools requires input data in different formats: sparse or dense matrices, text (ARRF, Cluto format) or binary files (R objects, Python objects). Thus data received from the feature extraction for each input file has to be unified  and converted. The extracted raw features can be filtered or transformed by a range of methods inside the clustering packages or in a system for distributional semantics called SuperMatrix (Broda and Piasecki, 2013).  Finally, the R package Stylo (Eder et al., 2013) or a text clustering tool called Cluto (Zhao and Karypis, 2005) are run to perform exploratory analysis, e.g. multidimensional scaling.</p>
                <p>To prevent the system from overloading and long response time the input data size is limited to 20 files. However, large text collections can be processed, if they are deposited in the dSpace repository.
                    <note place=""foot"" xml:id=""ftn3"" n=""3"">
                        <p rend=""footnote text"">
                            <ref target=""https://clarin-pl.eu/dspace/"">https://clarin-pl.eu/dspace/</ref>
                        </p>
                    </note> All corpora in dSpace can be annotated stored for further processing. Therefore, it is only left to run feature extraction and clustering tools inside WebSty.
                    <note place=""foot"" xml:id=""ftn4"" n=""4"">
                        <p rend=""footnote text""> http://ws.clarin-pl.eu/demo/cluto2.html</p>
                    </note>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Conclusion and future plans</head>
                <p>The paper presented opened, web-based system for exploring stylometric structures in Polish document collections. The web based interface and the lack of the technical requirements facilitates the application of text clustering methods beyond the typical tasks of the stylometry, e.g. analysis of types of blogs (Maryl, 2012), recognition of the corpus internal structure, analysis of the subgroups and subcultures, etc.</p>
                <p>The system is currently focused on processing Polish. However, as the feature representation is quite language independent, we plan to add converters for for other languages. </p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Broda, B., Kędzia, P., Marcińczuk, M., Radziszewski, A., Ramocki, R. and Wardyński, A.</hi> (2013). Fextor: A feature extraction framework for natural language processing: A case study in word sense disambiguation, relation recognition and anaphora resolution. In Przepiórkowski, A., Piasecki, M., Jassem, K., Fuglewicz, P. (Eds.), Computational Linguistics: Applications, Series: 
                        <hi rend=""italic"">Studies in Computational Intelligence</hi>, Vol. <hi rend=""bold"">458</hi>, Springer Berlin Heidelberg, pp. 41-62.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Broda, B. and Piasecki, M.</hi> (2013). Parallel, Massive Processing in SuperMatrix – a General Tool for Distributional Semantic Analysis of Corpora. 
                        <hi rend=""italic"">International Journal of Data Mining, Modelling and Management</hi>, <hi rend=""bold"">5</hi>(1): 1–19.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Broeder, D., Van Uytvanck, D., Gavrilidou, M., Trippel, T. and Windhouwer, M.</hi> (2012). Standardizing a component metadata infrastructure. In: Calzolari, N., Choukri, K., Declerck, T., Doğan, M.U., Maegaard, B., Mariani, J., Moreno, A., Odijk, J., Piperidis, S. (Eds.), 
                        <hi rend=""italic"">Proceedings of LREC 2012: 8th International Conference on Language Resources and Evaluation</hi>. European Language Resources Association (ELRA), pp. 1387-90.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Eder, M., Kestemont, M. and Rybicki, J.</hi> (2013). Stylometry with R: a suite of tools. In: 
                        <hi rend=""italic"">Digital Humanities 2013: Conference Abstracts</hi>. University of Nebraska-Lincoln, NE, pp. 487-89.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kędzia, P., Piasecki, M. and Orlińska, M. J.</hi> (2015). Word Sense Disambiguation Based on Large Scale Polish CLARIN Heterogeneous Lexical Resources. <hi rend=""italic"">Cognitive Studies | Études cognitives</hi>, <hi rend=""bold"">15</hi>: 269-92.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Marcińczuk, M., Kocoń, J. and Janicki, M.</hi> (2013). Liner2 - A Customizable Framework for Proper Names Recognition for Polish. In Bembenik, R., Skonieczny, Ł., Rybiński, H., Kryszkiewicz, M., Niezgódka, M., Intelligent Tools for Building a Scientific Information Platform, Series: 
                        <hi rend=""italic"">Studies in Computational Intelligence,</hi> Springer: Berlin Heidelberg, <hi rend=""bold"">467</hi>: 231-53.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Maryl, M.</hi> (2012). Kim jest pisarz (w internecie?). <hi rend=""italic"">Teksty Drugie</hi>, <hi rend=""bold"">6</hi>: 77-100.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Przepiórkowski, A., Bańko, M., Górski, R. L. and Lewandowska-Tomaszczyk, B. </hi>(Eds.),(2012). 
                        <hi rend=""italic"">Narodowy Korpus Języka Polskiego</hi>. Warszawa: PWN.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Radziszewski, A.</hi> (2013). A tiered CRF tagger for Polish, In Bembenik, R., Skonieczny, Ł., Rybiński, H., Kryszkiewicz, M., Niezgódka, M., Intelligent Tools for Building a Scientific Information Platform, Series: 
                        <hi rend=""italic"">Studies in Computational Intelligence</hi>. Springer Berlin Heidelberg, <hi rend=""bold"">467</hi>: 215-30.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Radziszewski, A., Wardyński, A., and Śniatowski, T.</hi> (2011). <hi rend=""italic"">WCCL: A morpho-syntactic feature toolkit</hi>. In Habernal, I. and Matoušek, V. (Eds.), Text, <hi rend=""italic"">Speech and Dialogue</hi>, Plzen, Springer: Berlin Heidelberg, LNAI 6836, pp. 434–41.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Rygl, J.</hi> (2014) Automatic Adaptation of Author’s Stylometric Features to Document Types, In Sojka, P., Horák, A., Kopeček, I. and Pala, K. (Eds.), 
                        <hi rend=""italic"">Proceedings of 17th International Conference TSD 2014</hi>. Brno, Czech Republic, LNCS 8655, Springer: Berlin Heidelberg, pp. 53-61.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Szałkiewicz, Ł. and Przepiórkowski, A.</hi> (2012). <hi rend=""italic"">Anotacja morfoskładniowa</hi>. In Przepiórkowski et al., pp. 59-96.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Walkowiak, T.</hi> (2015). Web based engine for processing and clustering of Polish texts. In Zamojski, W., Mazurkiewicz, J., Sugier, J., Walkowiak, T., Kacprzyk, J., <hi rend=""italic"">Proceedings of the Tenth International Conference on Dependability and Complex Systems DepCoS-RELCOMEX</hi>, Brunów, Poland, Series: Advances in Intelligent Systems and Computing Springer, Springer Berlin Heidelberg, pp. 515-22.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Witten, I. H., Frank, E. and Hall, M. A.</hi> (2011). Data Mining: Practical Machine Learning Tools and Techniques, Third Edition. <hi rend=""italic"">Series in Data Management Systems</hi>, Morgan Kaufmann.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Zhao, Y. and Karypis, G.</hi> (2005). Hierarchical Clustering Algorithms for Document Datasets. <hi rend=""italic"">Data Mining and Knowledge Discovery</hi>, 
                        <hi rend=""bold"">10</hi>(2): 141-68.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,distributed processing;nlp;stylometry;text classification;web-based system,English,authorship attribution / authority;content analysis;data mining / text mining;information architecture;linguistics;natural language processing;programming;software design and development;stylistics and stylometry;text analysis
2584,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Harvesting History: Democratizing The Past Through The Digitization Of Community History,,Connie Lee Lester,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>In 1993-94 Roy Rosenzweig and David Thelen conducted a set of surveys that resulted in the publication of a seminal work titled 
                <hi rend=""italic"">The Presence of the Past</hi>. The team of historians, museum curators and students surveyed the American public about their interaction with history in order to understand “better ways of connecting academic historians with larger audiences” (Rosenzweig and Thelen, 1998). Their study concluded that Americans’ interest in history related to those events and groups that intersected with their own lives. They universally trusted museums because they could see the objects and images that were the “stuff” of history, but family stories, letters, and photographs defined history for most of the survey participants. With the publication of the survey, Rosenzweig and Thelen entered a historiographic discussion that centered on ways to incorporate history from the “bottom up” and to make the museum experience more interactive (Wallace, 1996; Hayden, 1997; Gardner and LaPaglia, 2004; Lewis, 2005; Gordon, 2010; Anderson, 2012; and Falk and Dierking, 2012).
            </p>
            <p>Almost twenty years after Rosenzweig and Thelen conducted their survey, William G. Thomas, Patrick D. Jones and Andrew Witmer “advanced a movement to democratize and open history” with the founding of the History Harvest Project at the University of Nebraska (Thomas, Jones, and Witmer, 2013). Incorporating their own digital history experience into the classroom, Thomas, Jones, and Witmer developed student projects that operated at the “intersection of digital history and experiential learning.” Beginning in 2010 and continuing to the present, students worked with specific communities to organize History Harvest events at which they photographed objects and scanned images offered by individuals from their personal collections. The photographs and scans were uploaded into an Omeka database and organized into collections and digital exhibits. The project’s aim is to “make invisible archives and stories more visible.” The founders argue that to be successful, a History Harvest must be “organic, grassroots, and local.”</p>
            <p>At the University of Central Florida (UCF), a Regional Initiative for Collecting History, Experiences, and Stories (RICHES™) Project has advanced the concept of student-public engagement through the use of History Harvests to collect “invisible archives and stories” by making the presentation of the digitized material interactive and democratizing the ability of site users to utilize the data in the collections to develop their own narratives. This transformational addition to the efforts to democratize history provides the user with the data, the tools to analyze the data, and the digital space to organize the data in order to create a narrative.</p>
            <p>Working in Florida, students have access to a history that celebrates more than 500 years of interaction between Native Americans, Europeans, and Africans. The geographical position of Florida with its extensive coastline means that the history of the area has always been global. Interaction between the people of Caribbean Islands and those living in Florida pre-dated European claims for the region and expands the scope of scholarship. Successive waves of Spanish, French, British and American settlers altered the landscape and created new borderlands that continued until well into the twentieth century. Currently, the third most populous state in the nation, with an economy that ranges from agriculture to tourism to space exploration and high tech industries, Florida’s history is deep, diverse, and largely unexplored. </p>
            <p>Students in graduate classes in Public History at UCF have completed three History Harvests and three additional Harvests are planned for 2016. The largest of the completed Harvests was accomplished through a partnership between the Introduction to Public History class, the Oviedo (Florida) Historical Society and a RICHES partnership tech firm, EZ-Photo-I/O Track. The students worked with the historical society to understand the history of the 137-year-old agricultural community that was transformed by the founding of UCF and the influx of thousands of students and faculty only three miles from the center of town. Based on the information provided by the historical society and a National Endowment for the Humanities-funded, society-published book (Adicks and Neely) the students conducted sixteen oral histories with descendants of the original settlers. In order to generate community awareness of the upcoming History Harvest students participated in public events and obtained permission to post information on the community and historical society social media sites. Building on the planning documents produced by previous student-led History Harvests, they organized the release forms, prepared to obtain the stories associated with items brought for scanning, and worked with EZ-Photo staff to facilitate the scanning process. More than 500 images, documents, three dimensional items, pamphlets, and scrapbooks were scanned on the day of the History Harvest. They told the story of citrus grove owners, farm laborers and fruit packers, migrants and immigrants to the community, schools, churches, and community social life. </p>
            <figure>
                <graphic url=""519/image1.jpeg"" rend=""inline""/>
                <head>Oviedo History Harvest, 2015</head>
            </figure>
            <p>The UCF/RICHES History Harvest pushed the use of the collected data to a new, level by placing the material in RICHES Mosaic Interface™ 
                <ref target=""https://richesmi.cah.ucf.edu"">https://richesmi.cah.ucf.edu</ref> . RICHES MI was initially created with the help of a National Endowment for the Humanities Startup Grant, and is an Omeka archive with multiple custom plugins and a front end that is a unique mechanism for searching the archive. RICHES MI users can search the database using natural language, tags, and topics, and browse by collection categories to maximize their search results; analyze the results of their search using the RICHES-developed “Connections” feature to show the relationship between the returned item and other items in the database; and visualize their results on a Google map and through digital exhibits, map overlays, and other visualizations. Users can save their search results in a Bookbag where they can annotate individual items, store items in folders, map the items collected in folders on a Google map and a timeline, and analyze the connections between items in the folders. They can also develop a narrative using the story board feature to organize their data. Finally, a search in the Omeka archive provides the user with links to digitized primary and secondary sources relevant to the individual object as well as links to other databases or websites that might be useful to the researcher. 
            </p>
            <figure>
                <graphic url=""519/image2.jpeg"" rend=""inline""/>
                <head>Schematic of RICHES Functionality</head>
            </figure>
            <p>Student Harvests, like their counterparts elsewhere, also create digital exhibits to organize the collected images and documents into an interpretative narrative. Faculty and students gain much through their participation in the History Harvests. Students acquire digital skills in metadata writing, archiving and exhibit creation; they learn to conduct oral histories and plan events; and they develop an understanding of the connection between local history and national/global issues. Each student’s work is cited by name in the metadata and in digital exhibits. Faculty participants are organizing larger projects (i.e. Parramore Project and Glass Bank Project) that include History Harvests and are expected to result in scholarly and pedagogical publications. </p>
            <figure>
                <graphic url=""519/image3.png"" rend=""inline""/>
                <head>History Harvests in Omeka</head>
            </figure>
            <p>The next three projects, which are funded by a National Endowment for the Humanities “The Common Good: The Humanities in the Public Square” grant, will push the boundaries of History Harvests further: students will be using the oral histories and the collections obtained through History Harvests to produce a documentary film on an African American community’s struggles with urban development, to understand a coastal community’s relationship to an iconic building, and to provide evidence for two MA thesis projects dealing with Orlando’s LGBTQ community. The African American community project pairs students in an undergraduate history class with students at a predominantly black high school to explore changes that the construction of public buildings, entertainment and sports venues, an interstate highway, and the proposed expansion of UCF to a downtown campus have had on what was once a vibrant and cohesive African American community. The material collected will be used by a second undergraduate Honors class to produce a documentary film. This will be the fourth student produced film supervised by the course faculty. The second History Harvest will focus on an iconic building, the so-called Glass Bank, in Cocoa Beach, Florida. Constructed in the 1960s, the Glass Bank was demolished in 2014, but not before a partnership between RICHES and the Institute for Simulation and Training produced a 3D scan of the building. Recreated as it existed in 1963, the Glass Bank will serve as a vehicle for collecting images, artifacts and stories about the building as a center of community activity and economic development. See the video of the 3D scan 
                <ref target=""https://www.youtube.com/watch?v=DtVETzCND4M"">https://www.youtube.com/watch?v=DtVETzCND4M</ref> Finally, students will work with the GLBT Virtual Museum and the Orange County Regional History Center to harvest images, oral histories, and artifacts that tell the story of the gay destination, Parliament House, and the effect of the HIV/AIDS epidemic in Orlando. 
            </p>
            <p>The RICHES project builds upon the insights of Public History scholarship and Digital scholarship to understand the complex past of local communities and to connect those histories to larger narratives. Using History Harvests to build a bridge between individuals, families, and local museums and historical societies and the academic community of students and faculty provides a service to the public and experiential learning for students. Awareness of the interaction between local and the national/global events enables students to consider their own scholarship in new ways. Placing the material harvested in an interactive database like RICHES Mosaic Interface democratizes the history collected by providing context, analytical tools, and space for organizing personal narratives. </p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl><hi rend=""bold"">Wallace, M.</hi> (1996). 
                        <hi rend=""italic"">Mickey Mouse History and Other Essays on American Memory</hi>. Philadelphia, PA.
                    </bibl>
                    <bibl><hi rend=""bold"">Hayden, D.</hi> (1997). 
                        <hi rend=""italic"">The Power of Place: Urban Landscapes as Public History</hi>. Cambridge, MA.
                    </bibl>
                    <bibl><hi rend=""bold"">Adicks, R. and Donna N.</hi> (1998). 
                        <hi rend=""italic"">Oviedo: Biography of a Town</hi>. Ovideo, FL.
                    </bibl>
                    <bibl><hi rend=""bold"">Rosenzweig, R. and Thelen, D.</hi> (1998). 
                        <hi rend=""italic"">The Presence of the Past: Popular Uses of History in American Life</hi>. New York.
                    </bibl>
                    <bibl><hi rend=""bold"">Gardner, J. B. and LaPaglia, P. S. (eds).</hi> (1999), (2004).
                        <hi rend=""italic"">Public Essays from the Field</hi>. Malabar, FL.
                    </bibl>
                    <bibl><hi rend=""bold"">Lewis, C.</hi> (2005). 
                        <hi rend=""italic"">The Changing Face of Public History: The Chicago Historical Society and the Transformation of an American Museum</hi>. DeKalb, IL.
                    </bibl>
                    <bibl><hi rend=""bold"">Gordon, T. S. </hi>(2010). 
                        <hi rend=""italic"">Private History in Public: Exhibition and the Settings of Everyday Life</hi>. Nanham, MD.
                    </bibl>
                    <bibl><hi rend=""bold"">Anderson, G. (ed).</hi> (2012). 
                        <hi rend=""italic"">Reinventing the Museum: The Evolving Conversation on the Paradigm Shift</hi>. Lanham, MD.
                    </bibl>
                    <bibl><hi rend=""bold"">Falk, J. H. and Dierking, L. D.</hi> (2012). 
                        <hi rend=""italic"">The Museum Experience Revisited</hi>. Walnut Creek, CA.
                    </bibl>
                    <bibl><hi rend=""bold"">Thomas, W. G., Jones, P. D. and Witmer, A.</hi> (2013). “History Harvests: What Happens When Students Collect and Digitize the People’s History?” 
                        <hi rend=""italic"">Perspectives on History: The Newsmagazine of the American Historical Association</hi> (January 2013) 
                        <ref target=""https://www.historians.org/publications-and-directories/perspectives-on-history/january-2013/history-harvests"">https://www.historians.org/publications-and-directories/perspectives-on-history/january-2013/history-harvests</ref> (accessed October 15, 2015).
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,community;democratization;history harvest;riches;student projects,English,digital humanities - diversity;digital humanities - pedagogy and curriculum;folklore and oral history;historical studies;teaching and pedagogy
2674,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,A Tool for NLP-Preprocessing in Literary Text Analysis,,Nils Reimers;Fotis Jannidis;Stefan Pernes;Steffen Pielström;Isabella Reger;Thorsten Vitt,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>The possibilities for widening the spectrum of research questions by adopting new computational methodology seem to be almost unlimited for literary scholars with considerable programming skills. Researchers with little or no such skills, however, have to rely on user-friendly tools. Simple word counts are still among the most common, and admittedly often very useful features used in computational text analysis. Usually, linguistic annotations are needed for using more complex features in the analysis of style or content of a literary text. For example, a researcher might want to investigate style in terms of syntactic preferences by applying stylometric analysis on part-of-speech tag n-grams, to run topic modelling on specific word types only or to characterize the way an author describes figures by extracting all the adjectives that refer to a named entity. All these examples require of the scholar to first extract linguistic information from the text and use that information to define complex features. </p>
            <p>Computer linguists have developed several tools for the various tasks of natural language processing (NLP) that can automatically analyze a digital text and annotate it with such information. In the present spectrum of solutions for NLP tasks, there is a gap between tools for rather simple tasks and full programming frameworks which require significant programming skills. The one end of the spectrum is represented by WebLicht,
                <note place=""foot"" xml:id=""ftn1"" n=""1"">
                        <ref target=""http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/Main_Page"">http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/Main_Page</ref>
                </note> a web service that allows users to upload and process single files very comfortably. On the other end are GATE,
                <note place=""foot"" xml:id=""ftn2"" n=""2"">
                        <ref target=""https://gate.ac.uk/"">https://gate.ac.uk/</ref>
                </note> NLTK
                <note place=""foot"" xml:id=""ftn3"" n=""3"">
                        <ref target=""http://www.nltk.org/"">http://www.nltk.org/</ref>
                </note>, BookNLP
                <note place=""foot"" xml:id=""ftn4"" n=""4"">
                        <ref target=""https://github.com/dbamman/book-nlp"">https://github.com/dbamman/book-nlp</ref>
                </note> and the Darmstadt Knowledge Processing Repository (DKPro).
                <note place=""foot"" xml:id=""ftn5"" n=""5"">
                        <ref target=""https://www.ukp.tu-darmstadt.de/research/current-projects/dkpro/"">https://www.ukp.tu-darmstadt.de/research/current-projects/dkpro/</ref>
                </note>
            </p>
            <p>DKPro provides a programming framework in which many such NLP tools can be combined into an analysis pipeline. The pipelining approach is especially useful, often even necessary, when one NLP tool needs the annotations provided by another NLP tool in advance for extracting more complex linguistic features. DKPro thus provides access to tools like sentence splitters, tokenizers, part-of-speech taggers, named-entity recognizers, lemmatizers, morphological analyzers and parsers in many languages.
                <note place=""foot"" xml:id=""ftn6"" n=""6""> Not every kind of tool is available in all languages; it depends on the native support of the tools, not on the framework provided by DKPro.
                </note>
            </p>
            <p>While making NLP significantly easier by integrating many NLP tools into a single framework, the use of DKPro still requires a substantial knowledge of technologies like UIMA, Java and Maven. To further lower the skill threshold for literary scholars to use complex NLP output in computational text analysis, DARIAH-DE (the German branch of the European project Digital Research Infrastructure for the Arts and Humanities, funded by the German Federal Ministry of Education and Research) developed the DARIAH-DKPro-Wrapper (DDW).
                <note place=""foot"" xml:id=""ftn7"" n=""7"">
                        <ref target=""https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper"">https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper</ref>
                </note> The DDW bundles a pipeline with a set of commonly used NLP components into a java program to be executed with a single command. As DKPro in general, the wrapper provides transparent access to a whole set of different NLP tools which are downloaded as needed behind the scenes. Command line options and configuration files allow users a considerable degree of control over the pipeline and its components, giving partial access to DKPro functionality without requiring any programming knowledge. The DDW also solves the problem of different input and output formats of the tools, offering a unified access. Therefore, the DDW positions itself in between the two ends of the aforementioned spectrum: It runs locally, allows for the processing of multiple files and can be configured to a considerable extent to one’s own needs. Whereas the user of classical DKPro is a UIMA programmer, the DDW can be used by anybody who can copy a command into the command line. Nonetheless, the DDW in some cases offers more features than other more advanced solutions, as DKPro supports more tools and languages. It also integrates Stanford NLP and supports the highly efficient Treetagger. A list of components available for both DKPro and the DDW can be found of the DKPro project page.
            </p>
            <p>Furthermore, the DDW stores its output in a tab-separated plain text format inspired by CoNLL2009.
                <note place=""foot"" xml:id=""ftn8"" n=""8"">
                        <ref target=""https://ufal.mff.cuni.cz/conll2009-st/task-description.html"">https://ufal.mff.cuni.cz/conll2009-st/task-description.html</ref>
                </note> The format provides information on paragraph id, sentence id, token id, token, lemma, POS, chunk, morphology, named entity, parsing information and more. This format can be comfortably accessed in common scripting languages for further analysis, i.e. it can be directly read as a dataframe object in R or Python Pandas; it can even be opened in a common datasheet editor like Microsoft Excel.
            </p>
            <p>Scripts connecting the output format to popular text analysis tools like the R package 
                <hi rend=""italic"">stylo</hi>
                <note place=""foot"" xml:id=""ftn9"" n=""9"">Eder, Maciej, Mike Kestemont, and Jan Rybicki. ""Stylometry with R: a suite of tools."" 
                        <hi rend=""italic"">Digital Humanities 2013: Conference Abstracts</hi>. 2013. For the software see: 
                        <ref target=""https://sites.google.com/site/computationalstylistics/stylo"">https://sites.google.com/site/computationalstylistics/stylo</ref>
                </note> are currently under development. Dariah also prepared some tutorials explaining how to use the wrapper and showing the use of the output format in research in three use cases.
                <note place=""foot"" xml:id=""ftn10"" n=""10"">
                    <p>
                        <ref target=""https://rawgit.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/tutorial.html"">https://rawgit.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/tutorial.html</ref>
                    </p>
                </note>
            </p>
            <p>This poster will present the DDW and its file format as a new and comfortable means of providing linguistic annotations, thus significantly lowering the threshold for using complex NLP-based features in computational literary analysis.</p>
        </body>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,dkpro;nlp pipeline;user friendliness,English,natural language processing
3877,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Limiter L’Impact Des Erreurs OCR Sur Les Représentations Distribuées De Mots,https://dh2017.adho.org/abstracts/183/183.pdf,Axel Jean-Caurant;Cyrille Suire;Vincent Courboulay;Jean-Christophe Burie,poster / demo / art installation,"Les chercheurs en Humanites numeriques intéresses par l'analyse de grands corpus textuels utilisent de nombreuses méthodes et outils issus de domaines informatiques comme le traitement du langage naturel (Piotrowski, 2012) ou l'analyse de réseaux (Lemercier, 2005). Des methodes récentes fondées sur les réseaux de neurones présentent egalement un interêt majeur. Word2Vec est une méthode qui a grandement facilité l'utilisation de tels modules (Mikolov, 2013). Les différentes optimisations apportees permettent, très simplement, d'entraîner un module sur de grandes quantités de donnees en utilisant un simple ordinateur de bureau. Le code source a ete largement diffuse et a rendu cette methode très populaire, notamment parmi les chercheurs en Humanites numériques. Hamilton a par exemple montré l'interêt de ces modules pour analyser l'évolution de certains mots du langage au cours du temps (Hamilton, 2016). Ces méthodes peuvent également être utilisées à d'autres fins. En effet, de nombreux corpus utiles aux Humanités numériques sont issus de processus de reconnaissance de carac-téres (OCR). Malheureusement, ces processus gé-nérent trés souvent des erreurs, en particulier quand les documents analysés sont de mauvaise qualité (documents anciens ou mal numérisés par exemple). Ces erreurs touchent notamment les entités nommées comme les noms de lieux ou de personnes, particuliérement intéressants pour les chercheurs (Gefen, 2015). Ces erreurs ont un impact majeur sur l'accés a l'information car elles peuvent empécher d'accéder a toutes les occurrences d'un mot d'intérêt.

Dans ce poster, nous présentons la méthode que nous avons développée pour étendre l'usage de Word2Vec a l'identification des erreurs OCR dérivées d'entités nommées. Aprés avoir entraîné un modéle sur un corpus donné, chaque mot est associé a un vecteur représentatif. Il devient alors possible de comparer les vecteurs pour extraire des relations morphologiques ou sémantiques entre les mots. On peut par exemple calculer la distance cosinus qui sépare deux mots dans l'espace vectoriel du modéle. Si, au sein du corpus, ces mots apparaissent dans des contextes similaires, la distance qui les sépare sera faible. Or, une entité nommée, bien que mal reconnue par le processus OCR, apparaît souvent dans le méme contexte que l'entité originale. En combinant cette distance, qui agit sur les vecteurs, avec une distance d'édition sur les mots, nous pouvons identifier des mots proches sémantiquement et qui possèdent beaucoup de caractères en commun. Cette analyse produit ainsi une liste de termes qui ont toutes les chances d'étre des entités mal reconnues par le processus de reconnaissance de caractères.


Figure 1: Expérience menée par Bjerva et. al., qui présente les similarités entre différents personnages et quelques grands concepts. Plus une cellule est rouge, plus la similarité est importante.

Une fois les erreurs identifiées, il est possible de s'intéresser a une entité nommée particulière. Sur la base des résultats précédents, nous proposons la construction d'un nouveau vecteur associant le vecteur de l'entité originale et les vecteurs représentatifs des erreurs. Ce nouveau vecteur est le résultat de la combinaison linéaire des vecteurs du mot original et des erreurs OCR. Pour modérer l'importance des vecteurs dans la combinaison, ces derniers sont pondères selon le nombre d'occurrences du terme correspondant dans le corpus.


gothness greekness

liberty

antiquity romanness

Figure 2: Reproduction de l'expérience menée par Bjerva et. al., avec notre modèle modifié.


Figure 3: Comparaison des similarités Personne/Concept entre le modèle de Bjerva et. al. et notre modèle modifié. Chaque cellule représente la valeur absolue de la différence de similarité entre les deux modèles. Les cellules rouges présentent le plus de différences.

Nous avons expérimenté notre méthode en reproduisant l'expérience men^e par Bjerva et. al. (Bjerva, 2015). Ces derniers se sont interesses aux relations qu'entretiennent différentes personnalités du VIème siecle avec de grands concepts (Modernite, Liberte, Gothique, ...). Ils ont utilisé Word2Vec pour entraîner un modele sur environ 11 000 textes latins, pour ensuite comparer les distances qui separent les personnes des concepts dans l'espace vectoriel du modele (voir figure 1). Nous avons utilise notre méthode pour calculer, pour chaque personne d'intérét, un nouveau vecteur représentatif prenant en compte les différentes erreurs OCR identifiées. Les distances entre personnes et concepts au sein de notre modéle modifié sont présentées dans la figure 2. Pour plus de clarté, les deux modéles sont comparés dans la figure 3. On peut par exemple observer qu'Odovacer, la personne pour qui les différences sont les plus grandes, est assez peu citée dans le corpus. Notre méthode a cependant identifié de nombreuses erreurs OCR qui ont révélé des informations inconnues au seul vecteur de l'entité originale.

La méthode présentée ici permet d'identifier de potentielles erreurs OCR sur les entités nommées au sein d'un corpus. La prise en compte de ces erreurs peut avoir un impact non négligeable sur le modèle et donc sur les analyses qui en decoulent. Cela semble en particulier vrai pour les entites nommées peu présentes dans un corpus.

Bibliographie
Bjerva, J. and Praet, R. (2015). ""Word Embeddings Pointing the Way for Late Antiquity."" LaTeCH 2015: 53.

Gefen, A. (2015). Les enjeux épistémologiques des humanités numériques. Socio. La nouvelle revue des sciences sociales, 4: 61-74.

Hamilton, W. (2016). ‘‘Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.'' Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 1.

Lemercier, C. (2005). Analyse de reseaux et histoire. Revue D'histoire Moderne et Contemporaine(2): 88-112.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. and Dean, J. (2013). ‘‘Distributed representations of words and phrases and their compositionality.'' Advances in Neural Information Processing Systems. pp. 3111-3119

Piotrowski, M. (2012). Natural Language Processing for Historical Texts. (Synthesis Lectures on Human Language Technologies 17). San Rafael, CA: Morgan & Claypool.",txt,Creative Commons Attribution 4.0 International,,post-ocr error correction;word embeddings,French,"computer science;corpora and corpus activities;data mining / text mining;digitisation, resource creation, and discovery;information retrieval;library & information science;natural language processing;text analysis"
4088,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Optical Character Recognition with a Neural Network Model for Coptic,https://dh2017.adho.org/abstracts/502/502.pdf,Kirill Bulert;So Miyagawa;Marco Büchler,"paper, specified ""short paper""","Introduction
Optical character recognition (OCR) is the process of extracting text from images. The final results are machine readable versions of the original images. Nowadays every modern scanner comes with some kind of OCR, but the results may not be satisfying when the OCR is applied to historical texts, that

1.    do not use standard fonts,

2.    are not printed by a machine,

3.    have varying paper and font quality.

Furthermore, historical texts are not passed down through the centuries in their entirety but rather contain lacunae and fragmentary words. This makes automatic post-correction more difficult on historical texts than on modern ones.

We used two tools to create language- and even document- specific recognition patterns (or so-called models) to recognize printed Coptic texts. Coptic is the last stage of the pre-Arabic, indigenous Egyptian language. It was used to create a rich and unique body of literature: monastic, “Gnostic,” Manichaean, magical and medical texts, hagiographies, and biblical and patristic translations. We found that Coptic texts have properties which make them excellent candidates for reading by computers. The characters can easily be distinguished due to their limited number and the fact that almost all the hand-written texts exhibit characters with highly consistent forms.

Related Work
The process of digitizing historical documents can be split up into at least three major steps: (1) pre-processing, (2) text prediction (OCR), and (3) post-processing or correction.

Although many works already tackled subproblems (He et al, 2005; Gupta et al, 2007; Kluzner et al, 2009), Springman et al.(2014) presented the first complete approach containing all major steps for historical Greek and Latin books.

The first OCR results for printed Coptic texts were achieved by Mekhaiel (see Moheb's Coptic Pages) by using Tesseract to create a model for Coptic texts. Tesser-act assumes that the image was printed with a standardized font. Although it can be trained to use many different fonts, creating a general model that would satisfy scholars is not feasible. In the end, this model is sufficient for pure printed Coptic texts, but creates a lot of noise for texts with mixed languages or annotations. Such drawbacks can be easily overcome by checking against a dictionary, but historical languages often do not have a dictionary that could be considered complete, and the texts might only be fragments that require further analysis.

The recognition itself is performed by either Ocropy (Breuel, 2008) or Tesseract. Potentially, all character-based texts can be recognized. However, even though Mekhaiel provided a Coptic model for Tesseract, we were never able to achieve satisfying results on images which were not pre-processed.

Data Used
For training and testing, an expert on Coptic created a clean version and transcription of Kuhn’s 1956 edition “Letters and sermons of Besa.” This will also be made available to the interested public.

Besa is a fifth-century abbot of a monastery in Upper Egypt and Coptic writer, whose literary legacy consists mainly of letters to monks and nuns on questions of monastic life and discipline.

Simplified pages were created to find the limits of the trained models with optimal input data. Since creating simplified pages consumes a lot of time, we consider this task as impractical for real use scenarios. Nevertheless, the results on these simplified pages show the best possible prediction.

In Fig. 1 all characters and symbols that are going to be removed are marked red. The resulting simplified image can be seen in Fig. 2. By procedure, adjacent characters that are supposed to form one word are cut apart by gaps. Those gaps are going to be predicted differently by the two OCR engines.

neTMANoyoy. Mne-e-ooy nakim an ¿flneqm 14 :

xnx BHCA

[Fragment 35] A DENUNCIATION OF AN ERRING NUN

.... LMjA......iAjYU* ,Nj ...OH [HIM n] l6jTN[AA]qj-

[a£o]m e^pAi e[jclo>- h him ne[TN]AKToq ng [gJygiphnh •

Fig.1, Original Image (excerpt), red elements are missing in the simplified version

neTNANoyoy Mne-aooy hakim an ¿MneqHi ATTA BHCA

MA    A yo> N    OH N1M FI G TN AA Of

A£O M ej>pAi 6 A (I) H NIM ne TN AKTOq NG G yGipHNH

Fig. 2, Simplified version (excerpt)

Methodology
There are two methods to train for Coptic texts:

(i)    Tesseract needs a font as the baseline and matches the found letters against this font. This can be highly convenient since fonts do not show many variations within a single document. Additional fonts can be incorporated into the model with the drawback that the prediction requires more computational time. So far, we have used Mekhaiel's original model, and we are currently experimenting by adding document-specific characters to increase the accuracy of a single document.

(ii)    Ocropy, on the other hand, does not require a font. For training, it requires only a partial transcription: the ground truth. This transcription is used to train a neural network that can recognize the characters. Ocropy's drawback is that the ground truth cannot just be the alphabet but requires multiple pages of transcribed text with a representative letter frequency. Ocropy's training process is measured in iterations. Springmann proposed working with at least 30,000 iterations (a comment made by Springmann in a

private conversation, based upon his own experience).

For this contribution, we created an Ocropy model with a training set containing approximately 5,000 characters. This set includes superlinear strokes, braces and foreign characters which are not part of the Coptic alphabet.

Multilingual documents and documents containing foreign characters are considered complex. Stains on the document, bad image quality, and annotations like line numbers increase the complexity of documents as well. We, therefore, created special pages with reduced complexity. Our original pages were stripped offline numbering and footnote annotations. In the “clean” version, all foreign characters, punctuations and annotations inside the text were removed, leaving us with a pure Coptic text. We further stripped all clean versions of superlinear strokes, giving us the simplified version.

For testing, the selected pages were transcribed with corresponding 'original', 'clean' and 'clean without stroke or simplified' ground truths. All results were compared with 'Ocreval' (Baumann 2014)[9] against the ground truth.

Results
Prediction

Mekhaiel's original Tesseract model produced the best results on simplified pages with an accuracy of ~95%, while our Ocropy model performed better on the more complex pages. On the other hand, the Tes-seract tends to produce predictable errors. Character will, for example, always be recognised as □; while, Ocropy produces unpredictable errors. Although our Ocropy model is less accurate on simplified pages, it

surpasses Tesseract on noisier pages.

Comparison of Mekhaiels Tesseract model and our Ocropy model


Fig. 3, OCR accuracy on different complexity levels

Costs

We measured that a skilled person needs roughly 10 minutes for manual transcription and 5 additional minutes for proofreading per page. Ocropy's models are built on top of transcribed images. Therefore, an initial ground truth is always required. Training with Ocropy does not require further human interaction but consumes up to two days of CPU power (Core i3/5

2.4GHz/3.2GHz, 8GB RAM, SSD), training cannot be run in parallel. Tesseract's training process, on the other hand, depends on the font extraction. We do not have enough data to estimate the time required to extract a font from an image. Both predictions still have to be checked manually, which can take up to 5 minutes. With clean pages and reduced proofreading time per page, Fig. 4 shows an optimal OCR workload reduction (red lines) in comparison to manual transcription (yellow line). A more realistic scenario is mentioned in the discussion.


Fig. 4, workload comparison

Discussion
Our result shows that Tesseract outperforms Ocropy on simplified pages in terms of accuracy and amount of human work. Unfortunately, in a realistic scenario, the pictures will always contain some of the previously described complexities. Pre-processing of the data is, therefore, essential to obtain good results. In Figure 4, we also computed a more realistic scenario (blue lines) with a higher workload on pre-processing for Tesseract. It shows that creating an Ocropy model pays off for larger and more complex document sets.

Tesseract's overall acceptable performance is based on the fact that no model has to be trained. As creating and testing a model can consume more time than manual transcription and proofreading, the creation of clean images might still be less efficient than the manual approach even if a model can be reused.

As long as cleaned images images are one of the desired results, our works shows that the workload can be reduced by half. This applies especially to Ocropy, since ground truth creation and training fit into the normal transcription workflow.

Unicode ambiguities, which unfortunately result in encoding differences, require normalization and filtering. Otherwise, these encoding differences, which would not be seen as errors by humans, will be counted. Due to the same ambiguities, it is easy to mix characters from different code pages, especially on multilingual texts and text markings. It is, therefore, recommended that one use only corresponding code pages, especially with multilingual models. Tests with models containing multilingual fonts will be considered in further studies.

Conclusion
OCR of historical documents continues to be a hard problem, but we showed that utilizing OCR for the transcription of Coptic texts can reduce the overall workload. Since even the simplest images could not be recognized with 100% accuracy, further gains can only be achieved by better pre- and post-processing techniques.

A bigger workload reduction can be achieved by model reuse. However, no Coptic OCR models have been published besides Mekhaiel's. Therefore, we highly recommend publishing models alongside the transcription and suggest that it is possible to predict almost all well-preserved texts.

Also, although our model was able to partially predict multilingual texts, further studies are required. Multilingual texts require a specialized training process to compensate for the small numbers of foreign words.

Bibliography
He, J., Do, Q. D. M., Downton, A. C., and Kim, J. H. (2005) “A comparison of binarization methods for historical archive documents,” in Eighth International Conference on Document Analysis and Recognition (ICDAR05), p. 538542 Vol. 1.

Gupta, M. R., Jacobson, N. P., and Garcia, E. K. (2007). “{OCR} binarization and image pre-processing for searching historical documents,” Pattern Recognit., vol. 40, no. 2, pp. 389-397.

Kluzner, V., Tzadok, A., Shimony, Y., Walach, E., and An-tonacopoulos, A. (2009) “Word-Based Adaptive OCR for Historical Books,” in 2009 10th International Conference on Document Analysis and Recognition, pp. 501-505.

Springmann, U., Najock, D., Morgenroth, H., Schmid, H., Gotscharek, A., and Fink, F. (2014) “OCR of Historical Printings of Latin Texts: Problems, Prospects, Progress,” in Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, pp. 71-75.

Mekhaiel, M. S. (n.d.) “Moheb’s Coptic Pages.” [Online]. Available: http://www.moheb.de/. [Accessed: 01-Nov-2016].

“Tesseract    OCR.”    [Online].    Available:

https://github.com/tesseract-ocr. [Accessed: 01-Nov-2016].

“Ocropy.”    [Online].    Available:

https://github.com/tmbdev/ocropy. [Accessed: 13-Dec-2016].

Breuel, T. M. (2008) “The OCRopus open source OCR system,” Proc. SPIE 6815, Doc. Recognit. Retr. XV, 2008.

Baumann, R. (2014) “OCR Evaluation Tools.” [Online]. Available: https://github.com/ryanfb/ancientgreekocr-ocr-evaluation-tools. [Accessed: 01-Nov-2016].",txt,Creative Commons Attribution 4.0 International,,coptic;ocr;ocropy;tesseract,English,computer science;data mining / text mining;historical studies;image processing;interdisciplinary collaboration;near eastern studies;philology;text analysis
4193,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Transkribus: Handwritten Text Recognition technology for historical documents,https://dh2017.adho.org/abstracts/649/649.pdf,Louise Seaward;Maria Kallio,workshop / tutorial,"Topic

Transkribus is a platform for the automated recognition, transcription and searching of handwritten historical documents. Transkribus is part of the EU-funded Recognition and Enrichment of Archival Documents (READ) project. The core mission of the READ project is to make archival material more accessible through the development and dissemination of Handwritten Text Recognition (HTR) and other cutting-edge technologies.

The workshop is aimed at researchers and students who are interested in the transcription, searching and publishing of historical documents. It will introduce participants to the technology behind the READ project and demonstrate the Transkribus transcription platform. Our team has already conducted 30 similar workshops over the course of 2016, including several sessions with digital humanities scholars and students.

Transkribus can be freely downloaded from the Transkribus website. Participants will be instructed to create a Transkribus account and install Transkri-bus on their laptops in advance of the workshop. They should bring their laptops along to the workshop.

The workshop will consist of five parts:

Introduction to Handwritten    Text

Recognition (HTR) technology

The introduction to this workshop will explain how new algorithms and technologies are making it possible for computer software to process handwritten text. Handwritten Text Recognition (HTR) technology works differently from Optical Character Recognition (OCR) for printed texts (Leifert et al., 2016). Rather than focusing on individual characters, HTR engines process the entire image of a word or line, scanning it in various directions and then putting this data into a sequence. This introduction will outline the workings of HTR technology and show examples of the successful automatic transcription and searching of historical documents. It will also explain the possibilities of working with different languages and styles of handwriting. The latest experiments demonstrate that Transkribus can automatically generate transcripts with a Character Error Rate of 5-10%. This means that 90-95% of the characters in the transcript would be correct.

Overview of the READ project

This presentation will give an overview of the READ project and the specific tools it is creating. Computer scientists working on READ are developing HTR technology using thousands of manuscript pages with varying dates, styles, languages and layouts. Testing the technology on a large and diverse data set will make it possible for computers to automatically transcribe and search any kind of handwritten document, from the Middle Ages to the present day, from old Greek to modern English. This research has huge implications for the accessibility of the written records of human history. The READ project is making this technology available through the Transkribus platform but is also developing other tools designed to make it easier for archivists, researchers and the public to work with historical documents. The workshop leaders will present prototypes of some of these tools. These include a system of automatic writer identification, an elearning app to enable users to train themselves to read a particular style of writing, a mobile app to allow users to digitise and process documents in the archives and a crowdsourcing platform where volunteers can transcribe with the assistance of HTR technology. These tools will be open source and are designed to be used and adapted by other institutions and projects.

Introduction to Transkribus

HTR technology is made available through the Transkribus platform, which is programmed with JAVA and SWT (Muhlberger et al.) A transcription of a handwritten document can be undertaken in Tran-skribus for two main purposes. The first is a simple transcription - this allows users to train the HTR engine to automatically read historical papers. The second is an advanced transcription - this allows users to create a transcription of a document which may serve as the basis of a digital edition. This presentation will explain both uses of Transkribus.

HTR engines are based on algorithms of machine learning. The technology needs to be trained by being shown examples of at least 30 pages of transcribed material. This helps it to understand the patterns which make up words and characters. This training material is known as ‘ground truth' (Zagoris et al., 2012, Gatos et al., 2014). The workshop leaders will demonstrate how ‘ground truth' training data can be prepared using Transkribus. Participants can work with images of their own documents, or experiment with test documents already on the system.

Transkribus can also be used simply for transcription. This presentation will explain how to create a rich transcription of a document in the platform, using structural mark-up, tagging, document metadata and an editorial declaration.

Working independently with Transkribus

In the last part of the workshop, the participants will be able to try out the functions of Transkribus on their own laptops. They will be supported by the workshop leaders, who will explain the different elements of the platform and then give participants the chance to practice each function for themselves. The workshop leaders will circulate around the room to answer any questions.

The workshop leaders will demonstrate the following tasks. After each demonstration, participants will be given 10-15 minutes to practice what they have learned.

•    Document management - how to upload, view, save, move and export documents in standard formats (PDF, TEI, docx, PAGE XML)

•    User management - how to allow specific users to view and edit documents

•    Layout analysis - how to segment your documents to create training data for the HTR engines

•    Transcription - how to create a rich transcript with tags and mark-up

•    HTR - how to apply HTR models to automatically generate transcripts, how to conduct a keyword search of your documents, how to assess the accuracy of automatically generated transcripts

Question and Answer

The workshop will close with a Question and Answer session where participants can clarify anything they are unsure about. They will also have the opportunity to provide feedback on the Transkribus tool via our user survey.

Organizers

Louise Seaward
Dr. Seaward received her PhD in History from the University of Leeds (United Kingdom) in 2013. She is currently a research associate at University College London where she coordinates ‘Transcribe Bentham', the scholarly crowdsourcing initiative which asks members of the public to transcribe manuscripts written by the British philosopher Jeremy Bentham (17481832). Outside of digital humanities and Bentham, her research interests relate to the history of censorship and the Enlightenment.

Maria Kaiiio
Maria Kallio works as a Senior Research Officer at the National Archives of Finland where she is responsible for collections, crowd-sourcing and dissemination within the READ project. Currently she is also finishing her PhD in History at the University of Turku (Finland). In addition to digital humanities, her research interests include medieval literacy and written culture in all its diversity.

Proposed audience

Humanities and digital humanities scholars, archivists, librarians, computer scientists

Guidelines for Participants

Participants should register to attend the workshop by sending an email to Louise Seaward. Participants will need to bring their own laptops and install Transkribus before attending the workshop. If participants are interested in working with their own documents, they should bring a selection of digital images to the workshop. Otherwise, it will be possible to work with test documents already on the platform.

Bibliography

Leifert, G., Strauß, T., Grüning, T., and Labahn, R. (2016). ‘Cells in Multidimensional Recurrent Neural Networks’, https://arXiv.org/abs/1412.2620v02

Mühlberger, G., Colutto, S., Kahle, P., (forthcoming) ‘Handwritten Text Recognition (HTR) of Historical Docu-

ments as a Shared Task for Archivists, Computer Scientists and Humanities Scholars. The Model of a Transcription & Recognition Platform (TRP)' (pre-print)

Gatos, B., Louloudis, G., Causer, T., Grint, K., Romero, V.,

Sánchez, J.A., Toselli, A.H., and Vidal, E. (2014). ‘Ground-Truth Production in the tranScriptorium Project', Document Analysis Systems (DAS), 2014 11th IAPR International Workshop on Document Analysis Systems, 237-244

Stamatopoulos, N., and Gatos, B. (2015). ‘Goal-oriented performance evaluation methodology for page segmentation techniques', 13th International Conference on Document Analysis and Recognition (ICDAR), 281285.

Konstantinos, Z., Pratikakis, I., Antonacopoulos, A., Gatos, B., and Papamarkos, N. (2012). ‘Handwritten and Machine Printed Text Separation in Document Images Using the Bag of Visual Words Paradigm"", in: Frontiers in Handwriting Recognition (ICFHR), 2012 International Conference,    Bari,    103-108.    DOI:

10.1109/ICFHR.2012.207.",txt,Creative Commons Attribution 4.0 International,,archives;digitisation;handwriting recognition;ocr,English,computer science;historical studies;library & information science
4196,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,High Performance Computing for Photogrammetry and OCR Made Easy,https://dh2017.adho.org/abstracts/664/664.pdf,Quinn Dombrowski;Tassie Gniady;Megan Meredith-Lobay;John Edward Simpson,workshop / tutorial,"Computationally-intensive research methods have seen increasing adoption among digital humanities scholars, but for scholars outside R1 institutions with robust computing environments, techniques like pho-togrammetry or text recognition within images can easily monopolize desktop computers for days at a time. Even at institutions with a research computing program, systems are configured for scientific applications, and IT staff may be unaccustomed to working with humanities scholars, particularly those who are not already proficient at using the command line. National compute infrastructures in North America (Compute Canada and XSEDE) are a compelling alternative, providing no-cost compute allocations for researchers and offering support from technical staff interested in and familiar with humanities computing needs. This workshop will start by introducing participants to Compute Canada and XSEDE, cover how to obtain a compute allocation (including for researchers outside of the US and Canada), and proceed through two hands-on tutorials on research methods that benefit from the additional compute power provided by these infrastructures: 1) photogrammetry using PhotoScan and 2) using OCR via Tesseract to extract metadata from images.

Photogrammetry
Photogrammetry (generating 3D models from a series of partially-overlapping 2D images) is quickly gaining favor as an efficient way to develop models of everything from small artifacts that fit in a light box to large archaeological sites, using drone photography. Stitching photographs together, generating point clouds, and generating the dense mesh that underlies a final model are all computationally-intensive processes that can take up to tens of hours for a small object to weeks for a landscape to be stitched on a high-powered desktop. Using a high-performance compute cluster can reduce the computation time to about ten hours for human-sized statues and twenty-four hours for small landscapes. Generating a dense cloud, in particular, sees a significant performance when run on GPU nodes, which are increasingly common in institutional HPC clusters and available through Compute Canada and XSEDE.

One disadvantage of doing photogrammetry on an HPC cluster is that it requires use of the command line and Photoscan's Python API. Since it is not reasonable to expect that all, or even most, scholars who would benefit from photogrammetry are proficient with Python, UC Berkeley has developed a Jupyter notebook that walks through the steps of the photogrammetry process, with opportunities for users to configure the settings along the way. Jupyter notebooks embed documentation along with code, and can serve both as a resource tool for researchers who are learning Python, and as a stand-alone utility for those who want to simply run the code, rather than write it. Indiana University, on the other hand, has developed a workflow using a remote desktop interface so that all the GUI capabilities and workflows of PhotoScan are still available. A python script is still needed so that the user may avail herself of the compute nodes, but the rest of the workflow is very similar traditional PhotoScan usage. Finally, both methods offload the processing the HPC cluster, allowing users to continue to work on a computer that might normally be tied up by the processing demands of photogrammetry.

The workshop will give participants hands-on experience creating a 3D model using two different approaches: first, by accessing the Photoscan graphical user interface on a virtual desktop running on XSEDE's Jetstream cloud resource; and second, by using a Jupy-ter notebook running on an HPC cluster.

OCR
Optical Character Recognition (OCR) is a tool used for extracting text from images and is perhaps most well known as a core technology behind the creation of the Google Books and HathiTrust corpora. OCR continues to open historical texts for analysis at large scale, fuelling a significant portion of research work

within the digital humanities to the point that it would

be difficult to think of the “million books problem” existing without this technology. While there are many OCR tools available the most popular tool that is also free and open source is Tesseract.

This portion of the workshop will also make use of Jupyter Notebooks to provide templates for learning the development process and that can then be taken away to speed development of future code. We will feature two projects for participants to practice with. A “traditional” OCR task that will have workshop participants processing images from the London Times in a demonstration of the improvements in OCR over the past few years and a task focusing on processing historical photographs to find text that can be added to the associated metadata to improve the searchability of an index.

Target Audience
We anticipate that this workshop will appeal particularly to scholars who work with cultural heritage materials (a field where photogrammetry is an increasingly common method for generating digital surrogates), as well as those who work with archival photographs, and scholars with large corpora of photographs. It will also be relevant for scholars who already engage in computational analysis of primary sources, who wish to increase the efficiency of their analysis by leveraging high-performance compute environments. No previous experience with HPC environments is necessary. This workshop can accommodate 25 participants.

Instructors
Quinn Dombrowski
Quinn is the Humanities Domain Expert at Berkeley Research Computing. At UC Berkeley, Quinn works with humanities researchers and research computing staff at Research IT to bridge the gap between humanities research questions and campus-provided resources for computation and research data management. She was previously a member of the program team for the Mellon-funded cyberinfrastructure initiative Project Bamboo, has led the DiRT tool directory and served as the technical editor of DHCommons. Quinn has an MLIS from the University of Illinois, and a BA and MA in Slavic linguistics from the University of Chicago.

Tassie Gniady
Tassie manages the Cyberinfrastructure for Digital Humanities group at Indiana University. She has a PhD in Early Modern English Literature from the University of California-Santa Barbara where she began her digital humanities journey in 2002 under the wing of Patricia Fumerton. She coded the first version of the NEH-funded English Broadside Ballad Archive, making many mistakes and learning much along the way. She now has an MIS from Indiana University, teaches a digital humanities course in the Department of Information and Library Science at IU, and holds regular workshops on text analysis with R and photogramme-try.

Megan Meredith-Lobay
Megan Meredith-Lobay is the digital humanities and social sciences analyst, as well as the Vice President, for Advanced Research Computing at the University of Briitsh Columbia. She holds a PhD from the University of Cambridge in medieval archaeology where she used a variety of computing resources to investigate ritual landscapes in early medieval Scotland Scotland. Megan has worked at the University of Alberta where she supported research computing for the Faculty of Arts, and at the University of Oxford where she was the programme coordinator for Digital Social Research, an Economic and Social Research Council project to promote advanced ICT in Social Science research.

John Simpson
John Simpson joined Compute Canada in January 2015 as a Digital Humanities Specialist and bringing a diverse background in Philosophy and Computing. Prior to Compute Canada, he was involved in a research-intensive postdoctoral fellowship focusing on developing semantic web expertise and prototyping tools capable of assisting academics in consuming and curating the new data made available by digital environments. He has a PhD in Philosophy from the University of Alberta, and an MA in Philosophy and BA in Philosophy & Economics from the University of Waterloo. In addition to his role at WestGrid, John is also a Member-at-Large of the Canadian Society for Digital Humanities (CSDH-SCHN), a Programming Instructor

with the Digital Humanities Summer Institute (DHSI), and the national coordinator for Software",txt,Creative Commons Attribution 4.0 International,,3d modeling;high performance computing;ocr;visualization,English,archaeology;art history;classical studies;computer science
6292,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,"Chinese Text Project 
                     A Dynamic Digital Library of Pre-modern Chinese",,Donald Sturgeon,"paper, specified ""long paper""","<text>
        
            <div type=""div1"" rend=""DH-Heading1"">
                Introduction
                <p>Traditional full-text digital libraries, including those in the field of pre-modern Chinese, have typically followed top-down, centralized, and static models of content creation and curation. In this type of model, written materials are scanned, transcribed by manual effort and/or Optical Character Recognition (OCR), then corrected manually, reviewed, annotated, and finally imported into a system in their final, usable form. This is a natural and well-grounded strategy for design and implementation of such systems, with strong roots in traditional academic publishing models, and offering greatly reduced technical complexity over alternative approaches. This strategy, however, is unable to adequately meet the challenges of increasingly large-scale digitization and the resulting rapid growth in potential corpus size as ever larger volumes of historical materials are digitized by libraries around the world.</p>
                <p>The Chinese Text Project (https://ctext.org) is a full-text digital library of pre-modern Chinese written materials which implements an alternative model for creation and curation of full-text materials, adapting methodologies from crowdsourcing projects such as Wikipedia and Distributed Proofreaders (Newby and Franks 2003) while also integrating them with full-text database functionality. In contrast to the traditional linear approach, in which all stages of processing including correction and review must be completed before transcribed material is ingested into a database system, this approach works by immediately ingesting unreviewed materials into a publicly available, managed system, within which these materials can be navigated and used, as well as improved through an ongoing collaborative correction and annotation process. From a user perspective, this has the consequence that the utility of the system does not rest upon prior expert review of materials, but instead derives from provision to individual users of the ability to interact directly and effectively with primary source materials and verify accuracy of transcription and annotation for themselves. Combined with specialized Optical Character Recognition techniques leveraging features common to pre-modern Chinese written works (Sturgeon 2017a), this has enabled the creation of a scalable system providing access to a long tail of historical works which would otherwise not be available in transcribed form. The system is highly scalable and currently contains over 25 million pages of primary source material while being used by over 25,000 users around the world every day.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Creating transcriptions
                <p>The most fundamental type of material contained in the Chinese Text Project consists of digital facsimiles of pre-modern published works. These are typically ingested in bulk through collaboration with university libraries which have created high quality digital images of works in their collections. After ingestion, the next step in making these materials more useful to users is creation of approximate transcriptions from page images using OCR. Producing accurate OCR results for historical materials is challenging due to a number of issues, including variation in handwriting and printing styles, varying degrees of contrast between text and paper, bleed-through from reverse sheets, complex and unusual layouts, and physical, water or insect damage to the materials themselves prior to digitization. In addition to these challenges which are common to OCR of historical documents generally, OCR for premodern Chinese works faces additional difficulties in extracting training data due to the large number of distinct character types in the Chinese language. Most OCR techniques apply machine learning to infer from an image of a character which character type it is that the image represents, and these techniques require comprehensive training data in the form of clear and correctly labeled images in the same writing style for every possible character. This is challenging for Chinese due to the large number of character types needed for useful OCR (on the order of 5000); unlike historical OCR of writing systems with much smaller character sets, it is not feasible to simply create this data manually. Instead, training data is extracted through an automated procedure (Sturgeon 2017a) which leverages knowledge about existing transcriptions of other texts to assemble clean labeled character images extracted from historical works for every character to be recognized (Figure 1). Together with image processing and language modeling tailored to pre-modern Chinese, this significantly reduces the error rate in comparison with off-the-shelf OCR software.</p>
                <figure>
                    <graphic n=""1001"" width=""15.980833333333333cm"" height=""6.526388888888889cm"" url=""Pictures/256d1bd6a57b8d9bdfa0571116529e61.jpeg"" rend=""inline""></graphic>
                </figure>
                <p>Figure 1. OCR training data is extracted automatically from handwritten and block-printed primary source texts.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Navigating texts and page images
                <p>Once transcriptions of page images have been created, they are directly imported into the public database system. The system represents textual transcriptions as sequences of XML fragments, in which markup is used to express both the relationship between transcribed text and the page image to which it corresponds, as well as the logical structure of the document as a whole. This facilitates two distinct methods of interacting with the transcribed material: firstly, as a single document consisting of the transcribed contents of each page concatenated in sequence to give readable plain-text with logical structure (divisions into chapters, sections, and paragraphs); secondly, as a sequence of page-wise transcriptions, in which a direct visual comparison can be made between the transcription and the image from which it is derived (Figure 2). In both cases, an important contribution of the transcription is that it enables full-text search; the primary utility of the page-wise view is that it enables efficient comparison of transcribed material with the facsimile of the primary source itself. As these two views are linked to one another and created from the same underlying data, this makes it feasible to read and navigate a text according to its logical structure, and at any stage of the process jump to the corresponding location in the sequence of page images to confirm accuracy of the transcription.</p>
                <p>
                    <graphic n=""1002"" width=""7.882144444444444cm"" height=""6.420555555555556cm"" url=""Pictures/637327c13478e287154d398c24e7acce.jpeg"" rend=""inline""></graphic>
                    <graphic n=""1003"" width=""7.866944444444444cm"" height=""6.998630555555556cm"" url=""Pictures/6d925b79375705576d383925af259d85.jpeg"" rend=""inline""></graphic>
                </p>
                <p>
                    <hi rend=""italic"">Figure 2. Full-text search results can be displayed in context in a logical transcription view (left), as well as aligned directly together with the source image in an image and transcription view (right).</hi>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Crowdsourced editing and curation
                <p>As initial transcriptions are created using OCR, they inevitably contain mistakes. Users of the system have the option to correct mistakes they identify, as well as to annotate texts in a number of ways. Two distinct editing interfaces are provided: a direct editor, which enables direct editing of the underlying XML representation, and a visual editor allowing simplified editing of page-level transcriptions, which edits the same underlying content but does not require direct understanding or modification of XML. Regardless of which mechanism is used to submit an edit, all edits are committed immediately to the public system. Edits are versioned, allowing visualization of changes between versions and simple reversion of a text to its state at an earlier point in time. At present, the system receives on the order of 100 edits each day, representing much larger numbers of corrections, as editors frequently choose to correct multiple errors and sometimes entire pages in a single operation.</p>
                <p>Further visual editing tools supplement these mechanisms to enable crowdsourcing of more complex information. Illustrations are entered by the user drawing a rectangular box on the page image to indicate the location of the illustration, then filling in a simple form describing various aspects of it (Figure 3). This results in an XML fragment describing the illustration, which can simply be inserted into the text at the appropriate location to represent it. This allows the illustration to be extracted from its context on the page and represented in the full-text transcription view as well as in the page-wise view. It also facilitates illustration search functionality, where illustrations can be searched by caption across all materials contained in the system (Figure 4). A similar visual editing interface is used to enable the inputting of rare and variant characters which do not yet exist in Unicode. These characters are no longer in common use, but occur in many historical documents. The visual editing interface for rare character input also uses metadata provided by the user to identify whether a given character is the same as any existing character known to the system, and if so, assigns a common identifier so that data about these characters can be aggregated, and text containing such characters searched.</p>
                <figure>
                    <graphic n=""1004"" width=""15.980833333333333cm"" height=""11.147777777777778cm"" url=""Pictures/26e3665887963af1cd68a1f413f4c741.jpeg"" rend=""inline""></graphic>
                </figure>
                <p>Figure 3. Identification and markup of illustrations within source materials are crowdsourced using purpose-designed visual editing tools which convert user input into XML.</p>
                <figure>
                    <graphic n=""1005"" width=""15.980833333333333cm"" height=""9.70138888888889cm"" url=""Pictures/2015dcc8d3a71e1d205b21c00f891beb.jpeg"" rend=""inline""></graphic>
                </figure>
                <p>
                    <hi rend=""italic"">Figure 4. Image search: individual images are extracted from (and linked to) the precise locations at which they occur in source materials, and can be searched by caption.</hi>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Exporting data and integrating with external systems
                <p>In addition to the main user interface, a web-based Application Programming Interface (API) provides machine-readable access to data and metadata stored in the system. This facilitates text mining applications, as well as integration with other online systems. An example of the latter is the MARKUS textual markup system (De Weerdt et al. 2016), which can use the API to search for texts and load their transcriptions directly into this externally developed and maintained tool. An XML-based plugin system for the Chinese Text Project user interface also enables users to define and share extensions to the web interface which can be used to create connections to external projects and resources. This allows third-party tools such as MARKUS to integrate directly into the web interface, facilitating seamless connections between separately developed online projects. Text mining access is further facilitated by the provision of a Python module capable of accessing the API (Sturgeon 2017c), which is already in use in teaching and research (Sturgeon 2017b).</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Newby, G. B. and Franks, C.</hi> (2003). Distributed proofreading. 
                        <hi rend=""italic"">2003 Joint Conference on Digital Libraries, 2003. Proceedings.</hi> pp. 361–63 doi:
                        <ref target=""https://doi.org/10.1109/JCDL.2003.1204888"">10.1109/JCDL.2003.1204888</ref>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sturgeon, D.</hi> (2017a). Unsupervised Extraction of Training Data for Pre-Modern Chinese OCR.
                        <hi rend=""italic"" xml:space=""preserve""> Florida Artificial Intelligence Research Society</hi>. 
                        <hi rend=""italic"">Proceedings.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sturgeon, D.</hi> (2017b). Classical Chinese DH: Getting Started. 
                        <hi rend=""italic"">Digital Sinology</hi>
                        <ref target=""https://digitalsinology.org/classical-chinese-dh-getting-started/"">https://digitalsinology.org/classical-chinese-dh-getting-started/</ref> (accessed 27 November 2017).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sturgeon, D.</hi> (2017c). Chinese Text Project API wrapper 
                        <ref target=""https://pypi.python.org/pypi/ctext/"">https://pypi.python.org/pypi/ctext/</ref> (accessed 27 November 2017).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sturgeon, D.</hi> (2017d). Unsupervised identification of text reuse in early Chinese literature. 
                        <hi rend=""italic"">Digital Scholarship in the Humanities</hi> doi:
                        <ref target=""https://doi.org/10.1093/llc/fqx024."">10.1093/llc/fqx024.</ref>
                        <ref target=""https://academic.oup.com/dsh/advance-article/doi/10.1093/llc/fqx024/4583485"">https://academic.oup.com/dsh/advance-article/doi/10.1093/llc/fqx024/4583485</ref> (accessed 27 April 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Weerdt, H. D., Ming-Kin, C. and Hou-Ieong, H.</hi> (2016). Chinese Empires in Comparative Perspective: A Digital Approach. 
                        <hi rend=""italic"">Verge: Studies in Global Asias</hi>, 
                        <hi rend=""bold"">2</hi>(2): 58–69 doi:
                        <ref target=""https://doi.org/10.5749/vergstudglobasia.2.2.0058"">10.5749/vergstudglobasia.2.2.0058</ref>.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,chinese;crowdsourcing;ocr;text;transcription,English,"asian studies;crowdsourcing;digitisation, resource creation, and discovery;english;historical studies;image processing;philology;public humanities and community engaged scholarship"
6364,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,Prosopografía de la Revolución Mexicana: Actualización de la Obra de Françoise Xavier Guerra,,Martha Lucía Granados-Riveros;Diego Montesinos,poster / demo / art installation,"<text>
        
            <p>En 1985 se publicó el libro 
                <hi rend=""italic"">México: del antiguo régimen a la revolución</hi> del historiador Françoise Xavier Guerra, una referencia fundamental para el estudio de la Revolución mexicana. La obra revela las relaciones y tensiones entre la sociedad tradicional, un sistema heredado de la colonia y el Estado moderno proveniente en gran medida de los ideales liberales de la revolución francesa.
            </p>
            <p>El trabajo de Xavier Guerra se inscribe en una amplia tradición de la investigación prosopográfica que se extiende desde el siglo XIX (Verboven, Carlier y Dumolyn, 2007) hasta el relativamente reciente uso de herramientas computacionales para el manejo de bases de datos (Blust, 1989; Keats-Rohan, 2010). El cuerpo biográfico de su investigación está compuesto por más de siete mil actores sociales entre los que figuran individuos y colectividades, con aproximadamente cien mil datos asociados a los movimientos políticos. Para su análisis se construyó una base que sistematizó los datos en más de cincuenta categorías que codifican dos tipos de sucesos; aquellos personales como fecha de nacimiento, muerte y ascendencia familiar y aquellos sucesos relacionados con la vida política y social del actor como participación en batallas o los cargos públicos ocupados. Los sucesos se organizaron en módulos independientes, lo cual permitió enriquecer la base de datos con la captura de nuevos módulos para personajes ya establecidos.</p>
            <p>Dicha base de datos fue almacenada originalmente en tres cintas magnéticas de las cuales no se refieren más detalles, realizar nuevos análisis resulta inviable ya que el único medio en que está disponible actualmente es el impreso en los anexos de la obra señalada. El objetivo de este trabajo es la digitalización de la base de datos de Xavier Guerra, que permita la reproducción de los análisis del autor, así como la generación de nuevo conocimiento a partir del cruce de variables. </p>
            <p>Con ese fin, se creó un programa en Python que ocupa Tesseract, una biblioteca de reconocimiento de caracteres. Debido a la estructura modular de la base de datos, los renglones, columnas y espacios en blanco son significativos. Por lo tanto, se realizó un pre-procesamiento de las imágenes, para detectar la estructura espacial del texto, de manera que Tesseract procesará pedazos de texto organizados. En esta etapa se ocupó el framework OpenCV y la biblioteca Pytesseract. Posteriormente, el programa organizó la información en un esquema de base de datos dentro de un archivo SQL.</p>
            <p>En este póster presentamos el código desarrollado para la recuperación y organización de la base de datos, el funcionamiento de la base mediante algunas réplicas de los análisis que realizó Xavier-Guerra en su obra, así como el resultado de queries inéditos y por último el diseño inicial de la página que permita interactuar con los datos, de modo que los usuarios puedan consultar al sistema en términos de tiempo, geografía, compromisos políticos y relaciones de parentesco o sociales.</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>Ingrese sus referencias aquí:</bibl>
                    <bibl>
                        <hi rend=""bold"">Blust, N.</hi> (1989). Prosopography and the computer: problems and possibilities, en Denley, P. (ed.) 
                        <hi rend=""italic"">History and computing</hi> . no.2. Manchester, UK: Manchester University Press, pp. 12–18.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" xml:space=""preserve"">Keats-Rohan, K. </hi>(2010). Prosopography and Computing: a Marriage Made in Heaven?, 
                        <hi rend=""italic"">History and Computing</hi>, 12(1), pp. 1–11.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" xml:space=""preserve"">Verboven, K., Carlier, M. </hi>y
                        <hi rend=""bold"" xml:space=""preserve""> Dumolyn, J. </hi>(2007). A Short Manual to the Art of Prosopography, en Keats-Rohan, K. (ed.) 
                        <hi rend=""italic"">Prosopography Approaches and Applications. A Handbook</hi>. Oxford: University of Oxford, pp. 35–69.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,base de datos;digitalización;ocr;prosopografía;revolución mexicana,English,"anthropology;databases & dbms;historical studies;knowledge representation;networks, relationships, graphs;spanish;spanish and spanish american studies"
6409,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,Locating Place Names at Scale: Using Natural Language Processing to Identify Geographical Information in Text,,Lauren Tilton;Taylor Arnold;Courtney Rivard,poster / demo / art installation,"<text>
        
            <p> Historical sources are often tagged with metadata about place such as where the object was created, acquired, or stored. Rich latent geographical information is often also mentioned throughout textual documents. A challenge though is how to extract this spatial information at scale. For example, when a text mentions Paris, does the writer mean Paris, Texas, USA or Paris, France?  Out of context, most would assume the reference is to more populous capital of France, but it could also be the city in Texas. While close reading would provide an answer, this becomes a challenge when working with hundreds and thousands of documents. How might we be able to more accurately predict the exact location using the broader context?</p>
            <p>Our poster ""Locating Place Names at Scale: Using Natural Language Processing to Identify Geographical Information in Text""  addresses how computational methods can be used to identify and geolocate place-based data. We show how Named Entity Recognition (NER), a natural language processing (NLP) technique, can locate place names using the document's context. We then discuss how to geolocate those places names using a series of computational techniques.  Specifically, we start by finding references to specific political divisions (countries, states, and cities), georeferencing them through the Google API. Any political divisions that are uniquely determined become reference points. The reference points are then used to disambiguate terms with multiple results, such as Paris, France and Paris, Texas. Disambiguation is done by appending the political division to the name of the place in order of specificity. If this fails to uniquely determine locations, distances to the closest reference points in the text are used to break ties. This strategy increases proper place name identification and can be applied automatically over a large corpus of digitized texts. </p>
            <p>Finally, we turn to an example from our collaborative project on the United States Federal Writers' Project (FWP) entitled Voice of a Nation: Mapping Documentary Expression in New Deal America. During the New Deal, thousands of life histories were written to capture the American experience. While the location of the interviews provides insight into the geographic expanse of the collection (Figure 1), the interviewees consistently spoke about places beyond the location of the physical interview. We apply NER and NLP to identify the place names in the interviews. We are then able to identify and map the many different locations that interviewees mentioned (Figure 2).  Across over a thousand interview, what we see is that many of those interviewed spoke of migration - whether their own or their kin - generating a more complex understanding of movement and place during the early 20th century in the United States.</p>
            <p>
                <figure>
                    <graphic url=""Pictures/ed216cf6fa21a293b1262abcb28920df.png""></graphic>
                    Triangles represent where the metadata identified the interview location in Virginia.
                </figure>
            </p>
            <p>
                <figure>
                    <graphic url=""Pictures/961ce0761b20b970b17d117ca99f452b.png""></graphic>
                    Red “X”s  represent locations identified by the use of our algorithm, based on named entity recognition, to the text of the interview referenced in Figure 1
                </figure>
            </p>
        
    </text>",xml,Creative Commons Attribution 4.0 International,,geography;named entity recognition;nlp;place,English,"artificial intelligence and machine learning;data mining / text mining;english;geohumanities;historical studies;modeling;natural language processing;spatial & spatio-temporal analysis, modeling and visualization;technologies;visualization"
6525,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,Mitologias do Fascínio Tecnológico,,Andre Azevedo da Fonseca,"paper, specified ""short paper""","<text>
        
            <p>A cultura digital do século XXI tem sido marcada pela ascensão de um imaginário mágico em relação ao poder das tecnologias. Por meio de uma produção monumental de símbolos, as indústrias culturais e a publicidade das mais diversas empresas de tecnologia têm veiculado mensagens a fim de relacionar o consumo tecnológico à conquista progressiva da autonomia, da liberdade, da felicidade e, em última instância, da transcendência. Este imaginário que induz à devoção das tecnologias parece seduzir as novas gerações com a promessa da elevação dos seres humanos à condição de semidivindades a partir do consumo físico e simbólico de produtos e marcas.</p>
            <p>No entanto, sob o brilho deste deslumbre, o Estado e as corporações têm se movimentado no sentido de empregar recursos tecnológicos de forma sistemática para aprofundar o controle social de natureza tecnocrática, de modo que cidadãos e consumidores são observados e analisados em sua intimidade. Ofuscados pelo brilho mágico das tecnologias, usuários entregam voluntariamente informações detalhadas de suas personalidades e experiências pessoais para delegar aos algoritmos de inteligência artificial decisões cada vez mais importantes de suas experiências humanas, tornando-se mais vulneráveis a estímulos publicitários e propagandas ideológicas cada vez mais personalizadas e eficientes.</p>
            <p>Entre os vários elementos para que o capitalismo informacional lograsse legitimar essa sociedade de controle tecnocrático, observamos uma intensa produção simbólica nas indústrias culturais no sentido de instrumentar a cultura digital com um fabuloso repertório iconográfico para, primeiramente, exorcizar os temores apocalípticos que as tecnologias sem limites haviam inspirado na humanidade – sobretudo após o advento da bomba atômica e da chamada crise da razão – e, em seguida, substituir os antigos temores por uma nova devoção aos mitos tecnológicos. Nesse contexto, mitologias ancestrais que expressavam as maldições divinas decorrentes da desobediência de homens e mulheres que ousaram ultrapassar os limites do conhecimento foram esvaziadas e invertidas, de modo que os consumidores contemporâneos, mais do que apenas perder o medo, passaram a cultuar esses mitos: da maçã proibida do Éden à mação mordida da Apple, do terrível Big Brother de George Orwell ao sedutor Big Brother da Endemol, da maldição do monstro de Frankenstein à celebração do gênio do cientista impetuoso no imaginário do Vale do Silício.</p>
            <p>O objetivo desta pesquisa é compreender essa dinâmica de subversão de mitologias empregadas para superar os temores, atribuir uma conotação religiosa às experiências com tecnologias e, enfim, ofuscar o controle tecnocrático do ecossistema digital. Para isso, sob a perspectiva da Comunicação, da História Cultural e dos estudos de mitologia e imaginação social, analisamos um conjunto de símbolos evocados na imprensa, no cinema e na publicidade de empresas de tecnologia contemporâneas, situando-as no contexto histórico da utilização de arquétipos e mitologias na publicidade a partir do final do século XX. Como resultado, identificamos um conjunto de mitos e imagens arquetípicas manipuladas nas mídias para associar o consumo tecnológico ao imaginário sagrado da superação do pecado original, da reconquista do paraíso e da transcendência da condição humana.</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliografía
                    <bibl>BARTHES, R. (2009). Mitologias. 4 ed. Rio de Janeiro: Difel.</bibl>
                    <bibl>CHARTIER, R. (1985). A história cultural: entre práticas e representações. Rio de Janeiro: Difel/Bertrand Brasil.</bibl>
                    <bibl>ELLUL, J. (1964). The technological society. New York: Vintage Books.</bibl>
                    <bibl>JUNG, C. G. (2000). Os arquétipos e o inconsciente coletivo. 2 ed. Petrópolis: Vozes.</bibl>
                    <bibl>ROSZAK, T. (1972), A contracultura: reflexões sobre a sociedade tecnocrática e a oposição juvenil. 2 ed. Petrópolis: Vozes.</bibl>
                    <bibl>TURNER, F. (2006). From Counterculture to Cyberculture: Stewart Brand, the Whole Earth Network, and the Rise of Digital Utopianism. Chicago: The University of Chicago Press.</bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,cultura digital;imaginário tecnológico;mitologias contemporâneas;sociedade de consumo;tecnocracia,English,"audio, video, multimedia;criticism;cultural studies;digital humanities history;epistemology;film and media studies;hacker culture;historical studies;knowledge representation;portuguese;theory"
6541,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,OCR’ing and classifying Jean Desmet’s business archive: methodological implications and new directions for media historical research,,Christian Gosvig Olesen;Ivan Kisjes,"paper, specified ""short paper""","<text>
        
            <p>
                <hi rend=""color(111111)"" xml:space=""preserve"">This paper discusses the endeavours of the research project </hi>
                <hi rend=""italic color(111111)"">MIMEHIST: Annotating EYE’s Jean Desmet Collection</hi>
                <hi rend=""color(111111)"" xml:space=""preserve""> (2017-2018) - funded by the Netherlands Scientific Research Organisation - to do optical character recognition (OCR) and apply various computer vision techniques on the business archive of film distributor and exhibitor Jean Desmet</hi>
                <hi rend=""color(373737) background(white)"" xml:space=""preserve""> (1875-1956)</hi>
                <hi rend=""color(111111)"" xml:space=""preserve"">. </hi>
            </p>
            <p>The Desmet collection consists of approximately 950 films produced between 1907 and 1916, a business archive containing around 127.000 documents, some 1050 posters and around 1500 photos. The Collection is unique because of its large amount of rare films from the transitional years of silent cinema, and because of the richness of its business archive which holds extensive documentation of early film exhibition and distribution practices in the 1910s. These features contribute to its immense historical value which was one of the main reasons why it was inscribed on UNESCO’s Memory of the World Register in 2011.</p>
            <p>By OCRing and classifying Jean Desmet’s business archive, MIMEHIST will allow scholars to browse and annotate its documents - all scanned in high resolution - in the new ‘Media Suite’ of the Dutch national research infrastructure (CLARIAH). The results will be integrated in a search interface enabling media historians to identify word frequencies and topics as a basis for research on early film distribution and exhibition and, the paper argues, open for media historical research which productively builds on and expands the collection’s use in previous scholarship.</p>
            <p>
                <hi rend=""color(111111)"" xml:space=""preserve"">Throughout the past decades, Desmet’s business documents have offered a rich source for socio-economic cinema history. Media historians such as Karel Dibbets and Rixt Jonkman have studied parts of the collection’s (related) data by manually transcribing and organising it into databases (Jonkman, 2007; Dibbets, 2010). This work produced an empirical, quantitative foundation for network analysis of Dutch film distribution and exhibition in cinema’s earliest years. However, this research also made evident that the archive is </hi>too large and diverse to organise and transcribe manually. A particular challenge is that collection contains many different kinds of documents: personal letters, business letters, records of film rentals, postcards, newspaper clippings, telegrams, scraps of paper with notes, photographs etc. Furthermore, some documents are typewritten, others handwritten. 
            </p>
            <p>
                <hi rend=""color(111111)"">To allow scholars to research and annotate larger amounts of the archival documents’ data</hi> in CLARIAH’s Media Suite
                <hi rend=""color(111111)"">,</hi> automated information extraction from the documents seemed challenging yet promising.
                <hi rend=""color(111111)"" xml:space=""preserve""> MIMEHIST took up this challenge by trying OCR, document classification, topic modelling, named entity recognition and other visual and linguistic tools on the set of scans in order to extract as much data and metadata from the individual documents as possible. Different document types required different treatment. For instance, we quickly determined that it did not make much sense to do OCR on a  tiny handwritten note, while handwriting detection on the other hand would be possible and could yield productive results on such an item.</hi>
            </p>
            <p>
                <hi rend=""color(111111)"">E</hi>xperiments were conducted in visual document classification, visual document analysis and distant reading. Visual document classification was performed by clustering a combination of color and texture histograms derived from the scans. This step was taken mostly because the existing index of the archive is incomplete: it has information on the folders in the archive, which contain the documents, but not the documents themselves. The Media Suite works with individual documents, not folders, so it became necessary to, for instance, discern sub-folder covers from the documents inside.
            </p>
            <p>A second reason to do classification was that each type of document needs a different kind of processing - typed letters can be OCR-ed, but not photos, while handwritten letters could be classified by comparing handwriting styles. By separating different document types it became possible to employ the most effective information extraction tools on them. This procedure also allowed for finding visually similar documents, making it possible for researchers to look for similarities in for instance texture or color. </p>
            <p>The typewritten documents were OCR-ed, then classified on the basis of the recognized text in order to differentiate e.g. personal letters from business correspondence. Named entity recognition on the texts provided us with a network of people and places, with links to the letters. Attempts at handwriting recognition on the basis of ‘image texture’ histogram comparisons provided mixed results, - for the instances where larger samples of a single person’s handwriting were available it worked reasonably well, but for handwriting types occurring only a few times the confidence of the classifier was too low and such documents were classified as one of the more frequently occurring types. The results of these steps, in combination with the existing index’s metadata, provided a rich enough metadata structure for the use of individual documents in the tool.</p>
            <p>
                <hi rend=""color(111111)"" xml:space=""preserve"">In addition to a discussion of these steps, our paper reflects on the results’ epistemological implications for future research, by discussing them in relation to previous quantitative approaches to the Desmet Collection. From this vantage point, our paper argues that while previous quantitative studies of Desmet’s business documents were premised in the coding and transcription procedures of Cliometrics and </hi>
                <hi rend=""italic color(111111)"" xml:space=""preserve"">Annales </hi>
                <hi rend=""color(111111)"">historiography, MIMEHIST’s results nurture exploratory and qualitative research coupled with serendipitous search and annotation procedures focusing also on visual features. Consequently, the paper argues, researchers may to a greater degree than hitherto highlight data contingencies and multiplicity of viewpoints in the Desmet business archive.</hi>
            </p>
        
    </text>",xml,Creative Commons Attribution 4.0 International,,annotation;cinema history;jean desmet;ocr,English,"audio, video, multimedia;computer science;creative and performing arts, including writing;criticism;cultural and/or institutional infrastructure;data mining / text mining;digital humanities history;english;epistemology;film and media studies;theory"
7855,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,Introduction To The TXM Content Analysis Software,https://github.com/ADHO/dh2015/blob/master/xml/HEIDEN_Serge_Introduction_To_The_TXM_Content_Analysis_S.xml,Serge Heiden,workshop / tutorial,"<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Introduction To The TXM Content Analysis Software</title>
                <author>
                    <persName>
                        <surname>Heiden</surname>
                        <forename>Serge</forename>
                    </persName>
                    <affiliation>ENS de Lyon, France</affiliation>
                    <email>slh@ens-lyon.fr</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Pre-Conference Workshop and Tutorial (Round 2)</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>text analysis</term>
                    <term>txm software</term>
                    <term>xml</term>
                    <term>tei</term>
                    <term>nlp</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>corpora and corpus activities</term>
                    <term>encoding - theory and practice</term>
                    <term>natural language processing</term>
                    <term>text analysis</term>
                    <term>xml</term>
                    <term>concording and indexing</term>
                    <term>content analysis</term>
                    <term>visualisation</term>
                    <term>data mining / text mining</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p rend=""Standard"">The objective of the Introduction to the TXM Content Analysis Software tutorial is to introduce the participants to the methodology of textometric content analysis (
                <ref target=""http://textometrie.ens-lyon.fr/?lang=en"">http://textometrie.ens-lyon.fr/?lang=en</ref>) through the use of the free and open-source TXM software (
                <ref target=""http://sourceforge.net/projects/txm/files/documentation/TXM%20Leaftlet%20EN.pdf/download"">http://sourceforge.net/projects/txm/files/documentation/TXM%20Leaftlet%20EN.pdf/download</ref>) directly on their own laptop computers. At the end of the tutorial, the participants will be able to input their own textual corpora (Unicode-encoded raw texts or XML tagged texts) into TXM and to analyze them with the panel of content analysis tools available: word patterns frequency lists, kwic concordances and text browsing, rich full text search engine syntax (allowing to express various sequences of word forms, part of speech, and lemma combinations constrained by XML structures), statistically specific sub-corpus vocabulary analysis, statistical collocation analysis, etc.).
            </p>
            <p rend=""Standard"">During the tutorial, each participant will use TXM (from 
                <ref target=""http://sourceforge.net/projects/txm"">http://sourceforge.net/projects/txm</ref>) and the TreeTagger lemmatizer (
                <ref target=""http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger"">http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger</ref>) on her Windows, Mac, or Linux laptop and will leave the tutorial with a ready-to-use environment.
            </p>
            <p rend=""Standard"">The tutorial will also introduce the participants to the TXM community ecosystem (users mailing list and wiki, bug reports, etc.) and to the TXM portal version server software (see, for example, 
                <ref target=""http://portal.textometrie.org/demo"">http://portal.textometrie.org/demo</ref>) for online corpus distribution and analysis. Time permitting, TEI encoding aspects of corpora related to TXM could also be introduced, as well as speech transcriptions or parallel corpora encoding and analysis.
            </p>
            <p rend=""Standard"">Such tutorials have been given on a monthly basis in Lyon (France) since September 2012 (see, in French, 
                <ref target=""https://groupes.renater.fr/wiki/txm-users/public/ateliers_txm"">https://groupes.renater.fr/wiki/txm-users/public/ateliers_txm</ref>).
            </p>
            <p rend=""Standard"">It has proven to be very beneficial to participants from various fields of the humanities working on digital textual data: geography, history, linguistics, literary studies, sociology, psychology, urbanism, political sciences, economy, etc.</p>
            <p rend=""Standard"">The tutorial will be taught in English and will complement two accepted communications introducing the TXM platform given during the conference:</p>
            <p rend=""Standard""> • ‘Progressive Philology with TXM: From “Raw Text” to “TEI Encoded Text” Analysis and Mining’, #463.</p>
            <p rend=""Standard""> • ‘From KWIC Concordance to Video Excerpt or Folio Facsimile: Demonstration of Multimodal and Multimedia Corpora in TXM’, #468.</p>
            <p rend=""Standard"">
                <hi rend=""bold"">Tutorial Instructor</hi>
            </p>
            <p rend=""Standard"">Serge Heiden (slh@ens-lyon.fr)</p>
            <p rend=""Standard"">Project Manager of TXM Platform Development (
                <ref target=""http://textometrie.ens-lyon.fr/spip.php?article9"">http://textometrie.ens-lyon.fr/spip.php?article9</ref>) 
            </p>
            <p rend=""Standard"">
                <hi rend=""italic"">S. Heiden</hi> develops the textometry content analysis methodology in a research team of five people through the development of tools able to process richly encoded corpora (
                <ref target=""http://icar.univ-lyon2.fr/pages/equipe31.htm"">http://icar.univ-lyon2.fr/pages/equipe31.htm</ref>). Working on the relation between analysis tools and XML-TEI encoded corpora, he is involved in the TEI consortium activities as the TEI Tools SIG convener (
                <ref target=""http://www.tei-c.org/Activities/SIG/Tools"">http://www.tei-c.org/Activities/SIG/Tools</ref>).
            </p>
            <p rend=""Standard"">
                <hi rend=""bold"">Target Audience and Expected Number of Participants</hi>
            </p>
            <p rend=""Standard"">Participants can come from any humanities and social sciences disciplines. No previous statistical or XML background is necessary. Participants can come with their own corpora.</p>
            <p rend=""Standard"">The ideal number of participants is about 12 to 15 people; the maximum number of participants is about 20.</p>
            <p rend=""Standard"">Each participant should come with her own laptop computer.</p>
            <p rend=""Standard"">The tutorial needs to run at least for a full day*: typically half day for TXM tools fundamentals and half day for main corpus formats fundamentals (TXT and XML) and input procedures into the platform.</p>
            <p rend=""Standard"">*The regular TXM tutorials run for two days (one-day TXM introduction, one-day corpus formating and import into TXM).</p>
            <p rend=""Standard"">Brief Outline</p>
            <p rend=""Standard"">
                <hi rend=""italic"">9am–12pm (3h) + 1pm–5pm (4h) = 7h total</hi>
            </p>
            <p rend=""Standard"">Install and introduction: 45 minutes</p>
            <p rend=""Standard""> • TXM, TreeTagger, sample corpus installation checkup (participants will be asked to install the software before coming to the workshop to save time).</p>
            <p rend=""Standard""> • TXM user interface &amp; windows, corpus Description command.</p>
            <p rend=""Standard"">
                <hi rend=""italic"">Main tools: 2 hours, 15 minutes</hi>
            </p>
            <p rend=""Standard""> • Lexicon analysis and spreadsheet export.</p>
            <p rend=""Standard""> • Index building for distributional semantics and Corpus Query Language syntax.</p>
            <p rend=""Standard""> • Concordance and Edition browsing, Progression graphics.</p>
            <p rend=""Standard""> • Sub-corpus building, corpus partitioning, and specificity/factorial analysis/clustering.</p>
            <p rend=""Standard""> • Words co-occurrence analysis.</p>
            <p rend=""Standard""> • TXM portal demo (optional).</p>
            <p rend=""Standard""> • TXM community: mailing lists, websites, and documentation.</p>
            <p rend=""Standard"">Importing corpora into TXM: 4 hours</p>
            <p rend=""Standard""> • TXM import strategy and main corpus formats: TXT-Unicode+CSV, XML+CSV, XML-TEI: 1/2 hour.</p>
            <p rend=""Standard""> • TXT-Unicode sample corpus and TXT+CSV import into TXM, sample analysis: 1 hour, 15 minutes.</p>
            <p rend=""Standard""> • Introduction to XML and to TXT2XML conversion tools: 1/2 hour.</p>
            <p rend=""Standard""> • XML sample corpus and XML/w+CSV import into TXM, sample analysis: 1 hour, 45 minutes. </p>
        </body>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,nlp;tei;text analysis;txm;xml,English,concording and indexing;content analysis;corpora and corpus activities;data mining / text mining;encoding - theory and practice;english;natural language processing;text analysis;visualization;xml
9447,2020 - Ottawa,Ottawa,carrefours / intersections,2020,ADHO,ADHO,Carleton University;Université d'Ottawa (University of Ottawa),Ottawa,Ontario,Canada,https://dh2020.adho.org/,Sounding Spirit Overlays and Understories: OCR and Consortial Thematic Research Collections,,Jesse P. Karlsberg,"paper, specified ""short paper""","This presentation focuses on Sounding Spirit, an NEH-funded pilot public digital humanities project curating thematic research collections of digitized American vernacular sacred songbooks. Sounding Spirit uses Readux, a new platform developed by the Emory Center for Digital Scholarship that enables browsing, annotation, and publishing with federated collections of digitized books. First, I discuss how Sounding Spirit’s corpus of nineteenth- and early twentieth-century songbooks poses unique challenges that require enhancements to contemporary optical character recognition (OCR). These challenges outline new avenues for research that will enhance textual scholarship with large corpora of digitized works. Second, I propose Sounding Spirit’s model of interinstitutional collaboration as a replicable pathway for enhancing engagement with digitized books.",txt,This text is republished here with permission from the original rights holder.,,American music;iiif;ocr;public digital humanities;thematic research collections,English,"19th century;20th century;book and print history;contemporary;digital libraries creation, management, and analysis;english;musicology;north america;optical character recognition and handwriting recognition"
9453,2020 - Ottawa,Ottawa,carrefours / intersections,2020,ADHO,ADHO,Carleton University;Université d'Ottawa (University of Ottawa),Ottawa,Ontario,Canada,https://dh2020.adho.org/,Algorithms of Resistance: Using OCR and AI for Social Justice,,Nathan Kelber,"paper, specified ""long paper""","This long presentation will share key findings from the final report of On the Books: Jim Crow and Algorithms of Resistance, a Mellon-funded Collections as Data project using machine learning to systematically discover racism within North Carolina laws. The project will make North Carolina legal history accessible to researchers by creating a corpus of over one hundred years of North Carolina public, private, and local session laws and resolutions from the end of civil war through the civil rights movement (1865-1968). This project represents the intersection of many subject areas including critical race theory, American history, text and data mining, machine learning, and public humanities. It will be of interest to subject specialists, data science researchers, educators, librarians, and other information professionals.",txt,This text is republished here with permission from the original rights holder.,,algorithmic bias;jim crow;ocr;tdm,English,19th century;20th century;africa;african and african american studies;english;law and legal studies;north america;optical character recognition and handwriting recognition;text mining and analysis
9752,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Digital Fragmenta Historicorum Graecorum (DFHG),,Monica Berti,"paper, specified ""long paper""","<text>
        
            <p>This long paper presents the 
                <hi rend=""italic"">Digital Fragmenta Historicorum Graecorum</hi> (DFHG) project (
                <ref target=""http://www.dfhg-project.org/"">http://www.dfhg-project.org</ref>). The 
                <hi rend=""italic"">DFHG</hi> is the digital version of the five volumes of the 
                <hi rend=""italic"">Fragmenta Historicorum Graecorum</hi> (FHG), which is the first big collection of ancient Greek historical fragments published by Karl Müller (1841-1873). The 
                <hi rend=""italic"">FHG</hi> is a corpus of quotations and text reuses (fragmenta) of 636 ancient Greek fragmentary historians preserved by Classical sources. Fragmentary authors date from the 6th century BC through the 7th century CE and, except for the first volume, are chronologically distributed. Fragments are numbered sequentially and arranged by works and book numbers with Latin translations, commentaries, and critical notes.
            </p>
            <p>The 
                <hi rend=""italic"">DFHG</hi> is not a new edition of ancient Greek fragmentary historians, but a new digital resource to provide textual, philological, and computational methods for representing fragmentary authors and works in a digital environment. The reason for choosing the 
                <hi rend=""italic"">Fragmenta Historicorum Graecorum</hi> depends not only on an interest in Greek fragmentary historiography, which provides a rich collection of complex reuses of prose texts, but also on the necessity of digitizing printed editions and preserving them as structured machine readable corpora that can be accessed for experimenting with text mining of historical languages. Moreover, the 
                <hi rend=""italic"">FHG</hi> is still fundamental to understand recent editions of Greek historical fragments and in particular 
                <hi rend=""italic"">Die Fragmente der griechischen Historiker</hi> (FGrHist) by Felix Jacoby, who spent his life to change and improve the collection created by Karl Müller. Finally, the corpus of the 
                <hi rend=""italic"">FHG</hi> is open and big enough to perform computational experiments and obtain results.
            </p>
            <p>This paper presents tools and services that have been developed by the project, not only for accessing the entire collection of the 
                <hi rend=""italic"">FHG</hi>, but also for providing a new model that can be applied to other collections of fragmentary authors in order to visualize and explore their complex data and connect it with external resources for further developments. The presentation is organized according to the following topics:
            </p>
            <list type=""unordered"">
                <item>
                    <hi rend=""bold"">Visualization of DFHG contents.</hi> The 
                    <hi rend=""italic"">DFHG</hi> appears as an Ajax web page automatically generated by a PHP script querying an SQL database of the entire 
                    <hi rend=""italic"">FHG</hi>, which is accessible by browsing the whole collection or single volumes through a slide in/out navigation menu. The navigation menu allows scholars to navigate the 
                    <hi rend=""italic"">FHG</hi> with a comprehensive and detailed view of the structure of the entire collection and to jump to the relevant section without reloading the page. This kind of visualization is very helpful because the printed version of the 
                    <hi rend=""italic"">FHG</hi> doesn’t contain detailed tables of contents of its volumes, but only short and sometimes incomplete lists of authors published in the collection.
                </item>
                <item>
                    <hi rend=""bold"">Access to the DFHG.</hi> The 
                    <hi rend=""italic"">DFHG Digger</hi> filters the whole collection according to authors, works, work sections, and book numbers, while the 
                    <hi rend=""italic"">DFHG Search</hi> function is performed on fragments, translations, commentaries and source texts. Results show the number of occurrences in each 
                    <hi rend=""italic"">DFHG</hi> author and searched words are highlighted in the text. They also display, when available, the lemmatization of inflected forms and the disambiguation of named entities through external resources. The 
                    <hi rend=""italic"">DFHG</hi> provides a web API that can be queried with author names and fragment numbers. The result is a JSON output containing every piece of information about the requested fragment (e.g., 
                    <ref target=""http://www.dfhg-project.org/DFHG/api.php?author=ACUSILAUS&amp;fragment=10"">http://www.dfhg-project.org/DFHG/api.php?author=ACUSILAUS&amp;fragment=10</ref>). The 
                    <hi rend=""italic"">DFHG</hi> exports data to CSV and XML format files (both as EpiDoc XML and well formed XML).
                </item>
                <item>
                    <hi rend=""bold"">Integration with external resources.</hi> One of the main goals of the project is to make the 
                    <hi rend=""italic"">DFHG</hi> part of a bigger infrastructure of processed data. This is the reason why the 
                    <hi rend=""italic"">DFHG</hi> is integrated with external resources such as textual collections, authority lists, dictionaries, lexica and gazetteers. These resources are fundamental for disambiguating and annotating 
                    <hi rend=""italic"">DFHG</hi> data, which in turn offers a collection of parsed texts for enriching external libraries of Greek and Latin sources. The 
                    <hi rend=""italic"">DFHG</hi> is currently connected to different resources that provide morpho-syntactic information and named entities disambiguation of textual data of the 
                    <hi rend=""italic"">FHG</hi>. The 
                    <hi rend=""italic"">DFHG</hi> provides also a 
                    <hi rend=""italic"">Müller-Jacoby Table of Concordance</hi>, which is a complete correspondence between fragmentary historians published in the 
                    <hi rend=""italic"">FHG</hi> and in 
                    <hi rend=""italic"">Die Fragmente der griechischen Historiker</hi> including the 
                    <hi rend=""italic"">continuatio</hi> and the 
                    <hi rend=""italic"">Brill's New Jacoby</hi> (
                    <ref target=""http://www.dfhg-project.org/Mueller-Jacoby-Concordance/"">http://www.dfhg-project.org/Mueller-Jacoby-Concordance</ref>). The goal of this resource is to go beyond the 
                    <hi rend=""italic"">FHG</hi> corpus and produce a complete catalog of fragmentary authors of Greek literature published in different digital editions. This resource is progressively ingested into the 
                    <hi rend=""italic"">Perseus Catalog</hi> (
                    <ref target=""http://catalog.perseus.org/"">http://catalog.perseus.org</ref>).
                </item>
                <item>
                    <hi rend=""bold"">Data citation.</hi> It is possible to retrieve and export citations of 
                    <hi rend=""italic"">DFHG</hi> fragments and source texts down to the word level using URN identifiers. These URNs are combinable with a URL prefix (http://www.dfhg-project.org/DFHG/#) to generate stable links. The syntax of each URN represents the editorial work of Karl Müller, who has arranged the fragments in a sequence and has attributed them to fragmentary authors, works, work sections and book numbers (e.g., urn:lofts:fhg.1.hecataeus.hecataei_fragmenta.genealogiae.liber_secundus:350). The 
                    <hi rend=""italic"">DFHG</hi> provides also CITE URNs according to the guidelines of the CITE Architecture (
                    <ref target=""http://cite-architecture.org/"">http://cite-architecture.org/</ref>).
                </item>
                <item>
                    <hi rend=""bold"">Source Catalogs.</hi> The 
                    <hi rend=""italic"">DFHG</hi> includes a 
                    <hi rend=""italic"">Fragmentary Authors Catalog</hi> and a 
                    <hi rend=""italic"">Witnesses Catalog</hi> that have been created from 
                    <hi rend=""italic"">FHG</hi> data. These catalogs allow users to search and visualize the 636 Greek fragmentary historians of the collection and each of their witnesses (i.e., authors who preserve quotations and text reuses of the fragmentary historians). Data from both catalogs has been used to generate charts for visualizing chronological distributions and statistics of 
                    <hi rend=""italic"">FHG</hi> authors and their source texts. This data integrates also 
                    <hi rend=""italic"">Pleiades</hi> identifiers with geo-locations that have been used for producing maps that visualize the geographical distribution of 
                    <hi rend=""italic"">FHG</hi> authors and their witnesses.
                </item>
                <item>
                    <hi rend=""bold"">Text Reuse Detection.</hi> The 
                    <hi rend=""italic"">DFHG</hi> project offers experimental text reuse functionalities for automatic text reuse detection of 
                    <hi rend=""italic"">FHG</hi> fragmentary historians. This resource allows users to automatically detect text reuses (fragmenta) of 
                    <hi rend=""italic"">FHG</hi> authors in their witnesses. Users can insert an XML file URL or select one of the 
                    <hi rend=""italic"">PerseusDL</hi> or 
                    <hi rend=""italic"">Open Greek and Latin</hi> editions available in the 
                    <hi rend=""italic"">DFHG</hi>. Results display quotations and text reuses of 
                    <hi rend=""italic"">FHG</hi> authors within their source texts. The 
                    <hi rend=""italic"">DFHG</hi> allows scholars to download complete XML files of the source texts of the fragments with dfhg attributes that mark up the presence of 
                    <hi rend=""italic"">DFHG</hi> text reuses in the relevant passages of the source texts. 
                    <hi rend=""italic"">DFHG</hi> text reuse detection is based on the Smith-Waterman algorithm that performs local sequence alignment to detect similarities between strings.
                </item>
                <item>
                    <hi rend=""bold"">OCR Editing.</hi> The digital version of the 
                    <hi rend=""italic"">DFHG</hi> has been produced starting from the OCR output of the printed edition of the 
                    <hi rend=""italic"">FHG</hi>. Even if it is possible to obtain very good results when OCRing 19th-century editions of ancient Greek and Latin sources, OCRed texts still contain errors. The 
                    <hi rend=""italic"">DFHG</hi> offers an interface for manual OCR correction of source texts, fragments, Latin translations and commentaries. Corrections are validated or rejected by the project team through an administration page.
                </item>
            </list>
            <p>Further developments of the 
                <hi rend=""italic"">DFHG</hi> project aim at implementing named entities recognition in the texts of Greek and Latin 
                <hi rend=""italic"">fragmenta</hi> and in contributing to enrich the number of lemmata and inflected forms of Greek and Latin thesauri. The final goal of the project is to offer a new methodology based on digital and computational approaches to represent complex historical text reuse data. The 
                <hi rend=""italic"">DFHG</hi> also offers an open collection of quotations and text reuses of Greek fragmentary historians. This resource provides the community of scholars and students with machine processable data for historical and computational research.
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Berti, M.</hi> (2018). “Annotating Text Reuse within the Context: the 
                        <hi rend=""italic"">Leipzig Open Fragmentary Texts Series</hi> (LOFTS)”. In Tischer, U., Gärtner, U. and Forst, A. (eds), 
                        <hi rend=""italic"">Text, Kontext, Kontextualisierung. Moderne Kontextkonzepte und antike Literatur</hi>. Hildesheim, Zürich, and New York: Olms, 223-234.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Berti, M.</hi> (2019). “Historical Fragmentary Texts in the Digital Age”. In Berti, M. (ed), 
                        <hi rend=""italic"">Digital Classical Philology. Ancient Greek and Latin in the Digital Revolution</hi>. Berlin and Boston: De Gruyter, 257-276. doi: 10.1515/9783110599572-015
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Berti, M., Almas, B. and Crane, G.R.</hi> (2016). “The 
                        <hi rend=""italic"">Leipzig Open Fragmentary Texts Series</hi> (LOFTS)”. In Bernstein, N.W. and Coffee, N. (eds), 
                        <hi rend=""italic"">Digital Methods and Classical Studies</hi>. DHQ Themed Issue 10(2). 
                        <ref target=""http://www.digitalhumanities.org/dhq/vol/10/2/000245/000245.html"">http://www.digitalhumanities.org/dhq/vol/10/2/000245/000245.html</ref> (accessed 13 April 2019)
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Berti, M., Almas, B., Dubin, D., Franzini, G., Stoyanova, S. and Crane, G.R.</hi> (2014-2015). “The Linked Fragment: TEI and the Encoding of Text Reuses of Lost Authors”. 
                        <hi rend=""italic"">Journal of the Text Encoding Initiative</hi> 8. doi: 10.4000/jtei.1218
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Berti, M., Blackwell, C. W., Daniels, M., Strickland, S. and Vincent-Dobbins, K.</hi> (2016). “Documenting Homeric Text-Reuse in the 
                        <hi rend=""italic"">Deipnosophistae</hi> of Athenaeus of Naucratis”. In Bodard, G., Broux, Y. and Tarte, S. (eds), 
                        <hi rend=""italic"">Digital Approaches and the Ancient World</hi>. BICS Themed Issue 59(2): 121-139. doi: 10.1111/j.2041-5370.2016.12042.x
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Berti, M., Crane, G. R., Yousef, T., Bizzoni, Y., Boschetti, F. and Del Gratta, R.</hi> (2016). “
                        <hi rend=""italic"">Ancient Greek WordNet</hi> Meets the 
                        <hi rend=""italic"">Dynamic Lexicon</hi>: the Example of the Fragments of the Greek Historians”. In Mititelu, V.B., Forǎscu, C., Fellbaum, C. and Vossen, P. (eds), 
                        <hi rend=""italic"">Proceedings of the Eighth Global WordNet Conference, Bucharest, Romania, January 27-30</hi>. Bucharest, 34-38.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,fragmenta;historical corpora;ocr;philology;text-reuse,English,classical studies;corpus and text analysis;databases & dbms;digital textualities and hypertext;english;linguistics;open content and open science;scholarly publishing
9768,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Adapting a system for Named Entity Recognition and Linking for 19th century French Novels,,Aicha Soudani;Yosra Meherzi;Asma Bouhafs;Francesca Frontini;Carmen Brando;Yoann Dupont;Frédérique Mélanie-Becquet,poster / demo / art installation,"<text>
        
            <p>The annotation and linking of Named Entities - be it People, Places, or other proper names - in novels is an important task both for the creation of quality Digital Editions as well as for Digital Literary Stylistics and Spatial Humanities, which largely rely on Distant Reading techniques involving among other things spatialisation (Gregory et al., 2015) and network analysis (Elson et al., 2010). </p>
            <p>Automatic NERC (Named Entity Recognition and Classification) algorithms can be crucial in this sense, but they are often developed and tested on contemporary newspaper corpora and need to be adapted. As to NEL (Named Entity Linking), namely the task of referencing entities with links to external Knowledge Bases (KB), the additional issue arises of finding the correct reference base containing knowledge from the past.</p>
            <p>Also, the question of formats is crucial; the Text Encoding Initiative (TEI) is the format of choice for most digital editions in the humanities, but the majority of existing Natural Language Processing tools are not suited to support this type of input and require specific formats such as CONLL, which weren’t built to preserve text structure. </p>
            <p>In this poster, we describe a pipeline combining two different tools - SEM for NERC (Dupont, 2017) and REDEN for NEL (Brando et al., 2016; Frontini et al., 2016) - and its adaptation for the annotation of 19th century French novels. This work is set within the context of a larger initiative aiming at creating a “Time Machine” project for the city of Paris, following the example of the “Venice Time machine” (di Lenardo and Kaplan, 2015). The input of the pipeline is an XML-TEI edition and the output is an enriched version with tagged and identified Named Entities. REDEN provides also the necessary input for a dynamic cartography, based on information in the KB.</p>
            <p>
                <anchor xml:id=""id_docs-internal-guid-4119148b-7fff-c6b1-b230-3264786a89f6""></anchor>
                <hi rend=""color(#000000)"">
                    <figure>
                        <graphic url=""https://lh3.googleusercontent.com/mk321wrrWRLTMaOyojpuAoz687b0SjzPwbKn9KfUYAm648EIruWHsC4PNT8JBIIedvkIKfaPe8etyUw_w2Ciut7rmd-KIv_m71ii-eYC9ypfHw32ZZn2JXioOCrpclOiVh4hGrqB""></graphic>
                    </figure>
                </hi>
            </p>
            <p>
                <anchor xml:id=""id_docs-internal-guid-ab279ee4-7fff-8226-726b-c86ce4a9927f""></anchor>Figure 1 - The overall pipeline.
            </p>
            <p>
                <anchor xml:id=""id_docs-internal-guid-8c2db3ba-7fff-a22c-b0c9-2bbece654bbc""></anchor>
                <hi rend=""color(#000000)"">The corpus consists of the first two chapters of </hi>
                <hi rend=""color(#000000)italic"">Le Ventre de Paris</hi>
                <hi rend=""color(#000000)""> </hi>
                <hi rend=""color(#000000)"">(Zola, 1873) and the first chapter of </hi>
                <hi rend=""color(#000000)italic"">Cesar Birotteau</hi>
                <hi rend=""color(#000000)""> </hi>
                <hi rend=""color(#000000)"">(Balzac, 1837), the size is 30,889 words and both texts were annotated for places and fictional characters by two separate annotators; an inter-annotator agreement of 0,91 (F-measure) indicates a strong overall agreement. An Internationalized Resource Identifier (IRI) was added for those place names for which a reference existed in the KB.</hi>
            </p>
            <p>
                <ref target=""https://github.com/YoannDupont/SEM"">SEM</ref>
                <hi rend=""color(#000000)italic""> </hi>
                <hi rend=""color(#000000)"">is a machine learning system based on conditional random fields (CRF) models; its default French model for NER performs poorly on our corpus. We thus re-trained SEM using our gold standard, and evaluated the domain adaptation with two setups: </hi>
            </p>
            <list type=""unordered"">
                <item>Setup1: training on the first chapter of Zola and tested on the second one </item>
                <item>
                    <p>Setup2: training on the Zola subcorpus and tested on Balzac.</p>
                </item>
            </list>
            <table rend=""frame"" xml:id=""Tableau1"">
                <row>
                    <cell></cell>
                    <cell>Setup 1</cell>
                    <cell>Setup 2</cell>
                </row>
                <row>
                    <cell>Precision</cell>
                    <cell>1</cell>
                    <cell>0,7</cell>
                </row>
                <row>
                    <cell>Recall</cell>
                    <cell>0,69</cell>
                    <cell>0,26</cell>
                </row>
                <row>
                    <cell>F-measure</cell>
                    <cell>0,82</cell>
                    <cell>0,38</cell>
                </row>
            </table>
            <p>Results for Setup 1 are quite encouraging: they show that the model trained on one chapter performs very well on another. This means it could  in principle suffice to manually annotate (or correct) one chapter to obtain a system that can annotate the rest of the novel with an acceptable accuracy. Results for Setup 2 show that even two novels that are relatively close from a temporal, geographical and stylistic point of view are sufficiently different to cause an important drop in performance when the model trained on one is applied on the other, with important consequences for our ongoing work to constitute an adapted NERC model for 19th century French novels.</p>
            <p>
                <ref target=""http://github.com/cvbrandoe/REDEN"">REDEN</ref>
                <hi rend=""color(#000000)italic""> </hi>
                <hi rend=""color(#000000)"">relies on graph-based algorithms and on Semantic Web technologies (see Figure 2), and like many NEL algorithms, is composed of two phases, candidate retrieval and disambiguation. It was designed for DH applications and is adaptable to various domains thanks to the possibility of using different KBs.</hi>
            </p>
            <p>
                <figure>
                    <graphic url=""https://lh3.googleusercontent.com/FdQy0iRuzXGNPK2Rqz5TrpEeLsX9sQr44AqmdHLYXXJ5VNGgaG8MgdytZif_I-17xPbIXhWg4s1-LJAC17Txn_Troig6oD3Er8h1w0xJWcH-tSLgpPtn0NJS66NBnLTLM-i9AFAF""></graphic>
                </figure>Figure 2 - The REDEN model.
            </p>
            <p>
                <hi rend=""color(#000000)"">For this experiment, REDEN was tested on the task of referencing </hi>
                <hi rend=""color(#000000)italic"">placeNames</hi>
                <hi rend=""color(#000000)"">, using three different Kbs (see table below), a complete description of the evaluation metrics used in this table, and which are commonly used for NEL, can be found in Brando et al., (2016). The best configuration in terms of overall accuracy is the one using Wikidata; however, </hi>
                <ref target=""https://data.bnf.fr/"">BnF</ref>
                <hi rend=""color(#000000)""> </hi>
                <hi rend=""color(#000000)"">is a more accurate source of information for old place names (e.g. it records the older name “château de Bicêtre” for the “fort de Bicêtre”). Disambiguation accuracy is an interesting measure when there is more than one candidates for a place mention, and for Wikidata the value is strong. Weak values on NIL precision tell us that REDEN, which uses exact string match for retrieving candidates, sometimes misses the correct referent, and needs to be improved in this respect.</hi>
            </p>
            <table rend=""frame"" xml:id=""Tableau2"">
                <row>
                    <cell>
                        <p>KB</p>
                    </cell>
                    <cell>Candidate Precision</cell>
                    <cell>Candidate recall</cell>
                    <cell>NIL precision</cell>
                    <cell>NIL recall</cell>
                    <cell>Disambiguation accuracy</cell>
                    <cell>Overall linking accuracy</cell>
                </row>
                <row>
                    <cell>DBpedia</cell>
                    <cell>1</cell>
                    <cell>
                        <anchor xml:id=""id_docs-internal-guid-99656eb6-7fff-ddd9-4d4a-a2d87f66d5c5""></anchor>
                        <hi rend=""color(#000000)"">0,816</hi>
                    </cell>
                    <cell>
                        <p>
                            <anchor xml:id=""id_docs-internal-guid-a4f041b7-7fff-0b3a-502d-e301de2c78f5""></anchor>
                            <hi rend=""color(#000000)"">0,367</hi>
                        </p>
                    </cell>
                    <cell>
                        <p>
                            <anchor xml:id=""id_docs-internal-guid-34122790-7fff-1c22-d7c4-8e38b9ce7e07""></anchor>
                            <hi rend=""color(#000000)"">1</hi>
                        </p>
                    </cell>
                    <cell>
                        <p>
                            <anchor xml:id=""id_docs-internal-guid-1c831f45-7fff-2a54-5e3c-62e67ebb471f""></anchor>
                            <hi rend=""color(#000000)"">none</hi>
                        </p>
                    </cell>
                    <cell>
                        <p>
                            <anchor xml:id=""id_docs-internal-guid-4f3343f3-7fff-4aa7-4214-5f23b58d823b""></anchor>
                            <hi rend=""color(#000000)"">0,834</hi>
                        </p>
                    </cell>
                </row>
                <row>
                    <cell>BNF</cell>
                    <cell>0,76</cell>
                    <cell>0,63</cell>
                    <cell>0,58</cell>
                    <cell>0,97</cell>
                    <cell>1</cell>
                    <cell>0,7</cell>
                </row>
                <row>
                    <cell>Wikidata</cell>
                    <cell>0,91</cell>
                    <cell>0,83</cell>
                    <cell>0,44</cell>
                    <cell>1</cell>
                    <cell>1</cell>
                    <cell>0,85</cell>
                </row>
            </table>
            <p>Geo-visualisation</p>
            <p>
                <hi rend=""color(#000000)"">Once the digital edition is enriched with </hi>
                <hi rend=""color(#000000)italic"">placeName</hi>
                <hi rend=""color(#000000)""> </hi>
                <hi rend=""color(#000000)"">tags, REDEN allows for the exploration of the spatial dimension of texts by retrieving structured information about places. For instance, in Wikidata, resources are described according to an ontology which contains properties for coordinate locations &lt;https://www.wikidata.org/wiki/Property:P625&gt;, images &lt;https://www.wikidata.org/wiki/Property:P18&gt;. By dereferencing IRIs, it is possible to access values for the aforementioned properties and use them in the context of a Web mapping application thereby to project places as points onto a map along with media or facts about these places (see Figures below). </hi>
            </p>
            <p>
                <hi rend=""color(#000000)"">
                    <figure>
                        <graphic url=""https://lh4.googleusercontent.com/C8gmjLVDz8-CzQNcQra1Eh5v14axHBkCIaZLWLRpam8-Lkx5vagfDHhHaJjEYhB6qX2LjwlkbmR4zKh1bNT-NpM9oxcyRWKHdnD8TC9ArCGI0pGc0HxrzNLMEktvqx24CDlVuIv6""></graphic>
                    </figure>
                </hi>
                <anchor xml:id=""id_docs-internal-guid-54d0ebb0-7fff-3950-49fa-a625c17b168d""></anchor>
                <hi rend=""color(#000000)"">
                    <figure>
                        <graphic url=""https://lh6.googleusercontent.com/afl1jdoX8HgHPhACtguDp5STNhcXlywSgGsswpvDScjk3RCksQ7NDWYfG9o-riRjW78QOSGre0CBsCeQmCXRuwswm8S9z5pogDcX2p1aa-yXwoC8cLXRLnEBbsEbBmcA5Lcg9HDV""></graphic>
                    </figure>
                </hi>
                <hi rend=""color(#000000)""> </hi>
                <lb></lb>
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <anchor xml:id=""id_docs-internal-guid-8d00ea7e-7fff-e7f1-2c5a-c0e8d11e3b24""></anchor>
                        <hi rend=""color(#000000)bold"">Brando, C., Frontini, F. and Ganascia, J.-G.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2016). REDEN: Named Entity Linking in Digital Literary Editions Using Linked Data Sets. </hi>
                        <hi rend=""color(#000000)italic"">Complex Systems Informatics and Modeling Quarterly</hi>
                        <hi rend=""color(#000000)"">, </hi>
                        <hi rend=""color(#000000)bold"">0</hi>
                        <hi rend=""color(#000000)"">(7): 60–80.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Boeglin, N., Depeyre, M., Joliveau, T. and Le Lay, Y.-F.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2016). Pour une cartographie romanesque de Paris au XIXe siècle. Proposition méthodologique. </hi>
                        <hi rend=""color(#000000)italic"">Conférence Spatial Analysis and GEOmatics</hi>
                        <hi rend=""color(#000000)"">. (Actes de La Conférence SAGEO’2016 - Spatial Analysis and GEOmatics). Nice, France</hi>
                        <ref target=""https://hal.archives-ouvertes.fr/hal-01619600"">
                            <hi rend=""color(#000000)""> </hi>
                        </ref>
                        <ptr target=""https://hal.archives-ouvertes.fr/hal-01619600""></ptr>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(accessed 24 November 2018).</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Dupont, Y.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2017). Exploration de traits pour la reconnaissance d’entités nommées du Français par apprentissage automatiqu. Proceedings of RECITAL.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Elson, D. K., Dames, N. and McKeown, K. R.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2010). Extracting Social Networks from Literary Fiction. </hi>
                        <hi rend=""color(#000000)italic"">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</hi>
                        <hi rend=""color(#000000)"">. (ACL ’10). Stroudsburg, PA, USA: Association for Computational </hi>
                        <hi rend=""color(#000000)"">Linguistics, pp. 138–147</hi>
                        <ref target=""http://dl.acm.org/citation.cfm?id=1858681.1858696"">
                            <hi rend=""color(#000000)""> </hi>
                        </ref>
                        <ptr target=""http://dl.acm.org/citation.cfm?id=1858681.1858696""></ptr>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(accessed 24 November 2018).</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Frontini, F., Brando, C. and Ganascia, J. G.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2016). REDEN ONLINE: Disambiguation, Linking and Visualisation of References in TEI Digital Editions. </hi>
                        <hi rend=""color(#000000)italic"">Digital Humanities 2016: Conference Abstracts</hi>
                        <hi rend=""color(#000000)"">. Kraków: Jagiellonian University &amp; Pedagogical University, pp. 193–97</hi>
                        <ref target=""http://dh2016.adho.org/abstracts/362"">
                            <hi rend=""color(#000000)""> </hi>
                        </ref>
                        <ptr target=""http://dh2016.adho.org/abstracts/362""></ptr>
                        <hi rend=""color(#000000)"">.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Gregory, I., Donaldson, C., Murrieta-Flores, P. and Rayson, P.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2015). Geoparsing, GIS and textual analysis: current developments in spatial humanities research. </hi>
                        <hi rend=""color(#000000)italic"">International Journal of Humanities and Arts Computing</hi>
                        <hi rend=""color(#000000)"">, </hi>
                        <hi rend=""color(#000000)bold"">9</hi>
                        <hi rend=""color(#000000)"">(1): 1–14 doi:</hi>
                        <ref target=""https://doi.org/10.3366/ijhac.2015.0135"">
                            <hi rend=""color(#1155cc)underline"">10.3366/ijhac.2015.0135</hi>
                        </ref>
                        <hi rend=""color(#000000)"">.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Lenardo, I. di and Kaplan, F.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2015). Venice Time Machine : Recreating the density of the past. https://infoscience.epfl.ch/record/214895 (accessed 25 November 2018).</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Piatti, B., Bär, H. R., Reuschel, A.-K., Hurni, L. and Cartwright, W.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2009). Mapping Literature: Towards a Geography of Fiction. </hi>
                        <hi rend=""color(#000000)italic"">Cartography and Art</hi>
                        <hi rend=""color(#000000)"">. Berlin, Heidelberg: Springer, pp. 1--16</hi>
                        <ref target=""http://link.springer.com/chapter/10.1007/978-3-540-68569-2_15"">
                            <hi rend=""color(#000000)""> </hi>
                        </ref>
                        <ptr target=""http://link.springer.com/chapter/10.1007/978-3-540-68569-2_15""></ptr>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(accessed 11 January 2016).</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Piper, A., Algee-Hewitt, M., Sinha, K., Ruths, D. and Vala, H.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2017). Studying Literary Characters and Character Networks. </hi>
                        <hi rend=""color(#000000)italic"">Digital Humanities 2017, DH 2017, Conference </hi>
                        <hi rend=""color(#000000)italic"">Abstracts, McGill University &amp; Université de Montréal, Montréal, Canada, August 8-11, 2017</hi>
                        <ref target=""https://dh2017.adho.org/abstracts/103/103.pdf"">
                            <hi rend=""color(#000000)""> </hi>
                        </ref>
                        <ptr target=""https://dh2017.adho.org/abstracts/103/103.pdf""></ptr>
                        <hi rend=""color(#000000)"">.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""color(#000000)bold"">Vala, H., Jurgens, D., Piper, A. and Ruths, D.</hi>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(2015). Mr. Bennet, his coachman, and the Archbishop walk into a bar but only one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts. </hi>
                        <hi rend=""color(#000000)italic"">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</hi>
                        <hi rend=""color(#000000)"">. Lisbon, Portugal: Association for Computational Linguistics, pp. 769–774</hi>
                        <ref target=""http://aclweb.org/anthology/D15-1088"">
                            <hi rend=""color(#000000)""> </hi>
                        </ref>
                        <ptr target=""http://aclweb.org/anthology/D15-1088""></ptr>
                        <hi rend=""color(#000000)""> </hi>
                        <hi rend=""color(#000000)"">(accessed 24 November 2018).</hi>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,cartography;evaluation;linked data;named entity;nlp,English,"artificial intelligence and machine learning;computer science and informatics;english;geography and geo-humanities;linking and annotation;natural language processing;spatial & spatio-temporal analysis, modeling and visualization"
9781,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,A Digital Platform for the “Latin Silk Road”: Issues and Perspectives in Building a Multilingual Corpus for Textual Analysis,,Emmanuela Carbe';Nicola Giannelli,"paper, specified ""short paper""","<text>
        
            <div type=""div1"" rend=""DH-Heading1"">
                Introduction
                <p>In March 2018 the project Eurasian Latin Archive was started with the aim of building a digital platform for a large corpus of Latin texts and documents from medieval and early modern times. The corpus encompasses text from East Asia, including Latin-Chinese texts, on which a group of researchers began to work addressing specific multilingual issues. The corpus will be available within a digital library provided with tools for textual and thematic analysis. The goal of the project is the comparative linguistic analysis, both internal with other Latin texts from different eras and areas, and external with non-Latin texts on homogeneous subjects. The ultimate purpose of the project is to highlight relationships by extracting and investigating 1) historical-cultural data about religion, law, science, art, and customs; 2) linguistic data, to be compared with homologous values in European Latin literature, in order to determine the specificity of East Asian Latin and investigate overlappings and mismatches between Latin words and their local (mostly Chinese) equivalents. The start-up phase of the project (
                    <ref target=""http://www.dasmemo.unisi.it/"">DAS-MeMo</ref>), which will end by February 2020, is co-financed by Regione Toscana and QuestIT, an IT company specialized in NLP and Artificial Intelligence. The working group is interdisciplinary and brings together medievalists, digital humanists, engineers and IT specialists. The platform is inspired by the digital archive 
                    <ref target=""http://alim.unisi.it/"">ALIM</ref> (Archivio della Latinità Italiana del Medioevo), which acts as a starting point for the new archive. However, unlike ALIM, the project is geared towards providing textual analysis also with machine learning tools; ALIM focuses instead on digital representation of editions (including editiones principes) and takes into account texts of Italian Latinity exclusively (Ferrarini, 2017; Stella, 2015).
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Preliminary complexities: defining and creating the corpus
                <p>To define the corpus, a first census of over three hundred texts has been made (also thanks to online resources, such as 
                    <ref target=""https://www.univie.ac.at/Geschichte/China-Bibliographie/blog/"">Sinica 2.0</ref> and 
                    <ref target=""http://heron-net.be/pa_cct/index.php/Search/advanced/ccts"">CCT-Database</ref>). The amount of documents is due to increase, but it is sufficiently large to collect some preliminary results in the main projects’ tasks. It seems useful to classify documents into four main categories: 1. Born-digital editions, freely available or to request for; 2) modern (critical and non critical) editions that have entered the public domain; 3. editions with possible issues with OCR; 4. manuscripts. With the exception of the born-digital editions, the categories can a) be already digitized and made available by other Institutions; b) still need to be digitized. At this stage, besides the born-digital editions already acquired, seventy more documents of the type 2a and 2b have already been processed. In the case of critical editions, we chose to provide the text without encoding the critical apparatus. Items are being encoded in TEI P5, with a particular attention for the essentials metadata, such as VIAF and Wikidata ID when available, OCLC references, reliability both for external and internal processes of the item (i.e. responsible for digitization and/or OCR, checking, date history of all changes, evaluation of the resource, and so on), possibly opening the way to integration with semantic models (Ciotti, 2018; Ciotti et al., 2016).
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                As a non-conclusion: a structure that is modular and can grow more complex with time
                <p>For such an ambitious project it is obviously necessary to proceed by ensuring a modular architecture from the very beginning, so that it can be implemented within this project progressively and can possibly merge, through its interoperability features, with open source projects from other Institutions. For this purpose, a requirement analysis has been carried out, and a list of specifications has been redacted in order to provide a basis of guidelines for the subsequent implementation.</p>
                <p>The aim of this paper is to display the architecture of the project and some of the early results: a prototype based on the ElasticSearch search engine has been developed, which includes a search module (with multiple filter methods) and a browse module, with the possibility to investigate the corpus by authors, dates and periods, collections, sources, places mentioned within the document, languages used in the text, keywords. A particular attention is being paid to the recognition of entities. For the moment being, we are working on recognition of places, dates and people. For the particular case of places, we also need to take into account that location names often differ significantly from the ones used nowadays, to the point that many researchers actually disagree on the current location of cities mentioned in these texts. More tools will be added to the preliminary ones, in order to pursue a text analysis (Stella, 2018: 72-100; Eger et al., 2015). One of the most important issues remains however the encoding of multilingual documents, currently under examination by handling Prospero Intorcetta’s Sapientia Sinica, which contains chinese text, and transliterations in latin of the chinese terms.</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"" style=""font-size:9pt"">Ciotti, F.</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve""> (2018). A Formal Ontology for the Text Encoding Initiative.</hi>
                        <hi rend=""italic"" style=""font-size:9pt"">Umanistica Digitale</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">, </hi>
                        <hi rend=""bold"" style=""font-size:9pt"">3</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">(2018). doi: </hi>
                        <ref target=""http://doi.org/10.6092/issn.2532-8816/8174"">
                            <hi style=""font-size:9pt"">10.6092/issn.2532-8816/8174</hi>
                        </ref>
                        <hi style=""font-size:9pt"">.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:9pt"">Ciotti, F., Daquino, M. and Tomasi F.</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve""> (2016). Text Encoding Initiative Semantic Modeling. A Conceptual Workflow Proposal. In Calvanese, D., De Nart, D. and Tasso, C. (eds), </hi>
                        <hi rend=""italic"" style=""font-size:9pt"">Digital Libraries on the Move</hi>
                        <hi style=""font-size:9pt"">, vol. 612. Cham: Springer International Publishing, pp. 48-60. doi:</hi>
                        <ref target=""http://dx.doi.org/10.1007/978-3-319-41938-1_5"">
                            <hi style=""font-size:9pt"">10.1007/978-3-319-41938-1_5</hi>
                        </ref>
                        <hi style=""font-size:9pt"">.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:9pt"" xml:space=""preserve"">Eger, S., Brück, T. vor der, and Mehler, A. </hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">(2015). Lexicon-assisted tagging and lemmatization in Latin: A comparison of six taggers and two lemmatization methods. In </hi>
                        <hi rend=""italic"" style=""font-size:9pt"">Proceedings of the 9th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH)</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">. Beijing, China, pp. 105-13. doi: </hi>
                        <ref target=""http://dx.doi.org/10.18653/v1/W15-3716"">
                            <hi style=""font-size:9pt"">10.18653/v1/W15-3716</hi>
                        </ref>
                        <hi style=""font-size:9pt"">.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:9pt"">Russo, L.</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve""> (2005). ALIM, Archivio della latinità italiana del Medioevo. </hi>
                        <hi rend=""italic"" style=""font-size:9pt"">Reti Medievali Rivista,</hi>
                        <hi rend=""bold"" style=""font-size:9pt"">6</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">(1): 149-151. doi: </hi>
                        <ref target=""http://dx.doi.org/10.6092/1593-2214/181"">
                            <hi style=""font-size:9pt"">10.6092/1593-2214/181</hi>
                        </ref>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">. </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:9pt"">Stella, F.</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">(2015). Il problema della codifica nelle edizioni critiche digitali. In Del Corso, L., De Vivo, F., Stramaglia, A., </hi>
                        <hi rend=""italic"" style=""font-size:9pt"">Nel segno del testo. Edizioni, materiali e studi per Oronzo Pecere</hi>
                        <hi style=""font-size:9pt"">. Firenze: Gonnelli, pp. 347-360.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:9pt"" xml:space=""preserve"">Stella, F. </hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">(2018). </hi>
                        <hi rend=""italic"" style=""font-size:9pt"">Testi letterari e analisi digitali</hi>
                        <hi style=""font-size:9pt"">. Roma: Carocci.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:9pt"" xml:space=""preserve"">TEI Consortium </hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve"">(2019). </hi>
                        <hi rend=""italic"" style=""font-size:9pt"">TEI P5: Guidelines for Electronic Text Encoding and Interchange.</hi>
                        <hi style=""font-size:9pt"" xml:space=""preserve""> Version 3.5.0. Last updated on 29th January 2019.</hi>
                        <ref target=""http://www.tei-c.org/release/doc/tei-p5-doc/en/html/index.html"">
                            <hi style=""font-size:9pt"">http://www.tei-c.org/release/doc/tei-p5-doc/en/html/index.html</hi>
                        </ref>
                        <hi style=""font-size:9pt"">.</hi>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,corpus and text analysis;digital archives;nlp;tei,English,artificial intelligence and machine learning;corpus and text analysis;data mining / text mining;digital archives and digital libraries;english;medieval studies;oriental and asian studies
9806,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Mapping the Indefinable: Designing a Social Network Analysis Shiny App to Explore the Influence of East-West Exchanges on Poland’s Political Transformation,,Gregory Frank Domber;Kelly Bodwin,"paper, specified ""long paper""","<text>
        
            <p>Beginning in the 1950s Americans sponsored international exchange programs for Polish scientists and professionals, believing that exposure to the West would undermine Communism. In 1989, Poles underwent a negotiated revolution. Were these extensive American public diplomacy efforts successful? Can pathways of influence and shifts in perception within specific epistemic communities be measured, mapped, and visualized longitudinally to better understand exogenous influences on Eastern Europe’s democratization process?</p>
            <p>Based on an approach to quantifying individuals’ lives based on their “institutional affiliations,” our interdisciplinary team has designed an interactive social network analysis visualization app, built in R Statistical Software using the Shiny package. The app allows users to interactively explore the overlapping networks of political revolution and international exchange, and illustrate how these connections shifted over time. This provides insights into Poland’s specific experience, as well as a model for studies of other complex, longitudinal networks.</p>
        
    </text>",xml,This text is republished here with permission from the original rights holder.,,democratization;international exchanges;longitudinal network analysis;shiny app,English,"digital humanities (history, theory and methodology);english;history and historiography;network analysis and graphs theory;prosopography;spatial & spatio-temporal analysis, modeling and visualization"
9828,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,LiLa: Linking Latin. Building a Knowledge Base of Linguistic Resources for Latin,,Marco Passarotti;Flavio M. Cecchini;Greta Franzini;Eleonora Litta;Francesco Mambrini;Paolo Ruffolo;Rachele Sprugnoli,poster / demo / art installation,"<text>
        
            <div type=""div1"" rend=""DH-Heading1"">
                Introduction
                <p style=""text-align:left; "">The spread of information technology has led to a substantial growth in the quantity, diversity and complexity of linguistic data available on the web. Today, although a large variety of linguistic resources (such as corpora, lexica and ontologies) exist for a number of languages, these are often not interoperable. Linking linguistic resources to one another would maximise their contribution to, and use in, linguistic analysis at multiple levels, be those lexical, morphological, syntactic, semantic or pragmatic. In an ideal virtuous cycle, linguistic resources benefit from the Natural Language Processing (NLP) tools used to build them, and NLP tools benefit from the linguistic resources that provide the empirical evidence upon which they are built. </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                The project
                <p style=""text-align:left; "">The objective of the 
                    <hi rend=""italic"">LiLa: Linking Latin</hi> project (2018-2023) is to connect and, ultimately, exploit the wealth of linguistic resources and NLP tools for Latin developed thus far, in order to bridge the gap between raw language data, NLP and knowledge description.
                </p>
                <p style=""text-align:left; "">LiLa, which has received funding from the European Research Council (ERC) European Union’s Horizon 2020 Research and Innovation programme, is building a Linked Data Knowledge Base of linguistic resources and NLP tools for Latin. The Knowledge Base consists of different kinds of objects connected through edges labelled with a restricted set of values taken from a vocabulary of knowledge description. LiLa collects and connects both existing and newly-generated (meta)data. The former are mostly linguistic resources (corpora, lexica, ontologies, dictionaries, thesauri) and NLP tools (tokenisers, lemmatisers, PoS-taggers, morphological analysers and dependency parsers) for Latin. These are currently available from different providers under different licences. As for the latter, LiLa assesses a set of selected linguistic resources by expanding their lexical and/or textual coverage. In particular, it (a) enhances the Latin texts made available by existing digital libraries and resources with PoS-tagging and lemmatisation, (b) harmonises the annotation of the three Universal Dependencies treebanks for Latin
                    <note place=""foot"" xml:id=""ftn1"" n=""1"">
                        <p rend=""footnote text"">
                            <ref target=""http://universaldependencies.org/"">http://universaldependencies.org/</ref>
                            <hi rend=""Footnote_Characters"" xml:space=""preserve""> </hi>
                        </p>
                    </note>, (c) improves the lexical coverage of the 
                    <hi rend=""italic"">Latin WordNet</hi>
                    <note place=""foot"" xml:id=""ftn2"" n=""2"">
                        <p rend=""footnote text"">
                            <ref target=""http://www.cyllenius.net/labium/index.php?option=com_content&amp;task=view&amp;id=21&amp;Itemid=49"">
                                <hi rend=""Footnote_Characters"">http://www.cyllenius.net/labium/index.php?option=com_content&amp;task=view&amp;id=21&amp;Itemid=49</hi>
                            </ref>
                        </p>
                    </note> and the valency lexicon 
                    <hi rend=""italic"">Latin-Vallex</hi>
                    <note place=""foot"" xml:id=""ftn3"" n=""3"">
                        <p rend=""footnote text"">
                            <ref target=""https://itreebank.marginalia.it/view/lvl.php"">
                                <hi rend=""Footnote_Characters"">https://itreebank.marginalia.it/view/lvl.php</hi>
                            </ref>
                            <hi rend=""Footnote_Characters"" xml:space=""preserve"">  </hi>
                        </p>
                    </note>
                    <hi rend=""italic"">,</hi> and (d) expands the textual coverage of the 
                    <hi rend=""italic"" xml:space=""preserve"">Index Thomisticus </hi>Treebank
                    <note place=""foot"" xml:id=""ftn4"" n=""4"">
                        <p rend=""footnote text"">
                            <ref target=""https://itreebank.marginalia.it/view/ittb.php"">
                                <hi rend=""Footnote_Characters"">https://itreebank.marginalia.it/view/ittb.php</hi>
                            </ref>
                        </p>
                    </note>. Furthermore, LiLa builds a set of newly-trained models for PoS-tagging and lemmatisation, and works on developing and testing the best performing NLP pipeline for such a task.
                </p>
                <figure>
                    <graphic n=""1001"" width=""12.555361111111111cm"" height=""10.96786111111111cm"" url=""Pictures/5f460efbd5e0b0058a532023cb91eebe.png"" rend=""inline""></graphic>
                    Conceptual model of LiLa.
                </figure>
                <p style=""text-align:left; "">As can be observed from the simplified conceptual model illustrated in Figure 1, the LiLa Knowledge Base is highly lexically-based. 
                    <hi rend=""bold"">Lemmas</hi> are the key node type in the Knowledge Base. Lemmas occur in 
                    <hi rend=""bold"">Lexical Resources</hi> (as lexical entries), but may have one or more (inflected) 
                    <hi rend=""bold"">Forms</hi>. For instance, the lemma 
                    <hi rend=""italic"">puella</hi>, ‘girl’ has forms like 
                    <hi rend=""italic"">puellam</hi>, 
                    <hi rend=""italic"">puellis</hi> and 
                    <hi rend=""italic"">puellas</hi>. Forms, too, can occur in lexical resources; for instance, in a lexicon containing all the word forms of a language (e.g., 
                    <hi rend=""italic"">Thesaurus Formarum Totius Latinitatis</hi>, Tombeur, 1998). Both Lemmas and Forms can have one or more graphical variants (
                    <hi rend=""italic"">condicio</hi> vs. 
                    <hi rend=""italic"">conditio</hi>). The occurrences of Forms in real texts are 
                    <hi rend=""bold"">Tokens</hi>, which are provided by 
                    <hi rend=""bold"">Textual Resources</hi>. Texts in Textual Resources can be different editions/versions of the same Work (e.g., the various editions of the 
                    <hi rend=""italic"">Orator</hi> by Cicero, possibly provided by different Textual Resources). Finally, 
                    <hi rend=""bold"">NLP tools</hi> process either 
                    <hi rend=""bold"">Forms</hi> (e.g., a morphological analyser) or Tokens (e.g., a PoS-tagger).
                </p>
                <p style=""text-align:left; "">LiLa develops across the following five Work Packages (WPs):</p>
                <list type=""unordered"">
                    <item>WP1: 
                        <hi rend=""italic"">Selecting and Improving Linguistic Resources for Latin</hi>. This WP assesses resources eligible to enter the Knowledge Base, i.e., linguistic data-sets.
                    </item>
                    <item>WP2: 
                        <hi rend=""italic"">Building the Knowledge Base</hi>. This WP represents the core of the project, as it aims to model the linguistic resources for Latin collected and selected in WP1 to build the Knowledge Base. Furthermore, this WP aims to make NLP tools for Latin interoperable and to connect them with linguistic resources in order to exploit the empirical evidence these provide for different NLP purposes.
                    </item>
                    <item>WP3: 
                        <hi rend=""italic"">Querying the Knowledge Base</hi>. This WP intends to build a user-friendly interface to allow users to write and run SPARQL queries on interconnected linguistic resources.
                    </item>
                    <item>WP4: 
                        <hi rend=""italic"">Testing and Evaluating the Knowledge Base</hi>. This WP tests the Knowledge Base by conducting research on its (meta)data.
                    </item>
                    <item>WP5: 
                        <hi rend=""italic"">Disseminating the Results</hi>. This WP is devoted to the dissemination of the results of the project through publications, conference presentations, tutorials and workshops.
                    </item>
                </list>
                <p style=""text-align:left; "">This poster contribution presents the detailed structure of LiLa, describes how it meets the so-called 
                    <hi rend=""italic"">FAIR Guiding Principles</hi> for scientific data management and stewardship, which state that scholarly data must be 
                    <hi rend=""italic"">Findable</hi>, 
                    <hi rend=""italic"">Accessible</hi>, 
                    <hi rend=""italic"">Interoperable</hi> and 
                    <hi rend=""italic"" xml:space=""preserve"">Reusable </hi>(Wilkinson, 2016), and elaborates on the progress made in WP1.
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Acknowledgements
                <p style=""text-align:left; "">The LiLa project has received funding from the European Research Council (ERC) European Union’s Horizon 2020 Research and Innovation programme under grant agreement No. 769994.</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl style=""text-align:left; "">
                        <hi rend=""bold"">Tombeur, P.</hi> (1998). 
                        <hi rend=""italic"">Thesaurus formarum totius Latinitatis: a Plauto usque ad saeculum XXum</hi>. Brepols, Turnhout, Belgium.
                    </bibl>
                    <bibl style=""text-align:left; "">
                        <hi rend=""bold"">Wilkinson, M. D. et al.</hi> (2016). The FAIR Guiding Principles for scientific data management and stewardship. 
                        <hi rend=""italic"">Scientific Data</hi>, 
                        <hi rend=""bold"">3</hi>. http://dx.doi.org/10.1038/sdata.2016.18 
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,corpus;dictionary;latin;linguistic resource;nlp tools;thesaurus,English,classical studies;english;lexicography;linguistics;metadata;natural language processing;semantic web and linked data
9837,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,An Unsupervised Lemmatization Model for Classical Languages,,James O'Brien Gawley,poster / demo / art installation,"<text>
        
            <p>The lemmatization task, wherein a computer model must group together inflected forms derived from the same stem, or 'lemma,' is a fundamental problem in language modeling. Software that handles more complex humanistic tasks like authorship attribution or intertextuality detection relies on lemmatization as an early step. </p>
            <p>
                <hi style=""font-family:Helvetica Neue"">In classical languages, the current standard depends on training sophisticated model with supervised data sets (Burns, 2018). These data sets include correct lemmatization tagged by expert readers, a labor intensive task. Modern languages can avoid generating supervised training data by taking advantage of much larger data sets. Moon and Erk (2008), for example, used an aligned corpus of English and German to infer lemmatization schemes without recourse to hand-tagged training data. Classical languages do not feature very large aligned corpora, and may not have access to a database of expert annotation for training new models.</hi>
            </p>
            <p>This paper presents a technique for inferring a lemmatization model without training data, and tests the performance of this technique in classical Latin. Performance is on par with both supervised models of Latin, and models of modern languages derived from large, aligned data sets. In ambiguous cases, where a token might derive from more than one lemma, the model identifies the correct choice in roughly 66% of trials, or roughly twice as often as random chance.</p>
            <p>
                <hi style=""font-family:Helvetica Neue"">The technique presented here delivers this performance without including any input but raw text, and can be applied to languages for which such training data is limited or non-existent. The data set used to train the model was provided by the open-source Tesserae project (http://tesserae.caset.buffalo.edu). The test data was supplied by the Classical Language Toolkit (CLTK: http://cltk.org).</hi>
            </p>
            <p>
                <hi style=""font-family:Helvetica Neue"" xml:space=""preserve"">The lemmatization model begins by determining the relative frequency of all lemmata found in a given corpus. The underlying assumption is that selecting the more common lemma is the most computationally efficient way of disambiguating between possibilities. To illustrate with an example: in the first line of Vergil’s </hi>
                <hi rend=""italic"" style=""font-family:Helvetica Neue"">Aeneid</hi>
                <hi style=""font-family:Helvetica Neue"">, the word ‘Arma’ might stem from the verb ‘armō’, a verb meaning ‘to arm’ or  'arma,' a noun meaning 'weapons' (the latter possiblity is correct). One intuitive way to resolve ambiguity is to select the lemma that appears more frequently in the corpus. But how do we determine the frequency of each lemma, without training data, when the tokens in a given corpus are ambiguous?</hi>
            </p>
            <p>
                <hi style=""font-family:Helvetica Neue"" xml:space=""preserve"">To resolve this problem, the present study assumes that all possible lemmata are present in each ambiguous case. </hi>
                <hi rend=""bold"" style=""font-family:Helvetica Neue"">Error! Reference source not found.</hi>
                <hi style=""font-family:Helvetica Neue"" xml:space=""preserve""> below illustrates this process with three tokens from Vergil's </hi>
                <hi rend=""italic"" style=""font-family:Helvetica Neue"">Aeneid</hi>
                <hi style=""font-family:Helvetica Neue"" xml:space=""preserve"">. </hi>
            </p>
            <p>
                <hi rend=""bold"" style=""font-family:Helvetica Neue"">Figure 1</hi>
                <graphic n=""1001"" width=""16.51cm"" height=""9.11225cm"" url=""Pictures/f240f7269309ac13f10ed9e5ed95352e.png"" rend=""inline""></graphic>
            </p>
            <p>As illustrated, the word form 'arma' might come from the verb 'armō' or the verb 'arma' (the latter is correct), but the form 'armis', found later in the poem, might come from the noun 'arma' or the noun 'armus' (again, the latter is correct). Different forms of the noun 'arma' overlap with different lemma, but all of them share 'arma' as a potential stem. In other words, each lemmatization is correct in the same way, but incorrect in a different way. Over several million of tokens in the classical Latin corpus, the wrong answers begin to cancel each other out and the frequency counts in the model gradually begin to reflect the true rate of appearance of each lemma. </p>
            <p>
                <hi style=""font-family:Helvetica Neue"" xml:space=""preserve"">Once the unsupervised frequency model has been trained, the lemmatizer simply selects the most frequently seen stem in ambiguous cases. Given an inflected form which might come from either of several lemmata, the lemmatizer selects the lemma that is most frequent in the corpus. The example of 'arma' is shown in </hi>
                <hi rend=""bold"" style=""font-family:Helvetica Neue"">Error! Reference source not found.</hi>
                <hi style=""font-family:Helvetica Neue"">: because the noun 'arma' was seen more often in the corpus, it is selected as the correct lemmatization here.</hi>
            </p>
            <p>Figure 2</p>
            <figure>
                <graphic n=""1002"" width=""16.467666666666666cm"" height=""5.355166666666666cm"" url=""Pictures/9e73867bceab42d23e6218a30e29ce4a.png"" rend=""inline""></graphic>
            </figure>
            <p>
                <hi style=""font-family:Helvetica Neue"">Tested repeatedly against hand-lemmatized Latin text from the CLTK training model, this unsupervised lemmatizer selected the correct stem for roughly 89% of all tokens. This performance is comparable to the more sophisticated models currently in use for Latin lemmatization. It also exceeds the performance of random selection, which identifies the correct stem in only 79% of all tokens. Roughly 73% of Latin tokens are unambiguous.</hi>
            </p>
            <p>Languages with greater ambiguity, such as classical Hebrew, may not derive performance at this level. However under-served languages such as Coptic Egyptian might use this model to build reliable lemmatizers without consuming resources in the annotation of training data.</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"" style=""font-family:Helvetica Neue"">Burns, P.</hi>
                        <hi style=""font-family:Helvetica Neue"" xml:space=""preserve""> (2018). Backoff Lemmatization for Ancient Greek with the Classical Language Toolkit.</hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue"">2018 Digital Classicist London Seminars Series</hi>
                        <hi style=""font-family:Helvetica Neue"">. London, England. July 27</hi>
                    </bibl>
                    <bibl style=""text-align:left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue"">Moon, T. and Erk, K.</hi>
                        <hi style=""font-family:Helvetica Neue"" xml:space=""preserve""> (2008). Minimally supervised lemmatization scheme induction through bilingual parallel corpora.</hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue"">First International Conference on Global Interoperability for Language Resources</hi>
                        <hi style=""font-family:Helvetica Neue"">. Hong Kong, China, Jan 9-11</hi>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,latin;lemmatization;nlp,English,classical studies;corpus and text analysis;data mining / text mining;digital textualities and hypertext;english;linguistics;natural language processing
9912,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Kraken - an Universal Text Recognizer for the Humanities,,Benjamin Kiessling,"paper, specified ""short paper""","<text>
        
            <div type=""div1"" rend=""DH-Heading"">
                Introduction
                <p>Retrodigitization of both printed and handwritten material is a common prerequisite for a diverse range of research questions in the humanities. While optical character recognition on printed texts is widely considered to be fundamentally solved in academia, with the most commonly used paradigm (Graves et al., 2006) dating back to 2006, this hasn't translated into increased availability of adaptable, libre-licensed OCR engines to the technically inclined humanities scholar.</p>
                <p>The nature of the material of interest commands a platform that can be altered with minimum effort to achieve optimal recognition accuracy; uncommon scripts, historical languages, complex or archaic page layout, and non-paper writing surfaces are rarily satisfactorily addressed by off-the-shelf commercial solutions. In addition, an open system ameliorates the severe resource constraints of humanities research by enabling sharing of artifacts, such as training data and recognition models, inaccessible with proprietary OCR technology.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading"">
                Kraken
                <p>The Kraken text recognition engine is an extensively rewritten fork of the OCRopus system. It can be used both for handwriting and printed text recognition, is easily (re-)trainable, and great care has been taken to eliminate implicit assumptions on content and layout that complicate the processing of non-Latin and non-modern works.</p>
                <p>Thus Kraken has been extended with features and interfaces enabling the processing of most scripts, among them full Unicode right-to-left, bidirectional, and vertical writing support, script detection, and multiscript recognition. Processing of scripts not included in Unicode is also possible through a simple JSON interface to the codec mapping numerical model outputs to characters. The same interface provides facilities for efficient recognition of large logographic scripts.</p>
                <p>Output includes fine-grained bounding boxes down to the character level that may be used to quickly acquire a large number of samples from a corpus to assist in paleographic research. Kraken implements a flexible output serialization scheme utilizing a simple templating language. Templates are available for the most commonly used formats ALTO, hOCR, TEI, and abbyyXML.</p>
                <p>While including implementations of all the subprocesses needed in a text recognition pipeline, most functional blocks can be accessed separately on the command line, allowing flexible substitution of specially optimized methods. A stable programming interface allows total customization and integration into other software packages.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Recognition
                <p>
                    <figure>
                        <graphic url=""Pictures/1c3fc65599505037c8cac817a5c72615.png""></graphic>
                        Network architecture (H: sequence height, W: sequence length, C: alphabet size)
                    </figure>The recognition engine operates as a segmentation-less sequence classifier using an artificial neural network to map an image of a single line of text, the input sequence, into a sequence of characters, the output sequence. The artificial neural network employed is a hybrid convolutional and recurrent neural network trained with the CTC loss function (Graves et al., 2006) that reduces training data requirements to line-level transcriptions (Figure 3). Regularization is mainly provided by dropout (Hinton et al., 2012) after both convolutional and recurrent layers. User intervention in determining training duration and model selection is largely eliminated through early stopping.
                </p>
                <p>Specialized networks, e.g. for particularly complex scripts, can be assembled from building blocks with a simple network specification language although the default architecture shown in Figure 1 is suitable for the vast majority of applications.</p>
                <p>Processing of dictionaries and library catalogues with extensive semantic markup such as italic, underlining, and bolding, is also possible through specially prepared training data.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Layout Analysis and Script Detection
                <p>
                    <figure>
                        <graphic url=""Pictures/88bf936743d1a444d5c0d85be2449a0d.jpg""></graphic>
                        Sample output of the trainable segmentation method.
                    </figure>Kraken's layout analysis extracts text lines from an input image for later processing by the recognition engine. Apart from a basic segmenter taken from OCRopus a trainable line extractor is in the process of being implemented. Full trainability of layout analysis is of utmost importance to a truly universal OCR system, as text layout and its semantics varies widely across time and space, e.g. hand-crafted methods for printed Latin text are unlikely to work reliably on Arabic text or manuscripts with extensive interlinear annotation.
                </p>
                <p>The trainable layout analysis module consists of a two-step instance segmentation method: an initial seed-labelling network operates on the whole page labelling the area between baseline and mean of each line. As the output of the network is a probability of each pixel belonging to a baseline it is binarized using hysteresis thresholding after smoothing with a gaussian filter. The binarized image is then skeletonized and end point are extracted with a discrete convolution. Finally, the vectorized baseline between the endpoints is rectified and a variable environment calculated based on the distance of connected components from the labelled area is extracted.</p>
                <p>The seed-labelling network is a modified U-net (Ronneberger et al., 2015) on the basis of a 34-layer residual network (He et al., 2016) pretrained on ImageNet. </p>
                <p>Preliminary results on a page from a publicly available dataset of Arabic and Persian manuscripts (Kiessling et al., 2019) can be seen in Figure 2.</p>
                <p>Script detection, the basis for multi-script support in the recognizer, is implemented as a segmentation-less sequence classification problem, similar to text recognition. Instead of assigning a unique label to each code point or grapheme cluster we assign all code points of a particular script the same label. The network is then trained to output the correct sequence of script labels (Figure 3). The output sequence is then used to split the line into single-script runs that can be classified with monolingual recognition models (Figure 4).</p>
                <p>
                    <figure>
                        <graphic url=""Pictures/b7a30ed8865c06cc82568cbae3750e0e.png""></graphic>
                        Original and modified ground truth (top: original line, middle: transcription, bottom: assigned script classes)
                    </figure>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading"">
                Results
                <table rend=""frame"" xml:id=""Table1"">
                    <row>
                        <cell></cell>
                        <cell rend=""start color(#000000)bold"">Mean character accuracy</cell>
                        <cell rend=""start color(#000000)bold"">Standard deviation</cell>
                        <cell rend=""start color(#000000)bold"">Maximum accuracy</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)bold"">Prints</cell>
                        <cell></cell>
                        <cell></cell>
                        <cell></cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Arabic (Kiessling et al., 2017)</cell>
                        <cell rend=""start color(#000000)"">99.5%</cell>
                        <cell rend=""start color(#000000)"">0.05</cell>
                        <cell rend=""start color(#000000)"">99.6%</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Persian
                            <note xml:id=""ftn4"" place=""foot"" n=""1"">Mid-20th century printing</note>
                        </cell>
                        <cell rend=""start color(#000000)"">98.3%</cell>
                        <cell rend=""start color(#000000)"">0.33</cell>
                        <cell rend=""start color(#000000)"">98.7%</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Syriac
                            <note xml:id=""ftn3"" place=""foot"" n=""2"">Late-19th century printing in Serṭā form</note>
                        </cell>
                        <cell rend=""start color(#000000)"">98.7%</cell>
                        <cell rend=""start color(#000000)"">0.38</cell>
                        <cell rend=""start color(#000000)"">99.2%</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Polytonic Greek
                            <note xml:id=""ftn2"" place=""foot"" n=""3"">Late-19th century printing</note>
                        </cell>
                        <cell rend=""start color(#000000)"">99.2%</cell>
                        <cell rend=""start color(#000000)"">0.26</cell>
                        <cell rend=""start color(#000000)"">99.6%</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Latin (Springmann et al., 2018)</cell>
                        <cell rend=""start color(#000000)"">98.8%</cell>
                        <cell rend=""start color(#000000)"">0.09</cell>
                        <cell rend=""start color(#000000)"">99.3%</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Latin incunabula (Springmann et al., 2018)</cell>
                        <cell rend=""start color(#000000)"">99.0%</cell>
                        <cell rend=""start color(#000000)"">0.11</cell>
                        <cell rend=""start color(#000000)"">99.2%</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Fraktur (Springmann et al., 2018)</cell>
                        <cell rend=""start color(#000000)"">99.0%</cell>
                        <cell rend=""start color(#000000)"">0.31</cell>
                        <cell rend=""start color(#000000)"">99.3%</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Cyrillic</cell>
                        <cell rend=""start color(#000000)"">99.3%</cell>
                        <cell rend=""start color(#000000)"">0.15</cell>
                        <cell rend=""start color(#000000)"">99.6%</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)bold"">Manuscripts</cell>
                        <cell></cell>
                        <cell></cell>
                        <cell></cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Hebrew
                            <note xml:id=""ftn1"" place=""foot"" n=""4"">Midrash Tanhuma, BNF Héb 150</note>
                        </cell>
                        <cell rend=""start color(#000000)"">96.9%</cell>
                        <cell rend=""start color(#000000)"">-</cell>
                        <cell rend=""start color(#000000)"">-</cell>
                    </row>
                    <row>
                        <cell rend=""start color(#000000)"">Medieval Latin
                            <note xml:id=""ftn0"" place=""foot"" n=""5"">Josephus Latinus, Bamberg 78 with augmentation</note>
                        </cell>
                        <cell rend=""start color(#000000)"">98.2%</cell>
                        <cell rend=""start color(#000000)"">-</cell>
                        <cell rend=""start color(#000000)"">-</cell>
                    </row>
                </table>
                <p>
                    <figure>
                        <graphic url=""Pictures/bd9bbcd5083d643b673c7d8fbec928a0.png""></graphic>
                        Sample output of the script detection on a bilingual French/Arabic page. Note that Eastern Arabic are always classified as Latin text
                    </figure>Kraken has been used on a wide variety of writing systems, achieving uniformly high character accuracy (CER). Sample accuracies for a diverse set of scripts spanning across multiple centuries of printing are shown in Table 1.
                </p>
                <p>As a special use case we evaluated recognition of text and emphasis in a mixed English and romanized Arabic library catalog on a training set of 350 lines (50 lines in the validation set) resulting in an averaged CER of 99.3% (σ=0.16) over 10 runs with 95.38% CER on cursive and text with increased spacing (σ=1.46). When using only emphasized text accuracy as the stopping criterium mean accuracy rises to 99.03% (σ=0.28).</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Graves, A., Fernández, S., Gomez, F. and Schmidhuber, J.</hi> (2006). Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. 
                        <hi rend=""italic"">Proceedings of the 23rd International Conference on Machine Learning</hi>. ACM, pp. 369–376.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">He, K., Zhang, X., Ren, S. and Sun, J.</hi> (2016). Deep residual learning for image recognition. 
                        <hi rend=""italic"">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</hi>. pp. 770–778.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R. R.</hi> (2012). Improving neural networks by preventing co-adaptation of feature detectors. 
                        <hi rend=""italic"">ArXiv Preprint ArXiv:1207.0580</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kiessling, B., Miller, M. T., Maxim, G., Savant, S. B. and others</hi> (2017). Important New Developments in Arabographic Optical Character Recognition (OCR). 
                        <hi rend=""italic"">Al-ʿUṣūr Al-Wusṭā</hi>, 
                        <hi rend=""bold"">25</hi>: 1–13.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kiessling, B., Stoekl Ben Ezra, Daniel and Miller, Matthew Thomas</hi> (2019). BADAM: A Public Dataset for Baseline Detection in Arabic-script Manuscripts.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ronneberger, O., Fischer, P. and Brox, T.</hi> (2015). U-net: Convolutional networks for biomedical image segmentation. 
                        <hi rend=""italic"">International Conference on Medical Image Computing and Computer-Assisted Intervention</hi>. Springer, pp. 234–241.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Springmann, U., Reul, C., Dipper, S. and Baiter, J.</hi> (2018). Ground Truth for training OCR engines on historical documents in German Fraktur and Early Modern Latin. 
                        <hi rend=""italic"">ArXiv Preprint ArXiv:1809.05501</hi>.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,digitization;htr;layout analysis;ocr,English,"artificial intelligence and machine learning;computer science and informatics;digital humanities (history, theory and methodology);english;ocr and hand-written recognition;open/libre networks and software"
9924,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,A tool for multifaceted analysis of the Old Polish New Testament apocrypha,,Karolina Bohdanowicz;Karolina Borowiec;Anna Cieplicka;Michał Kozak;Dorota Rojszczak-Robińska;Justyna Wytrążek;Olga Ziółkowska,poster / demo / art installation,"<text>
        
            <p style=""text-align:left; "">The presented tool – web search engine – is being created as part of the project funded by the National Science Centre in Poland (no. 2017/26/E/HS2/000083): 
                <hi rend=""italic"">Origins of Polish language and religious culture in the light of the medieval Apocrypha of the New Testament. A universal tool for the study of Polish apocryphal texts.</hi> It will serve for multifaceted, multidisciplinary analysis of all saved Old Polish (i.e. until the middle of the 16th century) New Testament apocrypha.</p>
            <p style=""text-align:left; "">The religious Polish language from a historical point of view constitutes the most important variety of Polish – it formed the basis for the shaping of the Polish literary language. These texts are fundamental not only for the history of Polish culture, but also for the literature and language of the East Slavdom. This is also the most extensive body of the Polish mediaeval writing – the New Testament apocrypha consist of more than 2000 pages of manuscripts. Unfortunately, those texts are largely inaccessible or poorly accessible (unpublished, published only in transliteration) or available only in excerpts. Moreover, the editions remaining in circulation are not sufficient to conduct in-depth research. This is true for not only philological (linguistic studies, literary studies, the studies of sources), but also religious or cultural studies.</p>
            <p style=""text-align:left; "">Due to their complexity and diverse character the above mentioned texts require a digital way of presentation. Consequently, one of the aims of the project is to develop a tool enabling fully interdisciplinary and multifaceted studies. This tool will be an advanced search engine with the functionality of comparing results based on a meticulously developed database, including, among others, Latin sources, Slavic contexts and the employed themes.</p>
            <p style=""text-align:left; "">New transliterations and transcriptions of these texts will be prepared for the needs of the search engine, taking into account the latest research on them and Old Polish language in general. Due to the fact that the project engages not only philologists, the engine will meet the needs of researchers specializing in other fields:</p>
            <list type=""ordered"">
                <item>Including sources (such as Latin) or themes should help to conduct traditional historical-lexical interpretations. Owing to this, it will be possible to analyse these texts in the context of the studies concerning orality and literacy, vernacularism and also the art of memory. Indication of the texts used by the Old Polish writers will also enable the reception of theological works of selected Church Fathers in medieval Poland.</item>
                <item>The compilation of themes will make it be possible to carry out an analysis of theological awareness or spirituality of particular writers and their way of understanding the described events. It will also be possible to examine the customs connected with commenting biblical texts and to deepen knowledge about biblical studies in mediaeval Poland.</item>
                <item>Indication of parallel locations will enable research on the medieval creative process (unique ways of working out topics, translation techniques, ways of organizing the text – e.g. introducing quotations), it can also be a starting point for further comparative studies of Slavonic apocryphal narratives.</item>
            </list>
            <p style=""text-align:left; "">An additional help for non-philologists users will be a grammatical comment – another functionality facilitating understanding of the Old Polish text (grammatical forms that no longer exist in a language like dualis or aorist). The already existing tools for morphological analyses of natural languages are not suitable for the analysis of the Old Polish language. The application created for the needs of the project will facilitate annotation of grammar forms.</p>
            <p style=""text-align:left; "">The web portal designed in this way will be a modern research tool, answering the needs of not only philologists, but also theologists or culture experts. It will be implemented in Java programming language with the usage of Play Framework (backend) and in JavaScript programming language with the usage of the React library (frontend). The system will import apocrypha in the form of MsWord files and automatically transform them to TEI P5 files. Next these files will be stored in the relational database PostgreSQL associated with the portal. These texts will be simultaneously indexed in many ways in the search engine Apache Solr, due to this, the portal will offer an advanced search, not only by a phrase, but also by words taking into account the lemmatization of Old Polish language developed in the project. Other above-listed relationships, like sources or themes will also be stored in this database and indexed in Apache Solr. The portal will provide user-friendly browsing and searching according to these relationships as well as user-friendly views for comparing matching parts of texts.
            </p>
        
    </text>",xml,This text is republished here with permission from the original rights holder.,,mediaeval apocrypha;old polish;tei;web search engine,English,"corpus and text analysis;english;manuscripts description and representation;medieval studies;philology;project design, organization, management;text encoding and markup languages"
9956,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Computer Assisted Curation of Digital Cultural Heritage Repositories,,Matteo Lorenzini;Marco Rospocher;Sara Tonelli,"paper, specified ""short paper""","<text>
        
            <div type=""div1"" rend=""DH-Heading1"">
                Introduction
                <p>
                    <hi style=""font-size:10pt"">The objective of metadata curatorship is to ensure that users can effectively and efficiently access objects of interest from a repository, digital library, catalogue, etc. using well-assigned metadata values aligned with an appropriately chosen schema. However, we are often facing problems related to the low quality of metadata used for the description of digital resources, for example wrong definitions, inconsistencies, or resources with incomplete descriptions. There may be many reasons for that, all completely valid, e.g, in many cases those who host a digital repository have few human resources to work on improving metadata, and often data providers are not themselves the metadata creators.</hi>
                </p>
                <p>
                    <hi style=""font-size:10pt"">In this paper we present our ongoing work aiming at defining computable metrics to assess metadata quality and automatize metadata quality check process</hi>.
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                State of the art
                <p>
                    <hi style=""font-size:10pt"" xml:space=""preserve"">The curation framework developed by Bruce and Hillmann (2004) is considered as a benchmark in the pursuit of quality assessment of digital repository. This framework defines seven dimensions to measure the quality of the metadata: </hi>
                    <hi rend=""italic"" style=""font-size:10pt"">Completeness</hi>
                    <hi style=""font-size:10pt"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic"" style=""font-size:10pt"">Accuracy</hi>
                    <hi style=""font-size:10pt"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic"" style=""font-size:10pt"">Conformance to Expectations</hi>
                    <hi style=""font-size:10pt"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic"" style=""font-size:10pt"">Logical Consistency and Coherence</hi>
                    <hi style=""font-size:10pt"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic"" style=""font-size:10pt"">Accessibility</hi>
                    <hi style=""font-size:10pt"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic"" style=""font-size:10pt"">Timeliness</hi>
                    <hi style=""font-size:10pt"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic"" style=""font-size:10pt"">Provenance</hi>
                    <hi style=""font-size:10pt"">. In the digital cultural heritage domain, these dimensions are fundamental for the evaluation of metadata quality. The evaluation helps various curators to systematically identify metadata problems, and could be straightforwardly applied to Europeana Digital Library</hi>
                    <note place=""foot"" xml:id=""ftn1"" n=""1"">
                        <p style=""text-align:left;"">
                            <hi style=""font-size:10pt"">https://www.europeana.eu/portal/en</hi>
                        </p>
                    </note>
                    <hi style=""font-size:10pt"" xml:space=""preserve""> or the Ariadne</hi>
                    <note place=""foot"" xml:id=""ftn2"" n=""2"">
                        <p style=""text-align:left;"">
                            <hi style=""font-size:10pt"">http://www.ariadne-infrastructure.eu</hi>
                        </p>
                    </note>
                    <hi style=""font-size:10pt"" xml:space=""preserve""> project as well. From the literature analysis it can be inferred that the problem of metadata curation is one of the most debated topic in the domain of humanities (Dangerfield et al 2013). Few attempts (e.g., Ochoa, X. and Duval, E. (2009)) were made to automatically compute quality metrics: however, they either focus on one dimension or are specific of some repository or metadata schema/profile.</hi>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Proposal for automatic quality check
                <p>The solution we propose is a framework which aims at automatically checking metadata quality of a repository along the dimensions proposed by Bruce and Hillmann (2004). It would also enable reporting to metadata curators a detailed analysis of the metadata situation and suggestions for possible improvements. To develop such framework, two main activities have to be carried out:</p>
                <list type=""unordered"">
                    <item>To define metadata quality metrics, capturing the status of the metadata both at object level (i.e., how good are the metadata of a single entry in the repository) and, aggregated, at repository level;</item>
                    <item>To define algorithms to compute the aforementioned quality metrics, and (possibly) return suggestions on how to fix low-quality metadata;</item>
                </list>
                <p>We already identified some strategies on how to perform these activities along some of the dimensions proposed by Bruce and Hillmann (2004), and in particular:</p>
                <list type=""unordered"">
                    <item>
                        <hi rend=""italic"" style=""font-size:10pt"">Completeness</hi>
                        <hi style=""font-size:10pt"">: computed by statistical approach; ratio of filled fields with respect to metadata profile.  The resulting value is a real number between 0 and 1: the closer this value is to 1, the more complete the metadata description of the object.</hi>
                    </item>
                    <item>
                        <hi rend=""italic"" style=""font-size:10pt"">Accuracy</hi>
                        <hi style=""font-size:10pt"">: computed using Natural Language Processing applied to description field; several linguistic features (e.g., text length, syntactic complexity, conceptual density, entity mentions) are extracted from the description text, and, using part of the gold standard data as training set, a binary classifier will be trained to distinguish between “good” and “bad” descriptions based on such features. Each description is represented as a vector of features.</hi>
                    </item>
                    <item>
                        <hi rend=""italic"" style=""font-size:10pt"">Logical consistency &amp; Coherence</hi>
                        <hi style=""font-size:10pt"">: computed using semantic web technologies; leverage ontological background knowledge on artists, objects, dates, art periods, etc., to assess the coherence of metadata values (e.g., the dating of a metadata object should be compatible with the birthdate/deathdate of its author(s)); enforce use of reference ontologies / vocabularies on controlled metadata.</hi>
                    </item>
                </list>
                <p>To validate the quality of the developed metrics and algorithms, we will rely on a “gold standard” dataset manually collected from the available repositories. Part of this dataset of good quality data (i.e., positive examples), will also be complemented with bad metadata quality objects (i.e., negative examples) to support the training of the envisioned supervised approaches.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Work done so far
                <p>
                    <hi style=""font-size:10pt"">We collected an initial “gold standard” dataset of 100 high-quality records, in terms of metadata description, from the repository of the italian digital library Cultura Italia</hi>
                    <note place=""foot"" xml:id=""ftn3"" n=""3"">
                        <p style=""text-align:left;"">
                            <hi style=""font-size:10pt"">http://www.culturaitalia.it/</hi>
                        </p>
                    </note>
                    <hi style=""font-size:10pt"">, spanning different object typologies (paintings, drawings, sculptures, etc.), to test the effectiveness of the metrics and methods to be developed.</hi>
                </p>
                <p>
                    <hi style=""font-size:10pt"" xml:space=""preserve"">We started implementing the metrics and methods for the </hi>
                    <hi rend=""italic"" style=""font-size:10pt"">completeness</hi>
                    <hi style=""font-size:10pt"" xml:space=""preserve""> dimension. The idea is to count the available metadata, dividing this number over all expected metadata. More precisely, to capture that some metadata may be more important (e.g., compulsory fields) than others when estimating the completeness of an object description, we associate a weight to each metadata: e.g., compulsory fields may have weight 2, while non-compulsory ones weight 1. Hence, the completeness ratio results by dividing the sum of the weights of the available metadata over the sum of the weights of the expected metadata. The resulting value is a real number between 0 and 1: the closer this value is to 1, the more complete the metadata description of the object.</hi>
                </p>
                <p>We computed the completeness ratio for all the ~2,500 records of the “Palazzo Pitti” dataset in  CulturaItalia. Fig. 1 shows a breakdown of the records in 5 interval categories according to their completeness ratio (e.g., 5% of the records have a completeness ration between 0.2 and 0.4), while Fig.2 plots the frequency of each specific metadata in the dataset.</p>
                <figure>
                    <graphic n=""1001"" width=""15.733888888888888cm"" height=""9.736666666666666cm"" url=""Pictures/2ae63e6bb71136ec6b243898f513a215.png"" rend=""inline""></graphic>
                </figure>
                <p style=""text-align:center;"">Fig.1 Percentage of completeness ratio</p>
                <figure>
                    <graphic n=""1002"" width=""15.945555555555556cm"" height=""9.807222222222222cm"" url=""Pictures/eaed974f81e72e9ef9d43795d180268b.png"" rend=""inline""></graphic>
                </figure>
                <p style=""text-align:center;"">Fig.2 Frequency of the metadata elements</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Conclusion
                <p>
                    <hi style=""font-size:10pt"">Considering the amount of digital archives, problems related to metadata curation becomes evident. Reasons may be different: There is no curation task force, the metadata curation activity is delegated to the content providers or the metadata curation activity is made by hand. The development of an automatic process will enable the curators to not only obtain snapshots of the quality of a repository, but also to constantly monitor its evolution and how different events affect it without the need to run costly human effort. This could lead to the creation of innovative applications based on metadata quality that would improve the final user experience.</hi>
                </p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Bruce, T.R. and Hillmann, D.</hi> (2004) The Continuum of Metadata Quality: Defining, Expressing, Exploiting.
                        <hi rend=""italic"" xml:space=""preserve""> Metadata in Practice</hi>, Hillmann and Westbrooks.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Dangerfield, M.C. and Kalshoven, L.</hi> (2013) 
                        <hi style=""font-family:Trebuchet MS"">Report and Recommendations from the Task Force on Metadata Quality</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ochoa, X. and Duval, E.</hi> (2009) Automatic evaluation of metadata quality in digital repositories. 
                        <hi rend=""italic"">International Journal on Digital Libraries</hi>, 10, pp. 67–91. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Paulheim,H.</hi> (2017) Knowledge graph refinement: A survey of approaches and evaluation methods. 
                        <hi rend=""italic"">Semantic Web 8(3)</hi> pp. 489-508. 
                    </bibl>
                    <bibl style=""text-align:left;"">
                        <lb></lb>
                        <hi rend=""bold"">Radulovic, F., Mihindukulasooriya, N., Garca-Castro, R. and Gomez-Prez, A.</hi> (2018) A comprehensive quality model for Linked Data. 
                        <hi rend=""italic"">Semantic Web</hi>, 9, pp. 3-24.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,digital libraries;metadata curation;nlp;semantic web,English,"archives, repositories, sustainability and preservation;computer science and informatics;data models and formal languages;digital humanities (history, theory and methodology);english;GLAM: galleries, libraries, archives, museums;ontologies and knowledge representation"
10016,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Stylometry for Noisy Medieval Data: Evaluating Paul Meyer's Hagiographic Hypothesis,,Ariane Pinche;Jean-Baptiste Camps;Thibault Clérice,"paper, specified ""long paper""","<text>
        
            <p>The history of the composition of French saint’s Lives collections in prose is still a mystery. At the beginning of the XIII
                <hi rend=""sup"">th</hi> century, legendaries were already constituted and intermediary steps are missing to understand how the Lives have been gathered. One of the existing hypotheses is that small collections of saint’s Lives might have circulated as small independent units about one saint or a series of saints (Philippart, 1977), sometimes traceable to a single author or translator, under the material form of libelli.
            </p>
            <p>The first milestone in the study of the composition of legendaries was laid by Paul Meyer (1906). His studies led him to discover that some of these legendaries were derived from successive compilations. Using their macrostructure, Meyer dispatched them into families. He also had the intuition that the families were constituted of smaller components, groups or rather sequences of texts. He proposed the existence of primitive series based on authorship and the repetitive grouping of selected lives, such as the hagiographic collection of Wauchier de Denain’s Seint Confessor of the family C or the consecutive and recurrent series in the family B and C of the following three lives: Sixte, Laurent and Hippolyte. However, because most of the French saint’s lives are anonymous and because the collections were rearranged by multiple editors over time, it is extremely difficult to find what could have been the primitive series and Meyer couldn’t go further. This serial composition of the Lives of Saints is a datum also noted by other specialists of Latin hagiography such as Perrot (1992) and Philippart (1977), who even points out that these hagiographic series must be studied in their entirety in the same way as a literary work. But it is still very rare today that hagiographic text editing concerns a complete author’s legendary, mostly because of a lacking certainty about these groupings.</p>
            <p> From there, with an exploratory stylometric analysis, we first want to find if Meyer’s hypotheses are wrong, can be nuanced or completed. In a second time, we would like to discover if the observed proximity between saint’s lives can reveal series from the same author.</p>
            <p>
                <figure>
                    <graphic url=""Pictures/068b7632857a027448521cb15388a1b0.png""></graphic>
                </figure>
            </p>
            <p>We based our study on a legendary from family C, namely the manuscript fr. 412 of the Bibliothèque nationale de France. The task is complex, because textual variants and the absence of a standardised spelling can affect the stylometric analysis (Kestemont et al., 2015). While the lemmatization of the texts could have nullified the problem of spelling variation, in our case, the preparation of the corpus would have been extremely time-consuming. We decided to work from witnesses written by a single hand in the same manuscript in order to limit biases that could have been induced by spelling variations linked to the scribes and not to the author.</p>
            <p>
                <anchor xml:id=""id_result_box3""></anchor> The analysis of the complete legendary is made possible by the use of the OCR software Kraken (Kiessling, 2018), trained on about 8890 transcribed lines of ground truth and tested on 897 lines, which results in up to 95.2% success in Character Recognition Score. Such results allow us to hope that the stylometric analysis is not corrupted by the margin of error (Franzini et al., 2018).
            </p>
            <p>
                <figure>
                    <graphic url=""Pictures/5e39a5bb5f6b39fc107ccf18afbc2a9d.png""></graphic>
                </figure>
            </p>
            <p>
                <figure>
                    <graphic url=""Pictures/20780a0bd49e11c46ce90e9da89d01da.png""></graphic>
                </figure>
                <anchor xml:id=""id_result_box4""></anchor>We work from the text thus obtained. Because most of the texts of the legendary are anonymous, we follow an unsupervised approach to the analysis of the texts (Camps &amp; Cafiero, 2012), using agglomerative hierarchical clustering with Ward’s criterion (Ward, 1963), guided by its ability to form coherent clusters. 
            </p>
            <p> The texts are, on average, quite short, a known difficulty for stylometry (Eder, 2015), with a median value of
                <anchor xml:id=""id_rstudio_console_output""></anchor>16,863 characters (space excluded), but with extreme values of 
                <anchor xml:id=""id_rstudio_console_output2""></anchor>1,364 and 
                <anchor xml:id=""id_rstudio_console_output3""></anchor>85,378. Texts that are too short create a problem of reliability, as the observed frequencies may not accurately represent the actual probability of a given variable’s appearance (Moisl, 2011). To limit this issue, we removed texts below 5,000 characters. Because of the errors regarding segmentation in the OCR, we extracted character n-grams, ignoring spaces. We experimented with different lengths, but, following existing benchmarks (Stamatatos, 2013), we retained the 4000 most frequent 3-grams. The metric and choices of normalisation are also an important parameter, one to which much attention has been devoted (Evert et al., 2017; Jannidis et al., 2015).
            </p>
            <p> Following the benchmark by Evert et al. 2017, we chose to use Manhattan distance with z‑transformation (Burrows’ Delta) and vector-length Euclidean normalisation. The results are partially presented in figures 1 &amp; 2.</p>
            <p>
                <figure>
                    <graphic url=""Pictures/a84e8a05a9da3377aa939ce71354893f.png""></graphic>
                    
                        <lb></lb>Dendogram of agglomerative hierarchical clustering using Manhattan distance, z-transformation and vector length normalisation over 4000 most frequent 3-grams
                    
                </figure>
            </p>
            <p>
                <figure>
                    <graphic url=""Pictures/04e7040ff00d11cd2c740a28bf70666f.png""></graphic>
                    
                        <lb></lb>Dendogram of agglomerative hierarchical clustering using Kohonen SOM coordinates over 4000 most frequent 3-grams
                    
                </figure>
            </p>
            <p>Because, at the same time, the corpus is homogeneous, the texts can be quite short, and the data is noisy, separating them in stable clusters can prove quite hard. We tried to improve the quality of the signal by applying, first, a Kohonen self-organising map (Kohonen, 1988:59–69), and then using the coordinates of the points in the SOM for hierarchical clustering (Camps and Cafiero, 2012). </p>
            <p> In addition, the specificity of composition of the legendary C by successive additions (lives of A, then lives of B, and finally addition of new lives) allows us to ensure a quick control of the likelihood of some proposed groupings. The presence of the hagiographic collection of 
                <hi rend=""italic"">Seint Confessor</hi>s of Wauchier de Denain where the author identifies itself twice (both in 
                <hi rend=""italic"">Dialogues</hi> de Sulpice Sévère and in 
                <hi rend=""italic"">Vie de saint Martial</hi>
                <hi rend=""italic"">de Limoges</hi>) also serves as an indicator of validity. 
            </p>
            <p> The study has already shown interesting connections between the legendary of Wauchier de Denain and the 
                <hi rend=""italic"">Vie de Saint Lambert de Liège</hi> and some collections have been revealed. Two of them are quite certain, one of the first five texts of C, all about saint apostles and hypothetically from A, and another one of six virgin saints’ lives, all from B including the Lives of saint Agathe, Lucie, Agnès, Felicité, Christine and Cécile.
            </p>
            <p> To conclude, our analysis attempts to evaluate the best parameters for our study and to overcome certain difficulties inherent to our corpus. Two major obstacles have to be overcome: the lack of spelling standardisation and the lack of homogeneity in the separation of words. At the end of this prospective study, we hope to be able to reveal new hagiographic series prior to the composition of legendaries that were transmitted to us and that could have escaped us so far.</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Camps, J.-B. and Cafiero, F.</hi> (2012). Setting bounds in a homogeneous corpus: a methodological study applied to medieval literature. 
                        <hi rend=""italic"">Revue Des Nouvelles Technologies de l’Information</hi>, 
                        <hi rend=""bold"">SHS</hi>-
                        <hi rend=""bold"">1 (</hi>MASHS 2011/2012. Modèles et Apprentissages en Sciences Humaines et Sociales Rédacteurs invités : Mar): 55–84.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Evert, S., Proisl, T., Jannidis, F., Reger, I., Pielström, S., Schöch, C. and Vitt, T.</hi> (2017). Understanding and explaining Delta measures for authorship attribution. 
                        <hi rend=""italic"">Digital Scholarship in the Humanities</hi>, 
                        <hi rend=""bold"">32 (</hi>suppl_2): ii4 – 16 doi:10.1093/llc/fqx023.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Franzini, G., Kestemont, M., Rotari, G., Jander, M., Ochab, J. K., Franzini, E., Byszuk, J. and Rybicki, J.</hi> (2018). Attributing Authorship in the Noisy Digitized Correspondence of Jacob and Wilhelm Grimm. 
                        <hi rend=""italic"">Frontiers in Digital Humanities</hi>, 
                        <hi rend=""bold"">5</hi> doi:10.3389/fdigh.2018.00004. https://www.frontiersin.org/articles/10.3389/fdigh.2018.00004/full (accessed 7 August 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jannidis, F., Pielström, S., Schöch, C. and Vitt, T.</hi> (2015). Improving Burrows’ Delta – An empirical evaluation of text distance measures. 
                        <hi rend=""italic"">Digital Humanities Conference</hi>: 11.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kestemont, M.</hi> (2014). Function Words in Authorship Attribution. From Black Magic to Theory?. 
                        <hi rend=""italic"">Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLFL)</hi>. Gothenburg, Sweden: Association for Computational Linguistics, pp. 59–66 doi:10.3115/v1/W14-0908. http://aclweb.org/anthology/W14-0908 (accessed 23 November 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kestemont, M., Moens, S. and Deploige, J.</hi> (2015). Collaborative authorship in the twelfth century: a stylometric study of Hildegard of Bingen and Guibert of Gembloux. 
                        <hi rend=""italic"">Digtial Scholarship in the Humanities</hi>, 
                        <hi rend=""bold"">30 (</hi>2): 199–224 doi:http://dx.doi.org/10.1093/llc/fqt063.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kiessling, B.</hi> (2018). 
                        <hi rend=""italic"">OCR Engine for All the Languages. Contribute to Mittagessen/Kraken Development by Creating an Account on GitHub</hi>. Python https://github.com/mittagessen/kraken (accessed 26 November 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kohonen, T.</hi> (1988). Neurocomputing: Foundations of Research. In Anderson, J. A. and Rosenfeld, E. (eds). Cambridge, MA, USA: MIT Press, pp. 509–521 http://dl.acm.org/citation.cfm?id=65669.104428 (accessed 27 November 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Meyer, P.</hi> (1906). Légendes hagiographiques en français. 
                        <hi rend=""italic"">Histoire littéraire de la France</hi>, vol. 33. Imprimerie nationale. Paris, pp. 328–458 http://archive.org/details/histoirelittra33riveuoft (accessed 1 May 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moisl, H.</hi> (2011). Finding the Minimum Document Length for Reliable Clustering of Multi-Document Natural Language Corpora. 
                        <hi rend=""italic"">Journal of Quantitative Linguistics</hi>, 
                        <hi rend=""bold"">18 (</hi>1): 23–52 doi:10.1080/09296174.2011.533588.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Perrot, J.-P.</hi> (1992). 
                        <hi rend=""italic"">Le passionnaire français au Moyen âge</hi>. Genève, Suisse: Droz.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Philippart, G.</hi> (1977). 
                        <hi rend=""italic"">Les Légendiers latins et autres manuscrits hagiographiques</hi>. Turnhout (Belgique), Belgique: Brépols.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Stamatatos, E.</hi> (2009). A survey of modern authorship attribution methods. 
                        <hi rend=""italic"">Journal of the American Society for Information Science and Technology</hi>, 
                        <hi rend=""bold"">60 (</hi>3): 538–56 doi:10.1002/asi.21001.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Stamatatos, E.</hi> (2013) On the robustness of authorship attribution based on character n-gram features. 
                        <hi rend=""italic"">Journal of Law and Policy</hi>, 
                        <hi rend=""bold"">21 (</hi>2): 421–439.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ward, J. H.</hi> (1963). Hierarchical Grouping to Optimize an Objective Function. 
                        <hi rend=""italic"">Journal of the American Statistical Association</hi>, 
                        <hi rend=""bold"">58 (</hi>301): 236–44 doi:10.1080/01621459.1963.10500845.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,anonymous works;legendaries;meyer;ocr;stylometry,English,authorship attribution / authority;english;medieval studies;ocr and hand-written recognition;philology;stylistics and stylometry
10040,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Entangled Histories of Early Modern Ordinances. Segmentation of Text and Machine-Learned Metadating.,,Christel Annemieke Romein;Sara Floor Veldhoen,poster / demo / art installation,"<text>
        
            <p style=""text-align:left; "">Libraries and archives throughout Europe host books with ordinances, or individual ordinances (‘laws’) from the 15th to the 18th century (Härter, 1996). These texts contain indications of how governments of burgeoning states dealt with unexpected threats to safety, security, and order through home-invented measures, borrowed rules, or adjustments of what was established elsewhere. The hundreds of texts within these books are frequently consulted by researchers of various disciplines (e.g. history, law, political sciences, linguistics) to unravel rules for controlling complex societies. Having the possibility of a longitudinal search, based upon contents rather than the index or title, as well as having an overview based upon several states has so far been impossible due to the impenetrable amount of scanned texts. This project will disclose the entangled histories of neighbouring states, due to synchronic and diachronic comparisons – allowing a wider search and implementation in other projects Europe-wide. </p>
            <p style=""text-align:left; "">This project-in-process will focus on the Dutch Early Modern Ordinances, but the techniques will have a widespread positive effect. This project consists of three steps: </p>
            <p style=""text-align:left; "">1. 
                <hi rend=""italic"">Improving the OCR-techniques of digitised sources</hi>. We plan to use the Handwritten Text Recognition suite Transkribus to reprocess the files which currently have poor quality OCR. By treating the printed works – which were printed with hand-carved letters – as very consistent handwriting we hope to obtain a higher quality of recognition (CER &lt;5%).
            </p>
            <p style=""text-align:left; "">2. Segmentation of the texts, 
                <hi rend=""italic"">going from sentence-recognition</hi> (Lonij, Harbers 2016) 
                <hi rend=""italic"">to article-segmentation</hi> of larger texts; this requires that the computer is trained to recognise the beginning and end of texts, either as a chapter or as an individual text within a compilation of texts. Initially, this can be based upon image/layout but could evolve into integration with HTR, depending on its feasibility.
            </p>
            <p style=""text-align:left; "">3. We want to 
                <hi rend=""italic"" xml:space=""preserve"">create machine-generated metadata </hi>by training a computer to recognise the conditions that categories were based upon. This will be based on an already developed genre classifier (Lonij, Harbers, 2016) that can be fitted to automated content analysis. After 
                <hi rend=""italic"">supervised training</hi>, the computer can then suggest, apply and supplement categories to other texts based on the idea of topic modelling (Leydesdorff, 2017). This is a pilot that will prove the applicability of the tool to other languages as well. 
            </p>
            <p style=""text-align:left; "">Due to its significance for such a broadly studied range of sources, we hope to make the output available as RDF. We will use NLP such as NER technologies to identify dates, titles, persons; and implement the output in the Dutch national infrastructure. With the data generated in this project, visualisations of the development of laws across the Netherlands – and possibly Europe - would become possible.</p>
            <p style=""text-align:left; "">Outcomes of this project:</p>
            <list type=""unordered"">
                <item>Improved performance of the current OCR-recognition of (early modern) texts by incorporating HTR-techniques on printed texts. (A use-case of Transkribus.)</item>
                <item>The possibility to automatically recognise segments in text layout: beginning or end, columns, titles, dates, summaries and the body of the text. This data will be used for the RDF-compliant tool.</item>
                <item>Expansion of automatic content analysis based upon segments, rather than on lines or sentences, with a machine-learned algorithm, and applying this with standardised machine-learned metadata to early modern normative texts. This will allow researchers to search through approximately 15.000 ordinances and resolutions from various provinces in the Low Countries, accessible through uniform search terms. </item>
                <item>An RDF-compliant tool to enable application in other ongoing and future projects.</item>
                <item>Integration of the enhanced datasets in the CLARIAH/CLARIN ecosystem.</item>
            </list>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl style=""text-align:left; "">
                        <hi rend=""bold"">Härter, K.; Stolleis, M.; Schilling, L. (eds.)</hi> (1996), 
                        <hi rend=""italic"">Policey im Europa der Frühen Neuzeit</hi>. Frankfurt a/Main. 
                    </bibl>
                    <bibl style=""text-align:left; "">
                        <hi rend=""bold"">Härter, K. and M. Stolleis</hi> (1996), 
                        <hi rend=""italic"">Introduction to Repertorium der Policeyordnungen der frühen Neuzeit, Band 1. Deutsches Reich und geistliche Kurfürstentümer (Kurmainz, Kurköln, Kurtrier),</hi> edited by K. Harter. Frankfurt a/Main: pp. 1-36.
                    </bibl>
                    <bibl style=""text-align:left; "">
                        <hi rend=""bold"">Leydesdorff, L., and A. Nerghes.</hi> (2017) ""Co
                        <hi style=""font-family:Cambria Math"">‐</hi>word maps and topic modeling: A comparison using small and medium
                        <hi style=""font-family:Cambria Math"">‐</hi>sized corpora (N&lt; 1,000)."" 
                        <hi rend=""italic"">Journal of the Association for Information Science and Technology</hi>
                        <hi rend=""bold"">68(4)</hi>: 1024-35. 
                    </bibl>
                    <bibl style=""text-align:left; "">
                        <hi rend=""bold"">Lonij, J., Harbers, F.</hi> (2016), 
                        <hi rend=""italic"">Genre classifier</hi>. KB Lab: The Hague 
                        <ref target=""http://lab.kb.nl/tool/genre-classifier"">http://lab.kb.nl/tool/genre-classifier</ref>
                        <hi rend=""Hyperlink"">.</hi>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,automatic content analysis;machine-generated metadata;ocr;rdf;text-segmentation,English,corpus and text analysis;english;history and historiography;law;metadata;ocr and hand-written recognition
10084,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Improving OCR of Black Letter in Historical Newspapers: The Unreasonable Effectiveness of HTR Models on Low-Resolution Images,,Phillip Benjamin Ströbel;Simon Clematide,"paper, specified ""short paper""","<text>
        
            <div type=""div1"" rend=""DH-Heading1"">
                Introduction
                <p>The quality of Optical Character Recognition (OCR) is a decisive factor for the application of text mining techniques on historical newspapers (Chiron et al., 2017; Walker et al., 2010; Strange et al., 2014). OCR for texts published in black letter is particularly challenging due to several factors: the low distinctiveness of characters, the change over time regarding vocabulary and spelling, the use of small font sizes, and the oftentimes poor paper quality.</p>
                <p>Holley (2009) argued that in light of the poor OCR quality in newspapers, a focus on manual crowd-correction is more promising than investments in software development. Although automatic OCR post-correction can improve the quality of the text, the methods often lack precision, are not robust enough, or require a lot of in-domain training data (Alex et al., 2012; Chiron et al., 2017).</p>
                <p>The problems are manifold and complex, but recent progress in neural OCR techniques promises significant improvements (Springmann and Lüdeling, 2016). These OCR models often outperform commercial systems like 
                    <hi rend=""bold"">ABBYY FineReader</hi>
                    <hi rend=""bold"">
                        <note xml:id=""ftn0"" place=""foot"" n=""1"">
                            <ptr target=""https://www.abbyy.com""></ptr>
                        </note>
                    </hi>. However, the training of a neural system using open-source software (e.g., 
                    <hi rend=""bold"">Tesseract</hi>
                    <hi rend=""bold"">
                        <note xml:id=""ftn1"" place=""foot"" n=""2"">
                            <ptr target=""https://github.com/tesseract-ocr/tesseract""></ptr>
                        </note>
                    </hi>) is demanding. Integrated handwritten text recognition and annotation platforms like 
                    <hi rend=""bold"">Transkribus</hi>
                    <hi rend=""bold"">
                        <note xml:id=""ftn2"" place=""foot"" n=""3"">
                            <ptr target=""https://www.transkribus.eu""></ptr>
                        </note>
                    </hi> facilitate the creation of a ground truth, as well as the training and application of neural and corpus-specific models for OCR.
                </p>
                <p>Transkribus was initially designed to decipher manuscripts
                    <note xml:id=""ftn3"" place=""foot"" n=""4"">see 
                        <ptr target=""https://read.transkribus.eu/""></ptr>
                    </note>. It allows the manual transcription of uploaded documents so that they can be used as training material for 
                    <hi rend=""bold"">Handwritten Text Recognition</hi> (HTR) models (Weidemann et al., 2017). A useful feature of Transkribus’ HTR models is that the recognition of printed texts works just as well as that of manuscripts. A few dozen of corrected pages are sufficient for high-quality OCR results.
                </p>
                <p>In this study we illustrate how to drastically improve OCR quality for black letter in newspapers with a modest amount of manual work for ground truth creation. The integration of HTR model training into the Transkribus platform enables Digital Humanists to leverage the performance of neural OCR without having to tackle unnecessary technicalities. In our experiments we additionally address the following questions. Robustness: Are HTR models reusable for material that varies in digitisation quality (medium-resolution scans from microfilm vs. high-resolution scans from paper). Transferability: How well does a model perform on another newspaper than the one it was trained on?</p>
            </div>
            <div type=""div1"">
                
                    <anchor xml:id=""id_data-and-experiments""></anchor>Data and Experiments
                
                <p>We use PDFs with medium-resolution images produced in 2005 from scanned microfilms of the German-language 
                    <hi rend=""bold"">Neue Zürcher Zeitung</hi> (NZZ) for our experiments. The OCRed text stems from 
                    <hi rend=""bold"">ABBYY FineReader XIX</hi>
                    <hi rend=""bold"">
                        <note xml:id=""ftn4"" place=""foot"" n=""5"">
                            <ptr target=""https://www.frakturschrift.com/de:products:finereaderxix""></ptr>
                        </note>
                    </hi>, which was ABBYY’s product for 19th century black letter recognition at that time.
                </p>
                <p>The first experiment evaluates the differences between three OCR systems: (a) FineReader XIX (FRXIX) results from 2005, (b) ABBYY FineReader Server 11 (FRS11) results
                    <note xml:id=""ftn5"" place=""foot"" n=""6"">see 
                        <ptr target=""https://www.abbyy.com/de-de/finereader-server/""></ptr>, available within Transkribus
                    </note>, (c) Transkribus’ HTR model. Figure 1 shows example output from our three OCR systems.
                </p>
                <p>
                    <figure>
                        <graphic url=""Pictures/43d1bacd66807525a4cf35690f3fa329.png""></graphic>
                        Example excerpts with low-quality OCR from two pages of the NZZ (1819 left, 1859 right, red: FRXIX, blue: FRS11, green: Transkribus HTR)
                    </figure>
                </p>
                <p>In our second experiment we apply the HTR model trained on medium-resolution images to high-resolution images (400dpi) from 1899 digitised anew from paper in order to test the transferability of the model. We also analyse the performance of the HTR model in two other publications.</p>
                <div type=""div2"">
                    
                        <anchor xml:id=""id_creation-of-a-ground-truth-and-htr-model-training""></anchor>Creation of a ground truth and HTR model training
                    
                    <p>The NZZ had been published in black letter from 1780 until 1947. We chose one title page per year at random from this period and loaded the image extracted from the PDF into Transkribus. We used the Transkribus internal FRS11 to recognise the text in the images and manually corrected words and baselines. The resulting ground truth of 167 pages contains 304,286 words and 43,151 lines. Depending on the amount of text on a page, the correction of a page including the baselines (needed to train the HTR model) takes between 1 and 2.5 hours. We used 90/10 split for training and testing the model.</p>
                </div>
                <div type=""div2"">
                    
                        <anchor xml:id=""id_evaluation""></anchor>Evaluation
                    
                    <p>We use the bag-of-words F1-measure metrics of PRImA TextEval 1.4
                        <note xml:id=""ftn6"" place=""foot"" n=""7"">
                            <ptr target=""https://www.primaresearch.org/tools/PerformanceEvaluation""></ptr>
                        </note> for evaluation. The F1-measure is the harmonic mean of precision and recall. Precision gives the percentage of OCRed words that are part of the ground truth, while recall measures the percentage of ground truth words that were found by the OCR system. By applying a bag-of-words approach, possible differences in layout recognition cannot distort the results.
                    </p>
                </div>
            </div>
            <div type=""div1"">
                
                    <anchor xml:id=""id_results-and-discussion""></anchor>Results
                
                <p>Figure 2 shows the evaluation on all pages from the test set. The FRS11 (mean F1-measure 81.1%, SD 7.3%) beats the FRXIX (mean 67.8%, SD 11.1%) throughout. Our HTR model scores 97.0% (SD, 1.8%) on average and achieves significant improvements over both ABBYY products.</p>
                <p>
                    <figure>
                        <graphic url=""Pictures/16f498cc6da6a09f06204600b60f7129.png""></graphic>
                        Comparison between original FRXIX, FRS11, and Transkribus HTR.
                    </figure>
                </p>
                <p>The application of our HTR model to five high-resolution images of newspaper pages from the NZZ shows accuracies of at least 98% and an average improvement of 4.24% over FRS11 (see Figure 3).</p>
                <p>
                    <figure>
                        <graphic url=""Pictures/22e84f6523c4aa62b6c3f5166d3ef074.png""></graphic>
                        Comparison between FRS11 and Transkribus HTR model on five high-resolution images from 1899.
                    </figure>
                </p>
                <p>In terms of the transferability of our HTR model the average F1-measures of 98.6% (SD 1.9%) for the 
                    <hi rend=""bold"">Bundesblatt</hi>
                    <hi rend=""italic""> </hi>and 98.9% (SD 0.6%) for the 
                    <hi rend=""bold"">Neue Zuger Zeitung</hi> over five pages each show that although the model has been trained on the NZZ, it is able to score equally high on different publications. The FRS11 reaches 92.4% (SD 2.7%) for the Bundesblatt and 88.4% (SD 3.7%) for the Neue Zuger Zeitung, showing the superiority of our HTR model.
                </p>
            </div>
            <div type=""div1"">
                
                    <anchor xml:id=""id_conclusion""></anchor>Conclusion
                
                <p>We have shown that Transkribus is an excellent tool for creating HTR models for the OCR of newspapers typeset in black letter. Even with a limited amount of training data (150 pages), our HTR model consistently outperforms state-of-the-art commercial software. Our HTR model trained on medium-resolution images digitised from microfilm still performs better than commercial software when applied to high-resolution images derived from paper originals.</p>
                <p>Given the availability and abundance of digitised historical material in the form of PDF files with poorly OCRed text, our findings showcase how digital humanists can improve their source material for text mining with a reasonable effort.</p>
            </div>
            <div type=""div1"">
                Acknowledgments
                <p>We would like to express our gratitude to Günter Mühlberger and the Transkribus team for their support in training HTR models and partially correcting baselines of our ground truth. Moreover, we thank Camille Watter and Isabel Meraner for their help in the transcription process. This research is supported by the Swiss National Science Foundation under grant CR-SII5_173719.</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Alex, B., Grover, C., Klein, E., Tobin, R.</hi> (2012). 
                        <hi rend=""italic"">Digitised Historical Text: Does it have to be mediOCRe?</hi>, in: KONVENS. pp. 401–409.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Chiron, G., Doucet, A., Coustaty, M., Visani, M., Moreux, J.-P.</hi> (2017). 
                        <hi rend=""italic"">Impact of OCR Errors on the Use of Digital Libraries: Towards a Better Access to Information</hi>.
                        <hi rend=""italic""> 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)</hi>. IEEE. 
                        <ptr target=""https://doi.org/10.1109/jcdl.2017.7991582""></ptr>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Holley, R.</hi> (2009). 
                        <hi rend=""italic"">How Good Can It Get? Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs</hi>. D-Lib Magazine 15. 
                        <ptr target=""https://doi.org/10.1045/march2009-holley""></ptr>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Springmann, U., Lüdeling, A.</hi> (2016). 
                        <hi rend=""italic"">OCR of Historical Printings with an Application to Building Diachronic Corpora: A Case Study Using the RIDGES Herbal Corpus</hi>. CoRR abs/1608.02153.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Strange, C., McNamara, D., Wodak, J., Wood, I.</hi> (2014). Mining for the Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical Newspapers. 
                        <hi rend=""italic"">DHQ: Digital Humanities Quarterly 8</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Walker, D. D., Lund, W. B., Ringger, E. K.</hi> (2010). 
                        <hi rend=""italic"">Evaluating Models of Latent Document Semantics in the Presence of OCR Errors</hi>.
                        <hi rend=""italic""> Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</hi>. Association for Computational Linguistics, pp. 240–250.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Weidemann, M., Michael, J., Grüning, T., Labahn, R.</hi> (2017). 
                        <hi rend=""italic"">HTR Engine Based on NNs P2 Building Deep Architectures with TensorFlow</hi>. 
                        <ptr target=""https://read.transkribus.eu/wp-content/uploads/2017/12/Del_D7_8.pdf""></ptr>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,historical newspapers;htr;ocr;text mining;transkribus,English,"artificial intelligence and machine learning;data mining / text mining;digital humanities (history, theory and methodology);english;library & information science;ocr and hand-written recognition;software studies"
10353,2021 - Leiden 2021,Leiden 2021,The Humanities in a Digital World,2021,DHBenelux,DH Benelux,Leiden University,Leiden,South Holland,Netherlands,https://2021.dhbenelux.org/schedule/,Is your OCR good enough? Probably so. An assessment of the impact of OCR quality on downstream tasks for Dutch texts,,Konstantin Todorov;Mirjam Cuper;Giovanni Colavizza,paper,,txt,,10352.0,dutch;machine learning;natural language processing;ocr;text analysis,English,
10453,2016 - University of Leipzig,University of Leipzig,Modellierung - Vernetzung – Visualisierung: Die Digital Humanities als fächerübergreifendes Forschungsparadigma,2016,DHd,DHd,Universität Leipzig (Leipzig University),Leipzig,,Germany,http://dhd2016.de/,Annotation natürlichsprachlicher Texte aus Onlineforen zur Entwicklung domainspezifischer Ontologien,,Canan Hastik,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p rend=""paper_abstract"">Annotation natürlicher Sprachdaten aus sozialen Medien zur
        Erforschung zeitgenössischer Szenen, zur Sprach- und Trendanalyse und zur
        Weiterentwicklung von Sprachtechnologien gewinnt mit der zunehmenden Verfügbarkeit
        großer Datenbestände weiter an Bedeutung (Farzindar / Inkpen 2015). Zeitgenössische
        Kommunikation in sozialen Medien verfügt über inhaltliche und strukturelle
        Besonderheiten und ist von umgangssprachlicher Ausdrucksform geprägt. Beiträge, die
        im Kontext internetbasierter Diskussionskulturen in Foren entstehen, stellen eine
        wichtige Forschungsquelle dar. Diese nutzergenerierten Texte, in Form von semi- oder
        unstrukturierten Kommentaren, repräsentieren Meinungen und Bewertungen einer
        Gemeinschaft zu einem Thema, Produkt oder Werk und beziehen sich in der Regel auf
        inhaltliche, technische oder ästhetische Aspekte. Die Autoren verwenden dabei
        Sprachmittel wie Metaphern, Analogien, Ambiguität, Humor und Ironie sowie
        metalinguistische bildhafte Mittel wie Emoticons oder andere graphische Zeichen
        (Reyes et al. 2012).</p>
         <p rend=""paper_abstract"">Vor diesem Hintergrund adressiert dieses Projekt Herausforderungen, die bei der linguistischen und statistischen Verarbeitung von realen web-basierten Daten entstehen. Es wird ein Ansatz semi-automatischer Annotation zur Extraktion von Begriffen für die ontologiebasierte Beschreibung von computergenerierten audiovisuellen Kunstwerken einer digitalen Kunstszene präsentiert. Forschungsgegenstand ist die Diskussionskultur der Demoszene, einer spezialisierten Computerkunstszene. Bisher sind die zahlreichen Beiträge der Gemeinschaft, die sich auf ästhetische und technische Aspekte der Kunstwerke beziehen, nicht erschlossen. Bei diesen Beiträgen handelt es sich um informelle, emotionale, kurze und unstrukturierte Kommentartexte. Das verwendete Vokabular ist mehrsprachig und beinhaltet fachspezifische Terminologien, exklusive Neologismen und einen eigenen szenespezifischen orthographischen Stil. Diese Beiträge bieten detaillierte Einblicke in die Charakteristika der Werke, weshalb ihre Erschließung deren Verständnis fördert und eine gezielte Recherche einzelner Werke ermöglicht. Das Projekt befasst sich mit der Fragestellung, in wieweit sich aktuelle Verfahren der natürlichen Sprachverarbeitung (NLP), die auf grammatikalisch korrekte Schriftformen optimiert und auf Zeitungskorpora trainiert sind, anwenden lassen. Somit leistet das präsentierte Projekt einen Beitrag im Bereich der Entwicklung von Ansätzen zur Aufbereitung großer textbasierter Datenbestände sowie der Erforschung des Sprachgebrauchs zeitgenössischer digitaler Kunstszenen, aber auch hinsichtlich Nutzung semantischer Technologien.</p>
         <p rend=""paper_abstract"">Die Anwendung von NLP-Verfahren für textbasierte Kommunikation
          in soziale Medien bedarf einiger Anpassungen an die sprachlichen Besonderheiten
          (Maynard 2012). Die Nutzung standardisierter Techniken ist bisher nur wenig
          erfolgversprechend (Gimpel 2011; Finin 2010). Bestehende Frameworks, wie das Natural
          Language Toolkit (NLTK, vgl. Bird et al. 2015), bieten die Möglichkeit der
          Implementierung eines individuellen NLP-Prozesses, bei dem verschiedene
          Verarbeitungsschritte modular integriert und miteinander kombiniert werden können.
          Für das vorliegende Projekt wurde eine Pipeline konzipiert und implementiert, die
          die Generierung von Annotationsebenen, begonnen mit der Tokenisierung und
          Part-of-Speech Tagging bis hin zur Extraktion von relevanten werkbeschreibenden
          Begriffen umfasst. Zur Evaluation des entwickelten Ansatzes wird ein regelbasiertes
          überwachtes Experiment mit einer definierten Teilmenge von 1255 Kommentaren
          durchgeführt. Es lässt sich feststellen, dass Emoticons und Partikeln falsch
          verarbeitet werden. Darüber hinaus werden auch Nomen, Verben und Adjektive,
          insbesondere Gerundien häufig falsch annotiert. Das Experiment zeigt, dass die
          konzipierte Pipeline für das vorliegende Kommentarkorpus iterativ optimiert werden
          muss. Der generierte Index werkbeschreibender Terminologie wird ferner für die
          Erweiterung einer domainspezifischen Ontologie zur Unterstützung semantischer
          Annotation verwendet. Hierfür wird ein Ansatz für das Lernen von Ontologien aus
          Texten verfolgt, wobei die ermittelten Begriffe als Kandidaten für Instanzen
          beschrieben werden. Als Referenzontologie wird eine auf CIDOC CRM-basierte Adaption
          verwendet (Hastik et al. 2013).</p>
         <p rend=""paper_abstract"">Dieses Projekt präsentiert einen innovativen Ansatz, um mit NLTK Kommentartexte aus Onlineforen der Demoszene zu annotieren. Das Standard-Tagset muss jedoch angepasst werden. Die Erweiterung der CIDOC CRM-basierten Ontologie auf Basis des generierten Indexes ermöglicht die semantische Beschreibung der Werke.</p>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Bird, Steven / Klein, Ewan / Loper, Edward</hi> (2015):
              <hi rend=""italic"">Natural Language Processing with Python</hi>. NLTK
              Book <ref target=""http://www.nltk.org/book/"">http://www.nltk.org/book/</ref>
              [letzter Zugriff 15. Februar 2016].</bibl>
               <bibl>
                  <hi rend=""bold"">Farzindar, Atefeh / Inkpen, Diana</hi> (2015): <hi rend=""italic"">Natural Language Processing for Social Media</hi>. San
              Francisco: Morgan & Claypool.</bibl>
               <bibl>
                  <hi rend=""bold"">Finin, Tim / Murnane, Will / Karandikar, Anand / Keller,
                Nicholas / Martineau, Justin</hi> (2010): ""Annotating Named Entities in
                Twitter Data with Crowdsourcing"", in: <hi rend=""italic"">Proceedings of the
                NAACL HLT</hi> 80–88. </bibl>
               <bibl>
                  <hi rend=""bold"">Gimpel, Kevin / Schneider, Nathan / O'Connor, Brendan /
                  Dipanjan, Das / Mills, Daniel / Eisenstein, Jacob / Heilman, Michael /
                  Yogatama, Dani / Flanigan, Jeffrey / Smith, Noah A.</hi> (2011):
                  ""Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments"",
                  in: <hi rend=""italic"">Proceedings of the 49th Annual Meeting of the
                  Association for Computational Linguistics</hi> 42-47. </bibl>
               <bibl>
                  <hi rend=""bold"">Hastik, Canan / Steinmetz, Arnd / Thull, Bernhard</hi>
                  (2013): ""Ontology based Framework for Real-Time Audiovisual Art"", in: <hi rend=""italic"">IFLA World Library and Information Congress</hi>. 79th
                  IFLA General Conference and Assembly: Audiovisual and Multimedia with
                  Cataloguing <ref target=""http://library.ifla.org/87/1/124-hastik-en.pdf"">http://library.ifla.org/87/1/124-hastik-en.pdf</ref> [letzter Zugriff
                  15. Februar 2016]. </bibl>
               <bibl>
                  <hi rend=""bold"">Maynard, Diana / Bontcheva, Kalina / Rout, Dominic</hi>
                  (2012): ""Challenges in Developing Opinion Mining Tools for Social Media"",
                  in: <hi rend=""italic"">Proceedings of @NLP can u tag #usergeneratedcontent?!
                  Workshop at International Conference on Language Resources and
                  Evaluation (LREC 2012)</hi> 8.</bibl>
               <bibl>
                  <hi rend=""bold"">Reyes, Antonio / Rosso, Paolo / Buscaldi, Davide</hi>
                  (2012): ""From Humor Recognition to Irony Detection: The Figurative Language
                  of Social Media"", in: <hi rend=""italic"">Data Knowledge Engineering</hi>.
                  Applications of Natural Language to Information Systems 74: 1-12. </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,annotation;cidoc crm;nlp;nltk;ontologie,German,annotieren;bereinigung;datenerkennung;inhaltsanalyse;modellierung;sprache;text
10506,2016 - University of Leipzig,University of Leipzig,Modellierung - Vernetzung – Visualisierung: Die Digital Humanities als fächerübergreifendes Forschungsparadigma,2016,DHd,DHd,Universität Leipzig (Leipzig University),Leipzig,,Germany,http://dhd2016.de/,Dramenwerkbank - Automatische Sprachverarbeitung zur Analyse von Figurenrede,,Andre Blessing;Peggy Bockwinkel;Nils Reiter;Marcus Willand,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Einleitung</head>
            <p>In diesem Beitrag stellen wir erste Einsichten aus einer quantitativen Analyse von Dramen vor, sowie unsere Konzeption für eine darauf aufbauende interaktive Werkbank, die einen Anstoß für eine Diskussion zur Tool-Unterstützung quantitativer Dramenanalyse geben soll. Die Werkbank unterstützt interessierte Forscherinnen und Forscher beim Einlesen von Dramen aus TEI-basierten Quellen und befindet sich noch in Entwicklung
          <ref type=""note"" target=""n01"" n=""1"">1</ref>. Neben den in Dramen schon explizit kodierten strukturellen Informationen (Wer spricht was?) stellt die Werkbank insbesondere Möglichkeiten zur Verfügung mit Werkzeugen zur maschinellen Sprachverarbeitung auch den Inhalt der Figurenrede zu analysieren. Inspektions- und Aggregationswerkzeuge und -sichten erlauben auch die Analyse größerer Korpora.
        </p>
            <p>Um die Anwendungsgebiete der Werkbank aufzuzeigen, skizzieren wir – anhand einer
          <hi rend=""italic"">Pilotstudie</hi> zur Analyse des Verhältnisses von
          dramatischer Figur zur dramatischen Handlung – den Problemhorizont quantitativer
          Literaturwissenschaft. Dabei interessieren uns insbesondere diese Fragen: Gibt
          es einen Zusammenhang zwischen angenommenen prototypischen Rollen (Protagonist,
          Intrigant, König usw.) und Länge bzw. Häufigkeit der Redebeiträge oder der
          Referenz auf die Figur? Wird über bestimmte Figuren(-rollen) auf bestimmte Arten
          gesprochen (abwertend / aufwertend, ...)? Gibt es
          Figuren(-rollen)konstellationen, die häufig kookurrieren, und zwar in Bezug auf
          ihren eigenen Rede- und Bühnenbeitrag als auch im Bezug auf die Referenzen auf
          die Figuren? </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Dramenanalyse: Basics</head>
            <p>Dramentexte unterscheiden sich insbesondere durch zwei zusammenhängende
            Eigenschaften von Prosatexten: a) Dramatische Texte sind im Gegensatz zu vielen
            anderen Textsorten auf allen Ebenen (Akt- bis Redefolge) ausgesprochen gut
            strukturiert und ermöglichen somit eine verhältnismäßig unaufwändige
            Datenerhebung. Die Kehrseite der guten Strukturiertheit ist dass dramatische
            Texte damit nicht dem Prototyp eines Textes entsprechen, wie er von vielen
            Werkzeugen zur Sprachverarbeitung angenommen wird. Die maschinelle
            Sprachverarbeitung auf dramatischen Texten ist damit nicht durch existierende
            Werkzeuge „out of the box“ zu leisten. b) Die dramatischen Figuren sprechen <hi rend=""italic"">unvermittelt</hi>. Unterscheidungen zwischen Erzähler- und
            Figurenrede und -denken spielen in Dramen keine Rolle. Während Ansätze der
            Stilometrie, das Figurensignal vom Erzähler- und jenes wiederum vom
            Gattungssignal zu trennen (Jannidis 2014), noch in den Kinderschuhen stecken,
            muss sich die (teil-)automatische quantitative Dramenanalyse diesen
            interpretativen Problemen nicht stellen. Sie hat vor allem <hi rend=""italic"">technisch</hi>- <hi rend=""italic"">methodische</hi> Probleme zu lösen: a)
            Erfassung und Einlesen der Daten und b) (teil-)automatische Textanalyse in
            Dramen. Zu letzterem gehört auch der adäquate Einsatz von interpretierbaren
            Maßen und transparenten Verfahren sowie visuellen Repräsentationen von
            Ergebnissen. </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Erfassung und Einlesen der Daten: TEI-Integration</head>
            <p>Eine automatisierte Erfassung der Oberflächenstruktur inklusiver aller relevanten
              Metadaten dramatischer Texte ist die Grundvoraussatzung einer quantitativen
              Textanalyse im oben genannten Sinne. TEI / XML ist als Standard etabliert, um
              Texte und Korpora möglichst genau entsprechend der/einer gedruckten Edition
              digital zu kodieren (cf. TextGrid; DTA). Insbesondere erlaubt TEI auch die
              Kodierung von Seitenzahlen, Formatierungen, Zeilenumbrüchen, Kopf- und Fußzeilen
              und vieles mehr, was über den reinen Textinhalt hinausgeht.</p>
            <p>Wie Trilcke et al. (2015) auch schon festgestellt haben, ist die Extraktion der inhaltlichen Textstruktur aus den TEI-Daten keineswegs trivial. Für Netzwerkanalyse ist die eindeutige Identifizierung von Figuren besonders relevant, für eine (maschinelle, computergestützte) Analyse des Inhaltes und der Häufigkeit der Figurenrede kommen o.g. Formatierungsmarkierungen noch als Herausforderung hinzu. In unserer Werkbank bieten wir einen Plausibilitätscheck an, der es erlaubt, Fehler im Importprozess (die sowohl durch Fehlannahmen im Importmodul als auch durch Fehlkodierungen in den Quelldaten verursacht werden können) direkt zu erkennen und zu beheben. Einmal identifizierte und behobene Fehler fließen in die Quelldaten zurück.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>(Automatische) Textanalyse in Dramen</head>
            <p>In den bereits existierenden Arbeiten zur Stilometrie auf Dramen werden komplette
                Dramen verglichen (z. B. durch Vorverarbeitung mit DIGIVOY). Ein differenzierter
                Vergleich, bei dem einzelne Figuren oder Gruppen von Figuren betrachtet werden,
                ist so noch nicht möglich gewesen.</p>
            <p>Andere Projekte gehen genau den gegenteiligen Weg und verwerfen alle Dialoginhalte und beziehen ihre Netzwerkanalyse nur auf die Interaktion der jeweils in der Szene aktiven Figuren (cf. Trilcke et al.). Uns ist kein verfügbares System bekannt, das diese Lücke schließt und eine inhaltliche Analyse erlaubt, die sowohl die Interaktion der aktiven Figuren als auch deren Redeinhalt einbezieht. </p>
            <p>In unserer Werkbank erfolgt die Textanalyse mit computerlinguistischen
                  Werkzeugen, welche durch die CLARIN-D Infrastruktur (Mahlow et al. 2014)
                  bereitgestellt werden. Der Aufbau von Dramen erfordert eine spezielle
                  Herangehensweise bei der Textanalyse, da die in der Computerlinguistik oft
                  getroffene Annahme, dass Texte aus vollständigen und grammatikalisch
                  wohlgeformten Sätzen bestehen, in Dramen nicht zutrifft (wie auch in Texten aus
                  sozialen Medien oder in gesprochener Sprache). Daneben weisen Dramen die oben
                  genannte spezifische Struktur auf, die eine adäquate Vorverarbeitung bedingt. Um
                  eine Verarbeitung mit einer nicht modifizierten CL-Verarbeitungskette zu
                  ermöglichen, wird das Drama vorher in passende Textsegmente zerlegt. Segmente,
                  die zu einem Dialog gehören müssen nach der Verarbeitung wieder der jeweiligen
                  Figur zugeordnet werden. Im Kontext der Figurenanalyse sind insbesondere
                  Eigennamenerkennung und Koreferenzresolution von Interesse. Wenn man den
                  stilometrischen Blick weitet und auch syntaktische Konstruktionen (verwendet
                  eine Figur mehr oder weniger komplexe Satzstruktur?) untersuchen möchte, sind
                  auch andere linguistische Verarbeitungsschritte möglich. </p>
            <p>Die Ergebnisse dieser Verarbeitung werden nicht fehlerfrei sein, deswegen bietet
                    die Werkbank Möglichkeiten, die Ergebnisse zu korrigieren. Insbesondere die
                    Zusammenführung von unterschiedlich genannten oder geschriebenen (z. B. „Emilia“
                    vs. „Emilie“ oder „die Soldaten“ vs. „erster Soldat“) Figuren ist nicht trivial
                    und teilweise nur durch zusätzliches Weltwissen realisierbar. Damit dieser
                    Schritt vereinfacht wird kommt hier ein halb-automatischer Figurenabgleich zum
                    Einsatz. Das überarbeitete und manuell geprüfte Drama kann in einem
                    TEI-konformen Format exportiert werden, damit die so kuratierte Ressource wieder
                    der Community zur Verfügung gestellt werden kann. Linguistische Annotationen,
                    die in TEI nicht direkt repräsentiert werden können, werden in einem geeigneten
                    stand-off-Format exportiert.</p>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Pilotstudie</head>
               <p>In einer Pilotstudie haben wir anhand eines einzelnen Dramas exploriert, wie
                        der Zusammenhang von (der zentralen) Dramenfigur zur dramatischen Handlung
                        automatisiert sichtbar gemacht werden kann. Die (zentrale) Stellung im
                        Figurennetzwerk wird dabei nicht (wie in der aktuellen Forschung gängig;
                        vgl. Moretti 2011) lediglich durch häufige Präsenz oder Interaktion auf der
                        Bühne repräsentiert, sondern durch differenziertere Analysen der
                        Figurenaktivität. <hi rend=""italic"">Wie häufig</hi> eine Figur spricht, <hi rend=""italic"">wie viel</hi> sie spricht und <hi rend=""italic"">wie häufig
                        über sie</hi> gesprochen wird, sind dabei die Kerndaten der
                        quantitativen Analyse, auf der weiter vorzustellende Analysen beruhen. Eine
                        manuelle Datenerfassung übersteigt jedoch selbst bei einzelnen Dramen
                        schnell den vom Menschen leistbaren Zeiteinsatz (wie die in Abbildung 1
                        manuell erstellte Erfassung der Redeteile in <hi rend=""italic"">Emilia
                        Galotti</hi> zeigt): </p>
               <figure>
                  <graphic n=""1001"" width=""11.7348cm"" height=""11.260666666666667cm"" url=""p077-image1.png"" rend=""inline""/>
                  <p rend=""figure"">
                     <hi rend=""bold"">Abb. 1</hi>: „ <hi rend=""bold"">Token von
                        x</hi>“ = Gesamthäufigkeit der Nennung jeder einzelnen Namensvariante.
                        <hi rend=""bold"">Figurennennung</hi> = Nennung der Namensvarianten in der
                        Rede anderer Figuren. <hi rend=""bold"">Redehäufigkeit</hi> = Wie oft spricht
                        eine Figur. <hi rend=""bold"">Gesamtzahl der Wörter…</hi> = Redelänge in
                        Wörtern. <hi rend=""bold"">(Aktivitäts)Quotient</hi> = Summe der
                        Redehäufigkeit geteilt durch die Summe der Figurennennung: X > 1 = Aktiv
                        (Redet häufiger als über sie geredet wird); X < 1 = Passiv. </p>
               </figure>
            </div>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>NLP-Unterstützte Analysemöglichkeiten in Dramen</head>
            <p>Die Kombination von in Dramen vorhandenen strukturellen Informationen und durch automatische Verarbeitung ermittelte inhaltlich-semantische Information erlaubt neue, feinkörnige Analysen von Dramen. Die im Folgenden genannten sollen durch die Werkbank unterstützt werden, entweder durch Integration existierender oder durch Entwicklung neuer Tools.</p>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Oberflächenanalyse der Figuren</head>
               <p>Möglich ist eine automatische Auswertung der Figurenreden nach inhaltlichen
                          Kriterien. Ohne Vorwissen bereitstellen zu müssen, lassen sich wichtige
                          Begriffe, durch deren Verwendung sich eine Figur von anderen unterscheidet,
                          mit Verfahren wie TF*IDF ermitteln und z. B. als Tabelle oder als Wortwolke
                          darstellen. Komplexere Verfahren wie topic modeling (Blei et al. 2003) oder
                          Wortfeldanalysen können natürlich auch auf den Redeinhalt einer Person (ggf.
                          auf Akte / Szenen o. ä. eingeschränkt) angewendet werden, erfordern aber
                          zumindest die Einstellung von Parametern (z. B. Anzahl der topics im topic
                          modelling) oder das Spezifizieren von Wortfeldern. Automatische Methoden zur
                          Erweiterung von Wortfeldern (angelehnt an z. B. Query Expansion, vgl.
                          Manning et al. 2008) können diesen Prozess unterstützen und sollen im
                          Rahmen der Werkbank erprobt und integriert werden. Abbildung 2 zeigt eine
                          visuelle Auswertung dieser Analyse.</p>
               <figure>
                  <graphic n=""1002"" width=""16.003763888888887cm"" height=""10.787944444444445cm"" url=""p077-image2.png"" rend=""inline""/>
                  <p rend=""figure"">
                     <hi rend=""bold"">Abb. 2</hi>: Strukturelle und inhaltliche
                          Analyse von Schillers Johanna von Orleans. Unten: Figurenaktivität. Oben:
                          Prominenz ausgewählter semantischer Räume in der Figurenrede (Frankreich,
                          Gott, Militär). </p>
               </figure>
               <figure>
                  <graphic n=""1003"" width=""15.801947222222223cm"" height=""6.265333333333333cm"" url=""p077-image3.png"" rend=""inline""/>
                  <p rend=""figure"">
                     <hi rend=""bold"">Abb. 3</hi>: Anhand der Häufigkeit der
                          Figurennennung („Emilia“ vs. „Tochter“) kann der (bisher kaum erforschte)
                          Diskursverlauf im Sinne einer Unterscheidung in private und öffentliche
                          Konversation sehr gut nachvollzogen werden. </p>
               </figure>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Stilometrische Analysen von Figurenreden</head>
               <p>Stilometrische Analysen werden durch eine Schnittstelle ermöglicht, durch die
                            man Figurenrede als Datenstrukturen in R abrufen und dann nach diversen
                            Kriterien untersuchen kann, etwa mit Hilfe von stylo (Eder et al. 2013). Es
                            ließe sich z. B. untersuchen, ob Könige bei Schiller anders sprechen als bei
                            Lessing, oder ob Bürgerfiguren in einem bestimmten Dramenkorpus anders
                            sprechen als Adelsfiguren:</p>
               <figure>
                  <graphic n=""1004"" width=""8.049772222222222cm"" height=""9.837011111111112cm"" url=""p077-image4.jpg"" rend=""inline""/>
                  <p rend=""figure"">
                     <hi rend=""bold"">Abb. 4</hi>: Figurenreden, extrahiert aus 34
                            Dramen; nach Standeszugehörigkeit benannt.</p>
               </figure>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Sentiment-Analyse</head>
               <p>Durch Methoden aus der Sentiment-Analyse (die zur automatisierten Analyse von
                              Produktreviews eingesetzt wird) ließe sich z. B. analysieren, wie und ob
                              bestimmte Figuren über andere sprechen. Neben positiv / negativ wären auch
                              feinere, dramenspezifische Unterscheidungen denkbar (Feigling, Hahnrei,
                              ...).</p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Kombination mit Netzwerkanalyse</head>
               <p>Die Kombination dieser Techniken mit Netzwerkanalyseverfahren würde es
                                erlauben, im Netzwerk auch Entitäten darzustellen über die geredet wird,
                                ohne dass sie direkt im Drama vorkommen (z. B. Gott), Kanten zwischen Knoten
                                können dann (z. B. durch Farben) auch inhaltliche, relationale Informationen
                                kodieren (X spricht viel / positiv über Y).</p>
               <p>Eine Netzwerkdarstellung, in der die Position der Figuren nicht mehr zufällig
                                  (oder durch Layout-Algorithmen gesteuert) ist ist ebenfalls denkbar
                                  (Abbildung 4). Dabei werden prototypischen Figurenrollen feste Positionen in
                                  einem Raster zugewiesen, so dass große Mengen an Netzwerken schnell und
                                  direkt verglichen werden können.</p>
            </div>
         </div>
      </body>
      <back>
         <div type=""Notes"">
            <note id=""n01"" n=""1"">
                             http://www.ims.uni-stuttgart.de/short/dramen</note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Blei, David / Ng, Andrew Y. / Jordan, Michael I.</hi>
                                    (2003): „Latent Dirichlet Allocation“, in: <hi rend=""italic"">Journal of
                                    Machine Learning Research</hi> 3: 993–1022. </bibl>
               <bibl>
                  <hi rend=""bold"">Eder, Maciej / Kestemont, Mike / Rybicki, Jan</hi> (2013):
                                    „Stylometry with R: a suite of tools“, in: <hi rend=""italic"">Digital
                                    Humanities 2013 Conference Abstracts</hi> 487-89. </bibl>
               <bibl>
                  <hi rend=""bold"">Jannidis, Fotis</hi> (2014): „Der Autor ganz nah.
                                    Autorstil in Stilistik und Stilometrie“, in: Schaffrick, Matthias / Marcus
                                    Willand (eds.): <hi rend=""italic"">Theorien und Praktiken der
                                    Autorschaft</hi>. Berlin: De Gruyter 169-195. </bibl>
               <bibl>
                  <hi rend=""bold"">Mahlow, Cerstin / Eckart, Kerstin / Stegmann, Jens /
                                      Blessing, Andre / Thiele, Gregor / Gärtner, Markus / Kuhn, Jonas</hi>
                                      (2014): „Resources, Tools, and Applications at the CLARIN Center Stuttgart“,
                                      in: <hi rend=""italic"">Akten der 12. Konferenz zur Verarbeitung natürlicher
                                      Sprache (KONVENS 2014)</hi> 11-21. </bibl>
               <bibl>
                  <hi rend=""bold"">Moretti, Franco</hi> (2011): <hi rend=""italic"">Network
                                      Theory, Plot Analysis</hi>. LiteraryLab Pamphlet 2: <ref target=""http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf"">http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf</ref> [letzter
                                      Zugriff 20. August 2014]. </bibl>
               <bibl>
                  <hi rend=""bold"">Manning, Christopher D / Raghavan, Prabhakar / Schütze,
                                          Hinrich</hi> (2008): <hi rend=""italic"">Introduction to Information
                                          Retrieval</hi>. Cambridge: Cambridge University Press. </bibl>
               <bibl>
                  <hi rend=""bold"">Trilcke, Peer / Fischer, Frank / Kampkaspar, Dario</hi>
                                          (2015): „Digital Network Analysis of Dramatic“, in: <hi rend=""italic"">Digital Humanities 2015 Conference Abstracts</hi>: <ref target=""http://dh2015.org/abstracts/xml/FISCHER_Frank_Digital_Network_Analysis_of_Dramati/FISCHER_Frank_Digital_Network_Analysis_of_Dramatic_Text.xml"">http://dh2015.org/abstracts/xml/FISCHER_Frank_Digital_Network_Analysis_of_ Dramati/FISCHER_Frank_Digital_Network_ Analysis_of_Dramatic _Text.xml</ref>
                                          [letzter Zugriff 16. Februar 2016].</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,drama;nlp;sprachverarbeitung;textanalyse;werkbank,German,annotieren;beziehungsanalyse;computer;inhaltsanalyse;literatur;metadaten;netzwerkanalyse;personen;software;stilistische analyse;strukturanalyse;text
10544,2016 - University of Leipzig,University of Leipzig,Modellierung - Vernetzung – Visualisierung: Die Digital Humanities als fächerübergreifendes Forschungsparadigma,2016,DHd,DHd,Universität Leipzig (Leipzig University),Leipzig,,Germany,http://dhd2016.de/,Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts,,Matthias Boenig;Kay-Michael Würzner;Arne Binder;Uwe Springmann,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Einleitung</head>
            <p>Dieser Beitrag stellt eine neuartige Methode zur optischen Zeichenerkennung (
          <hi rend=""italic"">Optical Character Recognition, </hi>OCR) speziell für Textvorlagen des 17. Jahrhunderts vor. Anstatt ein neues OCR-Verfahren zu entwickeln, werden zwei etablierte Open-Source-Lösungen genutzt. Die Ausgaben der Programme werden computergestützt kombiniert, um so eine möglichst genaues Textergebnis zu erhalten. Die Besonderheiten und die Güte der Methode wird anhand der Texterfassung von Gelegenheitsgedichten von Simon Dach illustriert.
        </p>
            <div id=""h.6sg223rxpuol"" type=""div2"" rend=""DH-Heading2"">
               <head>OCR</head>
               <p>OCR bezeichnet die Gesamtheit von Verfahren, die in der Lage sind, aus
            Rastergrafiken Schriftzeichen zu erkennen. Der Begriff wird sowohl für die
            eigentliche Mustererkennung als auch für den gesamten Prozess der
            Bildverarbeitung verwendet. Letzterer gliedert sich normalerweise in drei
            Schritte: <hi rend=""bold"">1. Bildoptimierung</hi>: Diese besteht aus der
            Bitonalisierung der Digitalisate, ihrer Begradigung (sog. <hi rend=""italic"">Deskewing</hi>) und aus der Entfernung von Artefakten (sog. <hi rend=""italic"">Despeckling</hi>). Außerdem können beim Scannen
            entstandene Wellen in einzelnen Zeilen automatisch begradigt werden (sog.
            <hi rend=""italic"">Dewarping</hi>). <hi rend=""bold"">2.
            Strukturerkennung</hi> ( <hi rend=""italic"">Optical Layout
            Recognition</hi>, OLR): Die einzelnen Seiten werden u. a. in Spalten,
            Absätze und Zeilen gegliedert. <hi rend=""bold"">3. Mustererkennung
          </hi>(OCR): Für diese Aufgabe gibt es verschiedene Lösungsvorschläge sowohl
          im kommerziellen wie auch im Open-Source-Bereich. Besonders verbreitet sind
          die Software <hi rend=""italic"">FineReader</hi> der Firma ABBYY sowie <hi rend=""italic"">BITAlpha </hi>aus dem Hause Tomasi, die u. a. von
          Bibliotheken eingesetzt werden. Die bekanntesten Open-Source-Lösungen sind
          das ursprünglich von Hewlett-Packard entwickelte und heute von Google
          betreute <hi rend=""italic"">Tesseract</hi> (GitHub 2016a) und das
          ursprünglich am DFKI Kaiserslautern entwickelte <hi rend=""italic"">OCRopus</hi> (GitHub 2016b). </p>
               <p>Grundsätzlich lassen sich bei OCR zwei unterschiedliche Erkennungsansätze unterscheiden: zeichenorientierte Verfahren wie Tesseract vergleichen das Bild eines Zeichens Pixel für Pixel mit einer Datenbasis (dem sog. Modell) und geben das ähnlichste Zeichen zurück. Sequenzorientierte (segmentierungsfreie) Verfahren wie OCRopus legen ein Raster fester Größe über eine Zeile und bestimmen anhand der Folgen der einzelnen Spalten, repräsentiert als Bitvektoren (0 entspricht weiß, 1 schwarz) die wahrscheinlichste Zeichensequenz. </p>
            </div>
            <div id=""h.jloxih9r95e7"" type=""div2"" rend=""DH-Heading2"">
               <head>Gelegenheitsgedichte</head>
               <p>Unsere Studie beschäftigt sich mit OCR am Beispiel von Gelegenheitsgedichten
            des 17. Jahrhunderts, denen durch die von Segebrecht (1977) initiierte
            literaturwissenschaftliche Neubewertung eine zunehmende kulturgeschichtliche
            Bedeutung zukommt (vgl. Klöker 2010: 39). Der Zugriff auf diese Drucke wurde
            durch das <ref target=""http://www.vd17.de/index.php?article_id=26"">VD17</ref> (HAB 2007-2016)<ref type=""note"" target=""n01"" n=""1"">1</ref> und durch das <hi rend=""italic"">Handbuch des personalen
            Gelegenheitsschrifttums in europäischen Bibliotheken und Archiven</hi>
            (Garber 2001-2013) erleichtert. Dennoch kann ein digitales Korpus für diese
            Textsorte heute nur als Desiderat wahrgenommen werden. Für Werke von Simon
            Dach ist die Ausgangslage scheinbar besser: Mit der digitalisierten
            vierbändigen Ausgabe von Ziesemer (Ziesemer 1936-1938) steht ein großer Teil
            der heute bekannten Gedichte zur Verfügung (vgl. auch Dach o. J.; TextGrid
            2015). <ref type=""note"" target=""n02"" n=""2"">2</ref> Jedoch trübt sich dieser Eindruck beim textkritischen Blick. <ref type=""note"" target=""n03"" n=""3"">3</ref>
               </p>
               <p>111 Funeralschriften Simon Dachs wurden im Verlauf des DFG-Pilotprojektes zum
            <hi rend=""italic"">OCR-Einsatz bei der Digitalisierung der
              Funeralschriften der Staatsbibliothek zu Berlin</hi> (2009-2011)
              (Federbusch / Polzin 2013) digitalisiert und per OCR erfasst. Die in der
              vorliegenden Studie genutzten Drucke zeichnen sich dahingehend aus, dass
              eine einheitliche Schrifttype sowie ein einfaches Layout vorliegen. Im
              Unterschied zu Texten des 18. und 19. Jahrhunderts war für diese Drucke noch
              ein relativ hoher manueller Aufwand erforderlich. Die Schrifttypen weisen
              daher eine vergleichsweise hohe Varianz bzgl. ihrer Form auf. Die 111
              Trauergedichte weisen eine Textgenauigkeit von bis zu 95% auf. Der
              Schwerpunkt der folgenden Studie liegt auf der Entwicklung und Prüfung von
              Methoden, die perspektivisch eine korrektere Übertragung der Textquellen aus
              dem 17. Jahrhundert liefern soll. </p>
            </div>
         </div>
         <div id=""h.wiw9vn3xlbve"" type=""div1"" rend=""DH-Heading1"">
            <head>Arbeitsablauf</head>
            <figure>
               <graphic n=""1001"" width=""13.335cm"" height=""10.107083333333334cm"" url=""032-image1.png"" rend=""inline""/>
               <p>
                  <hi rend=""bold"">Abb. 1</hi>: Modell eines vollständigen Erfassungsworkflows
                (diese Studie betrifft die eingefärbten Stationen). </p>
            </figure>
            <p>Abbildung 1 gibt einen Überblick über den Arbeitsablauf der hier vorgestellten Methode. Im Unterschied zu existierenden Workflows unterteilt unser Vorschlag die Bildoptimierung in zwei Phasen: 1.
                <hi rend=""italic"">global</hi>: Das komplette Digitalisat wird beschnitten, binarisiert, begradigt und von Artefakten befreit. Danach findet die Optische Layouterkennung (OLR) statt. 2.
                <hi rend=""italic"">lokal</hi>: Die identifizierten Textzonen werden aus dem Bild der Seite ausgeschnitten und nochmals begradigt. Dadurch wird die häufig zu beobachtende Trapezform der Digitalisate, die durch Scannen von Büchern ohne Auftrennen des Buchrückens entsteht, behandelt. Die Bilder für die einzelnen Zonen werden anschließend in Zeilen zerschnitten und den OCR-Engines übergeben.
              </p>
            <p>Unser Vorgehen bei der OCR orientiert sich an der manuellen Texterfassung per <hi rend=""italic"">Double Keying</hi>: Dabei werden Texte von zwei unabhängigen
                Erfassern transkribiert. Im Vergleich der beiden Textversionen werden die
                Unterschiede ermittelt und die korrekte Version ausgewählt. Um den
                Genauigkeitsgewinn durch die Mehrfacherfassung zu erhöhen, wurden zwei
                paradigmatisch verschiedene OCR-Verfahren, Tesseract und OCRopus, mit
                unterschiedlichen Stärken und Schwächen eingesetzt. Beide Open-Source-Programme
                erlauben ein Training auf die vorwendeten Typen und die Anwendung spezifischer
                OCR-Modelle. Dies ist wie Springmann et al. (2015) zeigen ein wesentlicher
                Vorteil gegenüber den meisten Closed-Source-Lösungen, da die mitgelieferten
                OCR-Modelle insbesondere für frühe Druckerzeugnisse bzw. gebrochene Schriften
                sehr schlechte Ergebnisse bzgl. der Textgenauigkeit liefern. Die automatische
                Vereinigung der beiden Textversionen findet im Wesentlichen auf Basis einer
                Textdifferenzberechnung mit Hilfe von <hi rend=""italic"">diff</hi> (Hunt /
                McIlroy 1976) statt, wobei im Falle von Unterschieden verschiedene
                Bewertungsheuristiken zur Bestimmung der <hi rend=""italic"">korrekten</hi>
                Textversion eingesetzt werden. Das skizzierte Vorgehen erlaubt auch die
                Kombination von mehr als zwei Textversionen sowie den anschließenden Einsatz von
                OCR-Nachkorrekturverfahren (vgl. z. B. Vobl et al. 2014). </p>
         </div>
         <div id=""h.yuu4lt9yvmnl"" type=""div1"" rend=""DH-Heading1"">
            <head>Evaluation</head>
            <p>Die Güte der hier vorgestellten Methode wird anhand der Volltexterfassung von
                  Funeralschriften Simon Dachs (vgl. 1.2) evaluiert. Dabei konzentriert sich die
                  Evaluation auf drei Punkte: </p>
            <list type=""unordered"">
               <item>Welchen Einfluss hat die Wahl der Binarisierungsmethode auf die Textgenauigkeit?</item>
               <item>Wie groß ist der Unterschied zwischen einem Standardmodell und einem speziell für die zu erfassenden Texte trainierten Modell bzgl. der Textgenauigkeit?</item>
               <item>Kann die Vereinigung zweier durch OCR erzeugter Texte die Textgenauigkeit erhöhen?</item>
            </list>
            <p>Ein typisches Beispiel für die Untersuchungsgrundlage sowie die entsprechenden OCR-Ausgaben gibt Abbildung 2.</p>
            <figure>
               <graphic n=""1002"" width=""16.00113888888889cm"" height=""8.960555555555555cm"" url=""032-image2.png"" rend=""inline""/>
               <p>
                  <hi rend=""bold"">Abb. 2</hi>: Vergleich der OCR-Ergebnisse. </p>
            </figure>
         </div>
         <div id=""h.2eo3ulvoieam"" type=""div1"" rend=""DH-Heading1"">
            <head>Material</head>
            <div id=""h.wnodj54ezvdz"" type=""div2"" rend=""DH-Heading2"">
               <head>Ground Truth</head>
               <p>Voraussetzung für die Evaluation und das Modelltraining ist fehlerfreier
                        Volltext ( <hi rend=""italic"">Ground Truth</hi>). Um für die Studie
                        entsprechende Daten zu gewinnen, wurde eine manuelle Korrektur aller 111
                        Texte vorgenommen. Die Korrektur schloss nicht nur die Text-, sondern auch
                        die datenstrukturelle Ebene ein. Der Aufwand belief sich auf 150 Stunden. Im
                        Ergebnis liegen alle Texte im DTA-Basisformat vor und sind über die
                        Qualitätssicherungsplattform <ref target=""http://www.deutschestextarchiv.de/dtaq"">DTAQ</ref> zugänglich. </p>
            </div>
            <div id=""h.ns21lwuxkxmp"" type=""div2"" rend=""DH-Heading2"">
               <head>Materialauswahl</head>
               <p>Für das Training der spezifischen OCR-Modelle wurden 30 Seiten Ground-Truth zufällig ausgewählt. Für die Evaluation der Modelle wurden 25 andere zufällig ausgewählte Seiten verwendet.</p>
            </div>
            <div id=""h.b6hw3l3e9xjl"" type=""div2"" rend=""DH-Heading2"">
               <head>Referenzlexikon</head>
               <p>Zur Vereinigung beider OCR-Versionen wurde ein Referenzlexikon gültiger historischer Schreibungen des 17. Jahrhunderts herangezogen. Dazu wurden Wortformen (
                          <hi rend=""italic"">n</hi>=217067) aus DTA-Texten dieses Zeitraums extrahiert.
                        </p>
            </div>
         </div>
         <div id=""h.x3h4k58kl4a6"" type=""div1"" rend=""DH-Heading1"">
            <head>Durchführung</head>
            <div id=""h.uc8w3e3rtefw"" type=""div2"" rend=""DH-Heading2"">
               <head>Vorverarbeitung</head>
               <p>Für Beschneidung und Begradigung wurde das Programm <hi rend=""italic"">Scantailor</hi> (GitHub 2016 a) eingesetzt. Für die Binarisierung,
                          Artefaktbereinigung und Zeilenglättung wurde sowohl Scantailor als auch das
                          in OCRopus enthaltene Werkzeug <hi rend=""italic"">nlbin</hi> verwendet. </p>
            </div>
            <div id=""h.2pikwls1fou8"" type=""div2"" rend=""DH-Heading2"">
               <head>OLR</head>
               <p>Die einzelnen Textzonen (Abschnitte und Kustoden) wurden mit Hilfe von <hi rend=""italic"">Leptonica</hi> (Bloomberg 2001-2015) lokalisiert und
                            manuell nachkorrigiert. Für die Untergliederung der Zonen in Zeilen wurde
                            ebenfalls Leptonica eingesetzt. </p>
            </div>
            <div id=""h.4hfubd3yu7tf"" type=""div2"" rend=""DH-Heading2"">
               <head>OCR</head>
               <p>Die Zeichenerkennung erfolgte sowohl mit OCRopus als auch mit Tesseract. Die erste Versuchsreihe basierte auf mitgelieferten Modellen. Für die zweite Versuchsreihe wurden die OCR-Programme mit Ground-Truth-Daten trainiert. Für das Training der OCRopus-Modelle wurde OCRopus eingesetzt. Dabei wurde für das Training aus Gründen der Modellvergleichbarkeit eine feste Anzahl von Iterationsschritten (
                              <hi rend=""italic"">n</hi>=30000) festgelegt. Die Tesseract-Modelle wurden mit Hilfe von
                              <hi rend=""italic"">VietOCR</hi> erstellt.
                            </p>
            </div>
            <div id=""h.kw5xpj4qoltv"" type=""div2"" rend=""DH-Heading2"">
               <head>Textvereinigung</head>
               <p>Die Textvereinigung wurde in
                              <hi rend=""italic"">Python</hi> mit Hilfe des Moduls
                              <hi rend=""italic"">difflib</hi> implementiert. Neben dem Referenzlexikon standen zur Konfliktauflösung auch die von den OCR-Programmen zurückgelieferten Konfidenzen auf Zeichenebene zur Verfügung. Waren sich die beiden Engines bzgl. eines Wortes bzw. einer Textsequenz uneins, wurde zunächst dem Wort Vorrang gegeben, dass sich im Referenzlexikon befindet. Konnte dort keine der beiden Versionen gefunden werden, wurde die Entscheidung auf Basis der Konfidenzwerte getroffen.
                            </p>
            </div>
            <div id=""h.v61tqy2cwm7u"" type=""div2"" rend=""DH-Heading2"">
               <head>Qualitätsmessung</head>
               <p>Die Bestimmung der Textqualität erfolgte durch Messung des Anteils falsch erkannter Zeichen (Fehlerrate in Prozent) im Vergleich zum fehlerfreien Volltext.</p>
            </div>
         </div>
         <div id=""h.r6svsi1idfgr"" type=""div1"" rend=""DH-Heading1"">
            <head>Ergebnisse und Diskussion</head>
            <p>Tabelle 1 gibt einen Überblick über die Ergebnisse der Evaluation bzgl. der
                            Fehlerrate auf Zeichenebene unter Berücksichtigung der Vorverarbeitung des
                            Trainings- und Testmaterials, der Modellklasse (standard vs. spezifisch) und der
                            eingesetzten OCR-Software (OCRopus, Tesseract). Das beste (<hi rend=""bold color(38761D)"">grün</hi>) und das schlechteste Ergebnis (<hi rend=""bold color(980000)"">rot</hi>) sind hervorgehoben. Da wir keinen
                            Einfluss auf die Vorverarbeitung der Trainingsmaterialien der mitgelieferten
                            Modelle haben, ist die Matrix in dieser Hinsicht unvollständig. </p>
            <figure>
               <graphic n=""1004"" url=""032-image4.svg"" rend=""inline""/>
               <p>
                  <hi rend=""bold"">Tab. 1</hi>: Darstellung der Ergebnisse auf Einzel-OCR-Ebene im Bezug auf
                                Vorverarbeitungsmethode für Trainings- und Testmaterial, Modelltyp und verwendete
                                OCR-Software. </p>
            </figure>
            <p>Die geringste erreichte Fehlerrate (3,89 %) liegt etwa im Bereich der
                                Textgenauigkeit der 111 Gedichte aus der Pilotstudie von Federbusch (Federbusch
                                / Polzin 2013). Die Fehlerrate von Tesseract ist jeweils höher als die von
                                OCRopus. Der sequenzorientierte Ansatz hat klare Vorteile bei der Erkennung von
                                Schriftzeichen, die die typischen Charakteristika früher Drucke aufweisen. <ref type=""note"" target=""n05"" n=""5"">5</ref>
            </p>
            <p>Desweiteren zeigt sich, dass die Vorverarbeitung mit nlbin für Tesseract sowohl auf Trainings- als auch auf Testebene jeweils schlechtere Ergebnisse bringt. Für OCRopus sind die Ergebnisse bzgl. der Vorverarbeitung differenzierter: Die beste Kombination liefert eine Vorverarbeitung des Trainingsmaterials mit nlbin bei einer nachfolgenden Vorverarbeitung des Testmaterials mit Scantailor. Unterschiede im Ergebnis der Vorverarbeitung beider Programme illustriert Abbildung 3.</p>
            <figure>
               <graphic n=""1003"" width=""16.002cm"" height=""2.806347222222222cm"" url=""032-image3.png"" rend=""inline""/>
               <p> Abb. 3: Bild einer Textzeile nach der Vorverarbeitung mit nlbin (oben) und
                                  Scantailor (unten). </p>
            </figure>
            <p>Die von Scantailor durchgeführte Bildvorverarbeitung ist deutlich normativer und für einen zeichenorientierten Ansatz wie Tesseract besser geeignet. Das Training sequenzorientierter Ansätze leidet unter dieser Vergröberung.</p>
            <p>Es zeigt sich erneut, dass spezifisch trainierte Modelle eine massive Textgenauigkeitsverbesserung mit sich bringen können (vgl. auch Springmann et al. 2015).</p>
            <div id=""h.b53zdskw38g8"" type=""div2"" rend=""DH-Heading2"">
               <head>Textvereinigung</head>
               <p>Betrachtet man die Beispielausgaben in Abbildung 2, so wird der
                                    Qualitätsunterschied zwischen beiden OCR-Programmen ersichtlich. An
                                    einzelnen Stellen jedoch (z. B. Großbuchstaben am Anfang der Zeile im
                                    letzten Abschnitt) hat Tesseract Erkennungsvorteile.</p>
               <p>Ausgehend von diesem Befund wurde der jeweils genaueste Text von OCRopus und Tesseract miteinander vereinigt. Es hat sich gezeigt, dass die Konfidenzen, die die Programme für jedes Zeichen zurückliefern, kein verlässliches Kriterium sind, um Konflikte aufzulösen. Die Fehlerrate nimmt zu. Die Strategie, Wörter bzw. Sequenzen zu bevorzugen, die sich im Referenzlexikon befinden, hat dagegen eine messbare Verbesserung mit sich gebracht. Die Anzahl der falsch erkannten Zeichen konnte um 14 % reduziert werden (Fehlerrate 3,34 %). Es ist zu vermuten, dass der Effekt größer wäre, wenn zwei OCR-Ergebnisse mit vergleichbarer Qualität vorlägen. Dies bleibt jedoch zum jetzigen Zeitpunkt für Drucke des 17. Jahrhunderts ein Desiderat.</p>
            </div>
         </div>
      </body>
      <back>
         <div type=""Notes"">
            <note id=""n01"" n=""1"">Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts.</note>
            <note id=""n02"" n=""2"">Vgl auch Dach (o. J.) in <ref target=""http://www.zeno.org/Literatur/M/Dach,+Simon/Gedichte"">http://www.zeno.org/Literatur/M/Dach,+Simon/Gedichte</ref> sowie <ref target=""https://textgrid.de/digitale-bibliothek"">TextGrid</ref> (2015).</note>
            <note id=""n03"" n=""3"">„Ziesemers Dach-Ausgabe ist textlich zu wenig genau, um auch für die dort abgedruckten, fast ausnahmslos deutschsprachigen, Gedichte den Rückgriff auf die kasualen Einzeldrucke und andere zeitgenössische Ausgaben entbehren zu können. Jede Stichprobe erweist für jedes einzelne Gedicht Transkriptionsfehler und unerklärte Texteingriffe.“ (Walter 2008: 466)</note>
            <note id=""n05"" n=""5"">Für Frakturdrucke des 19. Jahrhunderts ist ein solch starker Unterschied zwischen den Tesseract und OCRopus nicht nachgewiesen.</note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Bloomberg, Dan</hi> (2001-2015): Leptonica <ref target=""http://www.leptonica.com/"">http://www.leptonica.com/</ref>
                                    [letzter Zugriff: 15. Oktober 2015].</bibl>
               <bibl>
                  <hi rend=""bold"">Dach, Simon</hi> (o. J.): <hi rend=""italic"">Gedichte</hi>
                  <ref target=""http://www.zeno.org/Literatur/M/Dach,+Simon/Gedichte"">
                                      http://www.zeno.org/Literatur/M/Dach,+Simon/Gedichte</ref> [letzter
                                      Zugriff 15. Oktober 2015]. </bibl>
               <bibl>
                  <hi rend=""bold"">Federbusch, Maria / Polzin, Christian</hi> (2013): <hi rend=""italic"">Volltext via OCR - Möglichkeiten und Grenzen</hi>.
                                      Testszenarien zu den Funeralschriften der Staatsbibliothek zu Berlin -
                                      Preußischer Kulturbesitz. Berlin Staatsbibliothek zu Berlin <ref target=""http://staatsbibliothek-berlin.de/fileadmin/user_upload/zentrale_Seiten/historische_drucke/pdf/SBB_OCR_STUDIE_WEBVERSION_Final.pdf "">http://staatsbibliothek-berlin.de/fileadmin/user_upload/zentrale_Seiten/historische_drucke/pdf/SBB_OCR_STUDIE_WEBVERSION_Final.pdf</ref>
                                      [letzter Zugriff 15. Oktober 2015].</bibl>
               <bibl>
                  <hi rend=""bold"">Garber, Klaus</hi> (2001-2013): <hi rend=""italic"">Handbuch
                                      des personalen Gelegenheitsschrifttums in europäischen Bibliotheken und
                                      Archiven</hi>. 13 Bände. Hildesheim / Zürich / New York: Olms /
                                      Weidmann. </bibl>
               <bibl>
                  <hi rend=""bold"">GitHub Inc.</hi> (2016a): <hi rend=""italic"">ScanTailor</hi>
                  <ref target=""http://scantailor.org"">http://scantailor.org/</ref> [letzter
                                      Zugriff 15. Oktober 2015].</bibl>
               <bibl>
                  <hi rend=""bold"">GitHub Inc.</hi> (2016b): <hi rend=""italic"">OCRopus</hi>
                  <ref target=""https://github.com/tmbdev/ocropy"">https://github.com/tmbdev/ocropy</ref> [letzter Zugriff 15. Oktober
                                        2015]. </bibl>
               <bibl>
                  <hi rend=""bold"">GitHub Inc.</hi> (2016c): <hi rend=""italic"">Tesseract</hi>
                  <ref target=""https://github.com/tesseract-ocr"">https://github.com/tesseract-ocr</ref> [letzter Zugriff 15. Oktober
                                          2015].</bibl>
               <bibl>
                  <hi rend=""bold"">HAB = Herzog August Bibliothek Wolfenbüttel</hi>
                                          (2007-2016): <hi rend=""italic"">VD17</hi>. Das Verzeichnis der im deutschen
                                          Sprachraum erschienenen Druck des 17. Jahrhunderts <ref target=""http://www.vd17.de/index.php?category_id=1&article_id=1&clang=0"">http://www.vd17.de/index.php?category_id=1&article_id=1&clang=0</ref>.</bibl>
               <bibl>
                  <hi rend=""bold"">Hunt, James W. / McIlroy, M. Douglas</hi> (1976): ""An
                                          Algorithm for Differential File Comparison"" in: <hi rend=""italic"">Computing
                                          Science Technical Report</hi> (Bell Laboratories) 41 <ref target=""http://www.cs.dartmouth.edu/~doug/diff.pdf"">http://www.cs.dartmouth.edu/~doug/diff.pdf</ref>
               </bibl>
               <bibl>
                  <hi rend=""bold"">Klöker, Martin</hi> (2010): ""Das Testfeld der Poesie.
                                          Empirische Betrachtungen aus dem Osnabrücker Projekt zur 'Erfassung und
                                          Erschließung von personalen Gelegenheitsgedichten'"", in: Keller, Andreas /
                                          Lösel, Elke / Wels, Ulrike / Wels, Volkhard (eds.): <hi rend=""italic"">Theorie und Praxis der Kasualdichtung in der Frühen Neuzeit</hi> (=
                                          Chloe. Beihefte zu Daphne 43). Amsterdam / New York: Rodopi 39-84. </bibl>
               <bibl>
                  <hi rend=""bold"">Python Software Fundation</hi> (1990-2016): <hi rend=""italic"">difflib - Helpers for Computing Deltas </hi>
                  <ref target=""https://docs.python.org/2/library/difflib.html"">https://docs.python.org/2/library/difflib.html</ref> [letzter Zugriff
                                          15. Oktober 2015].</bibl>
               <bibl>
                  <hi rend=""bold"">Segebrecht, Wulf </hi>(1977): <hi rend=""italic"">Das
                                          Gelegenheitsgedicht</hi>. Ein Beitrag zur Geschichte und Poetik der
                                          deutschen Lyrik. Suttgart: Metzler.</bibl>
               <bibl>
                  <hi rend=""bold"">Springmann, Uwe / Lüdeling, Anke / Schremmer, Felix</hi>
                                          (2015): ""Zur OCR frühneuzeitlicher Drucke am Beispiel des RIDGES-Korpus von
                                          Kräutertexten (Poster)"", in: <hi rend=""italic"">Tagung der DHd (Digitale
                                          Geisteswissenschaften im deutschsprachigen Raum)</hi>, Graz <ref target=""https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/mitarbeiter-innen/anke/pdf/SpringmannLuedelingSchremmer2015.pdf"">https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/mitarbeiter-innen/anke/pdf/SpringmannLuedelingSchremmer2015.pdf</ref>
                                          [letzter Zugriff 15. Oktober 2015].</bibl>
               <bibl>
                  <hi rend=""bold"">TextGrid </hi>(2015): <hi rend=""italic"">Die digitale
                                          Bibliothek bei TextGrid</hi>
                  <ref target=""https://textgrid.de/digitale-bibliothek"">https://textgrid.de/digitale-bibliothek</ref> [letzter Zugriff 15.
                                            Oktober 2015] </bibl>
               <bibl>
                  <hi rend=""bold"">VietOCR</hi>
                  <ref target=""http://vietocr.sourceforge.net/"">http://vietocr.sourceforge.net/</ref> [letzter Zugriff: 15. Oktober
                                              2015].</bibl>
               <bibl>
                  <hi rend=""bold"">Vobl, Thorsten / Gotscharek, Annette / Reffle, Uli /
                                                Ringlstetter, Christoph / Schulz, Klaus U.</hi> (2014): ""PoCoTo - an
                                                open source system for efficient interactive postcorrection of OCRed
                                                historical texts"" in: <hi rend=""italic"">Proceedings of the First
                                                International Conference on Digital Access to Textual Cultural Heritage
                                                (DATeCH '14)</hi>: 57-61 <ref target=""http://dl.acm.org/citation.cfm?id=2595197"">http://dl.acm.org/citation.cfm?id=2595197</ref> [letzter Zugriff 15.
                                                Oktober 2015].</bibl>
               <bibl>
                  <hi rend=""bold"">Walter, Axel E.</hi>(2008): ""Dach digital? Vorschläge zu
                                                einer Bibliographie und Edition des Gesamtwerks von Simon Dach nebst einigen
                                                erläuterten Beispielen vernachlässigter bzw. unbekannter Gedichte"", in:
                                                Walter, Axel E. (ed.) in: <hi rend=""italic"">Simon Dach (1605–1659)</hi>.
                                                Werk und Nachwirken. Tübingen: Niemeyer: 465-522.</bibl>
               <bibl>
                  <hi rend=""bold"">Ziesemer, Walter</hi> (ed.) (1936-1938): <hi rend=""italic"">Simon Dach: Gedichte</hi>. Vier Bände. Halle an der Saale:
                                                Niemeyer.</bibl>
               <lb/>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,17. jahrhundert;ocr;ocropus;tesseract;texterfassung,German,annotieren;bearbeitung;computer;datei;datenerkennung;modellierung;programmierung;text;transkription;umwandlung
10552,2016 - University of Leipzig,University of Leipzig,Modellierung - Vernetzung – Visualisierung: Die Digital Humanities als fächerübergreifendes Forschungsparadigma,2016,DHd,DHd,Universität Leipzig (Leipzig University),Leipzig,,Germany,http://dhd2016.de/,Attribuierung direkter Reden in deutschen Romanen des 18.-20. Jahrhunderts. Methoden zur Bestimmung des Sprechers und des Angesprochenen,,Markus Krug;Fotis Jannidis;Isabella Reger;Luisa Macharowsky;Lukas Weimer;Frank Puppe,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"">
            <head>
               <anchor id=""id_docs-internal-guid-4378e1fa-6ca5-cc9e-2c63-ec95391fd4ab""/>Problembeschreibung </head>
            <p>
               <anchor id=""id_docs-internal-guid-1f47a959-6ca3-8fa1-004e-73ced1fafd05""/>Im
            Folgenden wird ein Verfahren vorgestellt, das die automatische Zuordnung von
            direkter Rede in Erzähltexten sowohl zur sprechenden als auch zur angesprochenen
            Figur ermöglicht. Kann man eine solche automatische Zuordnung vornehmen,
            ermöglicht dies die Extraktion eines sozialen Netzwerks aus einem Text, wobei
            die Figuren als Knoten und die direkte Rede als Kanten modelliert werden (Elson
            / Dames 2010), aber sie kann auch eine wichtige Informationsquelle für andere
            analytische Schritte sein, z.B. zur Verbesserung der Koreferenzresolution oder
            zur Analyse der Quelle der Zuschreibung von Figurenattributen. </p>
         </div>
         <div type=""div1"">
            <head>Stand der Forschung</head>
            <p>Eine der ersten Arbeiten auf diesem Gebiet ist das System ESPER (Zhang et al.
              2003), das direkte Reden innerhalb von Kindergeschichten erkennen soll. Das
              System extrahiert zunächst die direkten Reden im Text und klassifiziert diese
              mit einem Entscheidungsbaum in zwei Kategorien, Sprecherwechsel bzw. kein
              Sprecherwechsel. Evaluiert werden die Ergebnisse mit zwei manuell annotierten,
              sehr unterschiedlichen Geschichten. Sie berichten eine Genauigkeit, gemeint ist
              hier die Anzahl der korrekt bestimmten Sprecher für alle direkten Reden, von
              47.6% und 86.7%. Glass und Bangay (2006), ebenfalls regelbasiert, bestimmen
              zunächst für eine direkte Rede das Kommunikationsverb und anschließend eine
              Menge von Akteuren, woraus letztendlich der Sprecher bestimmt wird. Sie
              evaluieren ihre Techniken auf 13 englischsprachigen fiktionalen Werken und
              berichten eine Genauigkeit von 79.4% (Glass / Bangay 2007). Iosif und Mishra
              (2014)  folgen im Prinzip dem Schema von Glass und Bangay (2007), ergänzen es
              aber durch eine aufwendigere Vorverarbeitung einschließlich
              Koreferenzresolution. Sie erreichen eine Genauigkeit von ca 84.5% und zählen
              damit zu den besten bisher veröffentlichten Ergebnissen. Ruppenhofer und andere
              (Ruppenhofer et al. 2010) berichten einen F-Score von 79% in der Zuordnung von
              Politikern zu ihren Aussagen in deutschsprachigen Kabinettsprotokollen aus den
              Jahren 1949-1960.</p>
            <p> Neben diesen regelbasierten Ansätzen werden auch maschinelle Lernverfahren
                eingesetzt. Zu den ersten erfolgreichen Systemen zählt das von Elson und McKeown
                (2010). Ihre Daten für die Sprecherzuordnung ließen sie über Amazons  <hi rend=""italic"">Mechanical Turk </hi>
                System bearbeiten. Ihr System klassifiziert zunächst
                regelbasiert eine direkte Rede in eine von fünf syntaktischen Kategorien.
                Für jede dieser Kategorien wurden anschließend eigenständige maschinelle
                Lernverfahren trainiert. Insgesamt erreichen sie eine Genauigkeit von etwa
                83%, ausgewertet anhand von englischen Romanen des 19 Jahrhunderts. O’Keefe
                und andere (O’Keefe et al. 2012), die an Elson und McKeowns Ansatz die
                Erstellung des Goldstandards und auch die praxisferne Verwendung von
                Informationen aus dem Goldstandard kritisieren,  betrachten die Zuordnung
                als Sequenzproblem. Sie nutzen die Klassifikationsangaben von vorhergehenden
                direkten Reden als Features für die gesamte Sequenz. In ihrer Evaluation
                vergleichen Sie drei Verfahren mit einer sehr einfachen regelbasierten
                Baseline. Ihre Ergebnisse bei der Anwendung des Systems auf zwei
                Zeitungskorpora - Wall Street Journal und Sydney Morning Herald - sowie die
                Sammlung literarischer Texte aus der Arbeit von Elson und McKeown zeigen
                einen großen Unterschied zwischen den Domänen. Sie erreichen auf den beiden
                Zeitungskorpora 84.1% (WSJ) bzw. 91.7% (SMH) Genauigkeit. Auf dem
                literarischen Korpus erreichen sie dagegen lediglich eine maximale
                Genauigkeit von 49%. (He et al. 2013) erreichen mit einem auf Ranking
                basierten maschinellem Lernverfahren unter der Ausnutzung von Features des
                Actor-Topic Modells (Celikyilmaz et al. 2010) auf dem Elson und
                McKeown-Korpus eine Genauigkeit zwischen 74.8% und 80.3%. Almeida und andere
                gehen von einer engen Verflechtung von Koreferenzresolution und
                Sprecherattribution aus und integrieren dabei beide Verfahren in ihrem
                Ansatz; die Ergebnisse der beiden einzelnen Lernverfahren werden in einem
                dritten Schritt verbunden. Sie erreichen damit 88.1% Genauigkeit (Almeida et
                al. 2014). Neuere Versuche mit Deep Learning-Verfahren aufgrund der Sprache
                der Figuren haben nur Genauigkeiten  von unter 50% erreicht (Chaganty /
                Muzny 2014).
              </p>
            <p>Die Zuordnung einer angesprochenen Figur wurde unserer Wissens noch in keiner anderen Arbeit untersucht.</p>
         </div>
         <div type=""div1"">
            <head>Daten und Annotation</head>
            <p> Für diese Arbeit verwenden wir Abschnitte des frei zugänglichen Korpus DROC.
                DROC besteht aus 89 Romanausschnitten, jeweils 130 Sätze lang, in denen alle
                Figurenreferenzen (mit und ohne Namen) und Koreferenzen annotiert sind. Aus dem
                Korpus wurden 77 Ausschnitte ausgewählt und mit einem eigens entwickelten Tool
                alle direkten Reden sowie die zugehörigen Sprecher und angesprochenen Figuren
                eingetragen. Jeder Text wurde von einem Annotator bearbeitet; eine zweite
                Annotation ist vorgesehen. Insgesamt wurden so 2264 direkte Reden mit Sprecher
                und Angesprochenen annotiert. Für die in Abschnitt 5 diskutierten Experimente
                wurde das Korpus in drei zufällige Mengen aufgeteilt:  </p>
            <figure>
               <graphic url=""v40-table1.png"" rend=""inline""/>
               <p rend=""figure"">
                  <hi rend=""bold"">Tab. 1</hi>: Überblick über die Auftrennung des in dieser Arbeit
                  verwendeten Korpus.</p>
            </figure>
         </div>
         <div type=""div1"">
            <head>Methoden</head>
            <p>Wir verwenden regelbasierte Verfahren und maschinelle Lernverfahren, aber anders
                  als in (He et al. 2013) oder (O’Keefe et al. 2012) dienen erstere nicht nur als
                  Baseline-Verfahren, sondern wurden soweit wie möglich optimiert. </p>
            <p>Wir verwenden die Techniken 2-Way Klassifikation und N-Way Klassifikation wie in
                    (O’Keefe et al. 2012) vorgeschlagen. Zusätzlich evaluieren wir
                    MaxEnt2WayToMatch, bei dem Kandidaten nur bis zum ersten tatsächlichen
                    Sprecherkandidaten erzeugt werden.</p>
            <p>Für die Sprecherzuordnung und Zuordnung eines Angesprochenen sind die in dieser Arbeit verwendeten Features in Tabelle A1 im Anhang zusammengefasst. </p>
            <p> Für diese Aufgabe haben sich regelbasierte Verfahren als konkurrenzfähig mit den
                      aktuellen ML-Verfahren erwiesen. Sie besitzen außerden den Vorteil, dass sie
                      nicht so viele Trainingsbeispiele benötigen. Die Grundstruktur des Algorithmus
                      ist der Idee des regelbasierten Koreferenzsystems von Stanford (Lee et al. 2011)
                      angelehnt. Es werden eine Reihe von Regelpässen nacheinander ausgeführt. Die
                      Regelpässe sind gemäß ihrer <hi rend=""italic"">Precision</hi> geordnet, d. h.
                      Regeln mit einer hohen <hi rend=""italic"">Precision</hi> werden zuerst
                      ausgeführt. Eine spätere Regel kann eine Entscheidung einer früheren Regel nicht
                      revidieren. Tabelle A2 im Anhang zeigt die in dieser Arbeit verwendeten
                      Regelpässe.  </p>
            <p>Mit Hilfe der Trainingsdaten konnte eine optimale Reihenfolge der Ausführung der Regeln empirisch ermittelt werden, bei der einige Regeln auch mehrfach angewendet werden. </p>
            <p>(1)→(2)→(3)→(4)→(5)→(6)→(7)→(5)→(6)→(8) →(9)→(5)→(6)→(7)→(10).</p>
         </div>
         <div type=""div1"">
            <head>Evaluation</head>
            <p>Die Parameter für die ML-Verfahren wurden auf dem Development-Anteil der Daten optimiert und anschließend gegen die Testmenge evaluiert. Für die regelbasierten Verfahren gibt es keine Unterscheidung zwischen Trainings- und Development-Korpus. Ein Sprecher gilt als korrekt bestimmt, wenn sich der vom System bestimmte Kandidat in der selben Koreferenzkette befindet, wie die Entität, die von unserem Annotator als korrekt markiert wurde. Tabelle 2 beschreibt die Ergebnisse bei der Anwendung der Verfahren auf das Testkorpus.</p>
            <figure>
               <graphic url=""v40-table2.png"" rend=""inline""/>
               <p rend=""figure"">
                  <hi rend=""bold"">Tab. 2</hi>: Ergebnisse der einzelnen Verfahren auf dem
                        Testkorpus, bestehend aus 20 zufällig gewählten Romanfragmenten. </p>
            </figure>
            <p>Unsere Experimente bestätigen die Aussagen von O’Keefe (O’Keefe et al. 2012),
                        dass 2Way ML-Verfahren bessere Ergebnisse in der Sprechererkennung liefern, als
                        korrespondierende NWay Verfahren. Analoges gilt für die Evaluation der CRFs, die
                        sogar beinahe den selben Wert für die Sprechererkennung liefern wie in (O’Keefe
                        et al. 2012). Sowohl auf dem Developmentkorpus, als auch auf dem Testkorpus
                        zeigen regelbasierte Ansätze deutliche Vorteile gegenüber den in dieser Arbeit
                        verwendeten ML-Verfahren. Es ist weiterhin ersichtlich, dass die Bestimmung des
                        Sprechers einfacher ist, als die Bestimmung des Angesprochenen. Wahrscheinlich
                        liegt das daran, dass im Fall der Sprecherzuschreibung mehr Information
                        vorliegt, nämlich die direkte Rede und der Kontext, während bei der Ermittlung
                        des Angesprochenen die direkte Rede selbst nur hilfreich ist, wenn ein
                        Angesprochener direkt darin vermerkt ist.</p>
            <p>Ein direkter Vergleich mit dem besten in der Literatur zu findenden Verfahren
                          (Almeida et al. 2014) kann direkt nicht durchgeführt werden. Berücksichtigt man
                          den Unterschied, der Verfahren von O’Keefe auf den Texten des WSJ und den
                          literarischen Texten, könnte eine Qualität von 90% Genauigkeit erreicht werden
                          und damit ein mit der state of the art vergleichbares, sogar möglicherweise
                          besseres Ergebnis. Im Gegensatz zu ihrem Verfahren ermitteln wir zudem auch noch
                          eine angesprochene Entität. </p>
         </div>
         <div type=""div1"">
            <head>Diskussion und Ausblick</head>
            <p> Die Ergebnisse zeigen, dass das regelbasierte Verfahren für diese Aufgabe
                            deutlich bessere Ergebnisse erzielen kann als alle ML-Verfahren, die in dieser
                            Arbeit getestet wurden. Es ist geplant, die hier erstellte Zuordnung in die
                            regelbasierte Koreferenzauflösung von (Krug et al. 2015) einzuarbeiten, um diese
                            damit zu verbessern. Weil unsere Hauptmotivation die Verbesserung der
                            Koreferenzresolution ist, diese aber im Ansatz von Almeida nicht wirksam
                            verbessert werden konnte, haben wir darauf verzichtet, deren komplexes
                            Lernverfahren nachzuvollziehen. Gerade die Ergebnisse, die in Tabelle 2 zu sehen
                            sind, zeigen, dass mögliche Dialogsequenzen genauer untersucht werden müssen, um
                            diese zuverlässig erkennen und auflösen zu können. Eine genaue Dialoganalyse
                            vereinfacht wiederum die Korefenzauflösung, so dass eine Extraktion von
                            Beziehungen zwischen Personen und Attributen zu Entitäten innerhalb der Romane
                            möglicher erscheint. </p>
         </div>
         <div type=""div1"">
            <head>
               <anchor id=""id_docs-internal-guid-1f47a959-6ca4-c170-f3ac-c04e907df805""/>Anhang
                            </head>
            <table rend=""frame"" id=""Tabelle3"">
               <row>
                  <cell>Featurebeschrei-bung (zwischen Kandidat und direkten Rede)</cell>
                  <cell>
                                  Verwendung für Sprecherzuord-nung
                                </cell>
                  <cell>
                                  Zuordnung des Angesprochenen
                                </cell>
               </row>
               <row>
                  <cell>1. Ist der Kandidat Subjekt</cell>
                  <cell>+</cell>
                  <cell>-</cell>
               </row>
               <row>
                  <cell>2. Das Verb in der Dependenzstruk-tur, auf das sich der Kandidat  bezieht</cell>
                  <cell>+</cell>
                  <cell>+</cell>
               </row>
               <row>
                  <cell>3. Das POS-Tag des Kandidaten</cell>
                  <cell>-</cell>
                  <cell>-</cell>
               </row>
               <row>
                  <cell>4. Ist der Kandidat ein Pronomen</cell>
                  <cell>-</cell>
                  <cell>-</cell>
               </row>
               <row>
                  <cell>5-6. Befindet sich der Kandidat im Akkusativ/Dativ</cell>
                  <cell>+/+</cell>
                  <cell>+/+</cell>
               </row>
               <row>
                  <cell>7. Kandidat befindet sich in einer direkten Rede</cell>
                  <cell>+</cell>
                  <cell>-</cell>
               </row>
               <row>
                  <cell>8. Kandidat erscheint in der aktuellen direkten Rede</cell>
                  <cell>-</cell>
                  <cell>-</cell>
               </row>
               <row>
                  <cell>9. Kandidat befindet sich im selben Satz wie die direkte Rede</cell>
                  <cell>+</cell>
                  <cell>-</cell>
               </row>
               <row>
                  <cell>10. Die Direkte Rede beginnt mit einem kleingeschriebe-nem Wort</cell>
                  <cell>+</cell>
                  <cell>-</cell>
               </row>
               <row>
                  <cell>11. Zwischen Kandidat und direkter Rede befindet sich ein Doppelpunkt</cell>
                  <cell>-</cell>
                  <cell>-</cell>
               </row>
               <row>
                  <cell>12-14. Distanz zw. Kandidat und direkter Rede in Sätze/Wörter/Entitäten</cell>
                  <cell>-/-/+</cell>
                  <cell>-/-/-</cell>
               </row>
               <row>
                  <cell>15-16. Wort an Position +1/-1</cell>
                  <cell>-/-</cell>
                  <cell>-/-</cell>
               </row>
               <row>
                  <cell>17-18. Wort an Position +1/-1 ist Satzzeichen</cell>
                  <cell>+/+</cell>
                  <cell>-/-</cell>
               </row>
               <row>
                  <cell>19-20. Wort an Position +1/-1 ist in direkter Rede</cell>
                  <cell>+/+</cell>
                  <cell>-/-</cell>
               </row>
               <row>
                  <cell>19-20. Kandidat ist Sprecher der direkten Rede an Position -1/-2</cell>
                  <cell>-/-</cell>
                  <cell>+/-</cell>
               </row>
               <row>
                  <cell>21-22. Kandidat ist Angesprochener der direkten Rede an Position -1/-2</cell>
                  <cell>-/-</cell>
                  <cell>-/-</cell>
               </row>
            </table>
            <p>
               <hi rend=""bold"">Tab. A1</hi>: Ein Überblick über die
                              in dieser Arbeit verwendeten Features. Durch + und - ist angegeben, ob
                              dieses Feature gewinnbringend eingesetzt werden konnte. Zur Wahl der
                              Features vgl. auch  (Elson / McKeown 2010) und (He et al. 2013). <lb/>
            </p>
            <table rend=""frame"" id=""Tabelle4"">
               <row>
                  <cell>Regelbezeichnung</cell>
                  <cell>Regelbeschreibung</cell>
               </row>
               <row>
                  <cell>(1) Explizite Sprechererkennung</cell>
                  <cell>Nutzt Pattern-Matching und grammatikalische Regeln um explizite Erwähnungen eines Sprechers im direkten Umfeld einer direkten Rede zu erkennen.</cell>
               </row>
               <row>
                  <cell>(2) Explizite Erkennung des  Angesprochenen</cell>
                  <cell>Nutzt Pattern-Matching und grammatikalische Regeln um explizite Erwähnungen eines Angesprochenen innerhalb der direkten Rede zu erkennen.</cell>
               </row>
               <row>
                  <cell>(3) Explizite Erkennung des Angesprochenen II</cell>
                  <cell>Wie (1) nur für den Angesprochenen</cell>
               </row>
               <row>
                  <cell>(4) Explizite Sprechererkennung II</cell>
                  <cell>Wie (1), nur der Kontext wird um 1 Satz außerhalb der direkten Rede erweitert.</cell>
               </row>
               <row>
                  <cell>(5) Vorwärtspropagierung</cell>
                  <cell>Zwei direkt aufeinanderfolgenden direkten Reden wird der Sprecher/Angesprochener der ersten direkten Rede zugeordnet, wenn beide direkte Reden innerhalb des selben Satzes liegen</cell>
               </row>
               <row>
                  <cell>(6) Rückwärtspropagierung</cell>
                  <cell>wie (5) nur mit entgegengesetzter Richtung der Ausführung</cell>
               </row>
               <row>
                  <cell>(7) Nachbarschafts-propagierung</cell>
                  <cell>Direkten Reden, die keinen eingeschobenen Kontext aufzeigen, wechseln den Sprecher/Angesprochenen ( falls vorhanden)</cell>
               </row>
               <row>
                  <cell>(8) Fragenpropagierung</cell>
                  <cell>Nach einer Frage wechseln Sprecher/Angesprochener</cell>
               </row>
               <row>
                  <cell>(9) Dialogpropagierung</cell>
                  <cell>Direkte Rede mit maximal einem zwischenliegenden Satz wechseln ihren Sprecher/Angesprochenen</cell>
               </row>
               <row>
                  <cell>(10) Default-Sprecher/ Angesprochener</cell>
                  <cell>Als Sprecher wird das letzte Subjekt außerhalb direkter Reden gesetzt, als Angesprochener das letzte Subjekt, das nicht Sprecher der aktuellen direkten Rede ist.</cell>
               </row>
            </table>
            <p>
               <hi rend=""italic"">
                  <hi rend=""bold"">Tab. A2</hi>: Überblick über die Regelpäse für das in dieser
                            Arbeit vorgestellte regelbasierte Verfahren zur Sprecherzuordnung bzw. Zuordnung
                            eines Angesprochenen. Optimale Reihenfolge der Ausführung der Regeln aufgrund
                            der Auswertung des Trainingssatzes:</hi>
            </p>
            <p>(1)→(2)→(3)→(4)→(5)→(6)→(7)→(5)→(6)→(8) →(9)→(5)→(6)→(7)→(10).</p>
            <figure>
               <graphic url=""040-1000000000000331000001BBE3C72D0D.png""/>
               <p>
                  <hi rend=""bold"">Abb. A3</hi>: Auszug aus Aston Louise “Lydia”: Beispiel für die
                                Erkennung von Sprecher und Angesprochenem in direkten Reden gemäß den Regeln in
                                Tabelle A2. Im ersten Durchlauf wird mit der Regel (1) die Sprecherin für die
                                direkten Reden <hi rend=""bold"">1</hi> und <hi rend=""bold"">5</hi> erkannt.
                                Anschließend erkennt Regel (7) in Rückwärtsrichtung jeweils abwechselnd
                                Sprecherin <hi rend=""bold"">4</hi> und <hi rend=""bold"">2</hi> und Angesprochene
                                in <hi rend=""bold"">3</hi>. Schließlich erkennt Regel (7) in Vorwärtsrichtung die
                                Sprecherin in <hi rend=""bold"">3</hi> und die Angesprochene in <hi rend=""bold"">4</hi> und <hi rend=""bold"">2</hi>. </p>
            </figure>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <anchor id=""id_docs-internal-guid-1f47a959-6ca4-35e7-7fb1-f0e048431e41""/>
                  <hi rend=""bold"">Almeida, Mariana S.C. / Almeida, Miguel B. / Martins, André
                                    F.T.</hi> (2014): ""A joint model for quotation attribution and
                                    coreference resolution"", in: <hi rend=""italic"">Proceedings of the 14th
                                    Conference of the European Chapter of the Association for Computational
                                    Linguistics, Gothenburg, Sweden</hi> 39-48. </bibl>
               <bibl>
                  <hi rend=""bold"">Bohnet, Bernd / Kuhn, Jonas</hi> (2012): ""The best of both
                                      worlds: a graph-based completion model for transition-based parsers."" In:
                                      <hi rend=""italic"">Proceedings of the 13th Conference of the European
                                        Chapter of the Association for Computational Linguistics</hi>. Avignon,
                                        France: 77-87. </bibl>
               <bibl>
                  <hi rend=""bold"">Chaganty, Arun / Muzny, Grace</hi> (2015): <hi rend=""italic"">Quote Attribution for Literary Text with Neural Networks</hi>
                  <ref target=""https://cs224d.stanford.edu/reports/ChagantyArun.pdf"">https://cs224d.stanford.edu/reports/ChagantyArun.pdf</ref> [letzter
                                            Zugriff 08. Februar 2016].</bibl>
               <bibl>
                  <hi rend=""bold"">Celikyilmaz, Asli / Hakkani-Tur, Dilek / He, Hua / Kondrak,
                                                Greg / Barbosa, Denilson</hi> (2010): ""The actortopic model for
                                                extracting social networks in literary narrative."", in: <hi rend=""italic"">Proceedings of the NIPS 2010 Workshop Machine Learning for Social
                                                Computing</hi>
                  <ref target=""https://webdocs.cs.ualberta.ca/~denilson/files/publications/nips2010.pdf"">https://webdocs.cs.ualberta.ca/~denilson/files/publications/nips2010.pdf</ref>
                                                  [letzter Zugriff 08. Februar 2016].</bibl>
               <bibl>
                  <hi rend=""bold"">Elson, David K. / Dames, Nicholas / McKeown, Kathleen
                                                      R.</hi> (2010a): ""Extracting social networks from literary fiction"", in:
                                                      <hi rend=""italic"">Proceedings of the 48th annual meeting of the
                                                        association for computational linguistics</hi>. Association for
                                                        Computational Linguistics <ref target=""http://www1.cs.columbia.edu/~delson/pubs/ACL2010-ElsonDamesMcKeown.pdf"">http://www1.cs.columbia.edu/~delson/pubs/ACL2010-ElsonDamesMcKeown.pdf</ref>
                                                        [letzter Zugriff 08. Februar 2016]. </bibl>
               <bibl>
                  <hi rend=""bold"">Elson, David K. / McKeown, Kathleen R.</hi> (2010b):
                                                          ""Automatic Attribution of Quoted Speech in Literary Narrative"", in: <hi rend=""italic"">Proceedings of AAAI</hi> 1013-1019.</bibl>
               <bibl>
                  <hi rend=""bold"">Glass, Kevin / Bangay, Shaun</hi> (2006): ""Hierarchical rule
                                                            generalisation for speaker identification in fiction books"", in: <hi rend=""italic"">Proceedings of the 2006 annual research conference of the
                                                            South African institute of computer scientists and information
                                                            technologists on IT research in developing countries</hi>. South African
                                                            Institute for Computer Scientists and Information Technologists:
                                                            31-40.</bibl>
               <bibl>
                  <hi rend=""bold"">Glass, Kevin / Bangay, Shaun</hi> (2007): ""A naive
                                                              salience-based method for speaker identification in fiction books"", in: <hi rend=""italic"">Proceedings of the 18th Annual Symposium of the Pattern
                                                              Recognition Association of South Africa (PRASA’07)</hi>
                  <ref target=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.494.3729&rep=rep1&type=pdf"">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.494.3729&rep=rep1&type=pdf</ref>
                                                                [letzter Zugriff 16. Februar 2016].</bibl>
               <bibl>
                  <hi rend=""bold"">He, Hua / Barbosa, Denilson / Kondrak, Grzegorz</hi> (2013):
                                                                  ""Identification of Speakers in Novels"", in: <hi rend=""italic"">Proceedings of
                                                                  the 51st Annual Meeting of the Association for Computational
                                                                  Linguistics</hi>. Sofia, Bulgaria: 1312-1320.</bibl>
               <bibl>
                  <hi rend=""bold"">Iosif, Elias / Mishra, Taniya</hi> (2014): ""From Speaker
                                                                    Identification to Affective Analysis: A Multi-Step System for Analyzing
                                                                    Children’s Stories"", in: <hi rend=""italic"">EACL</hi> 2014: 40-49. </bibl>
               <bibl>
                  <hi rend=""bold"">Jannidis, Fotis / Krug, Markus / Reger, Isabella / Toepfer,
                                                                        Martin / Weimer, Lukas / Puppe, Frank</hi> (2015): “Automatische
                                                                        Erkennung von Figuren in deutschsprachigen Romanen”, in: <hi rend=""italic"">Digital Humanities im deutschsprachigen Raum (Dhd 2015), Graz,
                                                                        Austria</hi>. </bibl>
               <bibl>
                  <hi rend=""bold"">Joachims, Thorsten</hi> (2002): <hi rend=""italic"">Learning
                                                                          to classify text using support vector machines</hi>. Methods, theory and
                                                                          algorithms (= The Springer International Series in Engineering and Computer
                                                                          Science 668). New York: Springer.</bibl>
               <bibl>
                  <hi rend=""bold"">Krug, Markus / Puppe, Frank / Jannidis, Fotis / Macharowsky,
                                                                              Luisa / Reger, Isabella / Weimer, Lukas</hi> (2015): ""Rule-based
                                                                              Coreference Resolution in German Historic Novels"", in: <hi rend=""italic"">Proceedings of the Fourth Workshop on Computational Linguistics for
                                                                              Literature</hi> 98-104. </bibl>
               <bibl>
                  <hi rend=""bold"">Lee, Heeyoung / Peirsman, Yves / Chang, Angel / Chambers,
                                                                                  Nathanael / Surdeanu, Mihai / Jurafsky, Dan</hi> (2011): ""Stanford's
                                                                                  multi-pass sieve coreference resolution system at the CoNLL-2011 shared
                                                                                  task"", in: <hi rend=""italic"">Proceedings of the Fifteenth Conference on
                                                                                  Computational Natural Language Learning: Shared Task</hi>. Association
                                                                                  for Computational Linguistics <ref target=""http://nlp.stanford.edu/pubs/conllst2011-coref.pdf"">http://nlp.stanford.edu/pubs/conllst2011-coref.pdf</ref> [letzter
                                                                                  Zugriff 08. Februar 2016]. </bibl>
               <bibl>
                  <hi rend=""bold"">McCallum, Andrew Kachites</hi> (2002): <hi rend=""italic"">MALLET: A Machine Learning for Language Toolkit</hi> <ref target=""http://mallet.cs.umass.edu"">http://mallet.cs.umass.edu</ref>
                                                                                    [letzter Zugriff 08. Februar 2016].</bibl>
               <bibl>
                  <hi rend=""bold"">Mikolov, Tomas / Sutskever, Ilya / Chen, Kai / Corrado, Greg
                                                                                        / Dean Jeffrey</hi> (2013): ""Distributed representations of words and
                                                                                        phrases and their compositionality"", in: <hi rend=""italic"">Advances in
                                                                                        neural information processing systems</hi> 26 <ref target=""http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</ref>
                                                                                        [letzter Zugriff 08. Februar 2016].</bibl>
               <bibl>
                  <hi rend=""bold"">O'Keefe, Tim / Pareti, Silvia / Curran, James R. /
                                                                                            Koprinska, Irena / Honnibal, Matthew</hi> (2012): ""A sequence labelling
                                                                                            approach to quote attribution"", in: <hi rend=""italic"">Proceedings of the
                                                                                            2012 Joint Conference on Empirical Methods in Natural Language
                                                                                            Processing and Computational Natural Language Learning</hi>. Association
                                                                                            for Computational Linguistics, 2012: 790–799. </bibl>
               <bibl>
                  <hi rend=""bold"">Rahman, Altaf / Ng, Vincent </hi> (2011): ""Narrowing the
                                                                                              modeling gap: a cluster-ranking approach to coreference resolution"", in: <hi rend=""italic"">Journal of Artificial Intelligence Research</hi> 40:
                                                                                              469-521.</bibl>
               <bibl>
                  <hi rend=""bold"">Ruppenhofer, Josef / Sporleder, Caroline / Shirokov,
                                                                                                  Fabian</hi> (2010): ""Speaker Attribution in Cabinet Protocols"", in: <hi rend=""italic"">The seventh international conference on Language Resources
                                                                                                  and Evaluation (LREC)</hi> 2510-2515. </bibl>
               <bibl>
                  <hi rend=""bold"">Schmid, Helmut</hi> (1999): ""Improvements in part-of-speech
                                                                                                    tagging with an application to German"", in: Armstrong, Susan / Church,
                                                                                                    Kenneth / Isabelle, Pierre / Manzi, Sandra / Tzoukermann, Evelyne /
                                                                                                    Yarowsky, David (eds.): <hi rend=""italic"">Natural language processing using
                                                                                                    very large corpora</hi> (= Text, Speech and Language Technology 11). New
                                                                                                    York: Springer 13-25.</bibl>
               <bibl>
                  <hi rend=""bold"">Schmid, Helmut / Laws, Florian</hi> (2008): ""Estimation of
                                                                                                      conditional probabilities with decision trees and an application to
                                                                                                      fine-grained POS tagging"", in: <hi rend=""italic"">Proceedings of the 22nd
                                                                                                      International Conference on Computational Linguistics (Coling 2008)</hi>
                                                                                                      777–784.</bibl>
               <bibl>
                  <hi rend=""bold"">Sutton, Charles / McCallum, Andrew</hi> (2006): ""An
                                                                                                        introduction to conditional random fields for relational learning"", in:
                                                                                                        Getoor, Lise / Taskar, Ben (eds.):<hi rend=""italic"">Introduction to
                                                                                                        statistical relational learning</hi>. Cambridge, MA / London: The MIT
                                                                                                        Press 93-128. </bibl>
               <bibl>
                  <hi rend=""bold"">Zhang, Jason Y. / Black Alan W. / Sproat, Richard</hi>
                                                                                                          (2003): ""Identifying speakers in children's stories for speech synthesis"",
                                                                                                          in: <hi rend=""italic"">EUROSPEECH</hi> 2041-2044. </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,nlp;quantitative textanalyse;sprechererkennung,German,annotieren;inhaltsanalyse;literatur;modellierung;personen;programmierung;strukturanalyse
10687,2018 - University of Cologne,University of Cologne,Kritik der digitalen vernunft,2018,DHd,DHd,,Cologne,,Germany,https://dhd2018.uni-koeln.de/,Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?,,Matthias Boenig;Maria Federbusch;Elisa Herrmann;Clemens Neudecker;Kay-Michael Würzner,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Einleitung</head>
            <p>Die Verwendung von Referenzdaten für das Training und die Auswertung statistischer Annotations- und Analyseverfahren ist ein Kernmerkmal empirischer Forschung, zu der auch die Digital Humanities zählen möchten.<ref target=""ftn1"" n=""1""/>
                   Die wichtigste Grundlage für den erfolgreichen Einsatz statistischer Verfahren liegt in der Verwendung geeigneter, den Algorithmen zugrunde liegender Modelle. Für deren Erstellung ist neben einem passenden Lernverfahren das Vorhandensein von Trainingsdaten eine wesentliche Voraussetzung. Werden Forschungsdaten, die mit quantitativen Methoden entstanden sind, mit Referenzdaten verifiziert und interpretiert, ist ein kritischer Blick auf Auswahl, Erstellung und Umgang mit selbigen ein häufig vernachlässigter Bereich.
                </p>
            <p>Vor diesem Hintergrund richtet der vorliegende Beitrag einen kritischen Blick auf die Rolle von und den Umgang mit Ground-Truth-Daten im Bereich der Digital Humanities. Drei Beobachtungen sollen zur Diskussion gestellt werden:</p>
            <list type=""ordered"">
               <item>Es fehlt den Digital Humanities an einheitlichen und etablierten Ground-Truth-Datensets für die Evaluierung von Forschungsergebnissen auf Basis quantitativer Methoden.</item>
               <item>Es fehlt den Digital Humanities an Richtlinien zur Erstellung und Verfahren zur Verifizierung von Ground Truth.</item>
               <item>Es fehlt den Digital Humanities an akzeptierten und operationalisierbaren Metriken zur Qualitätsbestimmung von Ground Truth und abgeleiteten Datenanalysen.</item>
            </list>
            <p>Anhand der automatischen Texterfassung, die als Analogie für ein allgemeines Modell eines empirischen Forschungsprozesses<ref target=""ftn2"" n=""2""/> gesetzt wird, sollen im Folgenden die Probleme diskutiert und Möglichkeiten gezeigt werden, wie diesen Defiziten begegnet werden kann.
                </p>
            <figure>
               <graphic n=""1001"" width=""16.002cm"" height=""3.314347222222222cm"" rend=""inline"" url=""BOENIG_Matthias_Ground_Truth__Grundwahrheit_oder_Ad_Hoc_L_su-image1.png""/>
               <head>Abbildung : Gegenüberstellung eines Modells eines Forschungsprozesses aus der empirischen Sozialforschung<ref target=""ftn3"" n=""3""/> und dessen Anwendung auf den Prozess der automatischen Texterfassung
                    </head>
            </figure>
            <p>Unter Ground Truth wird in diesem Kontext die Dokumentation ausgewählter Merkmale (Zeichen, Zeilen, Absätze, Spalten, Abbildungen, usw.) des Textes in Form einer digitalen Transkription verstanden. Dabei ist je nach Anwendung zwischen allgemeineren Referenz- und spezifischeren Trainingsdaten zu unterscheiden.</p>
            <p>Die Volltext-Digitalisierung von Archiv- und Bibliotheksbeständen, größeren Dokumentsammlungen oder Korpora wird heute von unterschiedlichen Seiten verfolgt: So werden beispielsweise seit 2005 massenhaft Bibliotheksbestände von Google im Rahmen von öffentlich-privaten Partnerschaften sowohl als Bild als auch als Text digitalisiert. Daneben unterstützen Stiftungen, Fördereinrichtungen wie die DFG sowie die Haushaltsmittel der Institutionen die Digitalisierungen im Rahmen spezifischer Projekte. Im Ergebnis dieser Digitalisierungsbemühungen stehen Volltextsammlungen höchst unterschiedlicher Qualität, Vollständigkeit, Interoperabilität und Nachnutzbarkeit.</p>
            <p>Mit der OCR-D-Initiative wird erstmals versucht, die technischen und organisatorischen Grundlagen dafür zu schaffen, einen breiten heterogenen Bestand von Drucken aus dem 16. – 18. Jahrhundert vollständig und einheitlich in elektronischen Volltext umzuwandeln und frei zur Verfügung zu stellen. Unter Einbeziehung einzelner Modulprojekte wird vom DFG-geförderten Koordinierungsprojekt<ref target=""ftn4"" n=""4""/> die Transformation der Drucke in strukturierten Volltext konzeptuell und prototypisch vorbereitet. Dazu werden im Rahmen von OCR-D Anwendungen, Adaptionen und Weiterentwicklungen von Verfahren der Optical Character Recognition (OCR) für historische Drucke geprüft bzw. implementiert und in einer finalen, prototypischen Produktionsumgebung kombiniert. Eine zentrale Aufgabe von OCR-D besteht dabei in der Bereitstellung eines umfassenden Ground-Truth-Korpus, das sowohl Referenz- als auch Trainingsdaten sowie Richtlinien zur Transkription von Texten für deren Verwendung als Ground Truth umfasst. Damit können sowohl Texte bezüglich ihrer Zeichengenauigkeit transparent geprüft als auch spezielle statistische Modelle für die Text- und Strukturerkennung trainiert werden.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Diskussion</head>
            <p>1. Gegenstand der Digital Humanities ist das digitale Objekt, beispielsweise digitaler Text.<ref target=""ftn5"" n=""5""/> Vergleicht man den Bereich der automatischen Texterfassung mit einem in der empirischen Sozialforschung etablierten Modell des Forschungsprozesses, wird deutlich, dass die Referenzdaten in beiden Prozessen, im Besonderen in der Phase der Evaluation nach Abschluss der Datenanalyse, eine bedeutende Rolle spielen. In dieser Phase wird auf entsprechende Referenzdaten oder -systematiken zurückgegriffen, die in der Phase der Theoriebildung identifiziert und angesammelt wurden. Anzumerken ist, dass der Forschungsprozess nicht isoliert zu betrachten ist, da er schon zu Beginn bei Formulierung und Auswahl des Forschungsproblems auf vorhandene Forschungsdaten zurückgreift. Der traditionelle Forschungsprozess hat durch ein System von Referenzendaten (u. a. Wörterbücher, Editionen, Nachschlage- und Quellenwerke) ein System der 
                    <hi rend=""italic"">referenzbasierten</hi> Evaluation geschaffen. Dieses System wird gestützt durch Konventionen der Zitierung, Dokumentation, Verzeichnung und Aufbewahrung in Institutionen sowie spezifischen Publikationsformen. Der Forschungsprozess in den Digital Humanities kann auf diesen Hintergrund nur teilweise bzw. gar nicht zurückgreifen, da bisher zu wenige digitale Daten vorliegen.
                </p>
            <p>Auf der anderen Seite haben die Digital Humanities bezüglich der Verfügbarmachung von Quellcode und Forschungsdaten einen großen Vorsprung gegenüber anderen datenbasiert arbeitenden Disziplinen (etwa der kognitiven Psychologie). 
                    <hi rend=""italic"">Reproducible Science</hi> wird sowohl gefördert als auch propagiert. Problematisch sind aber die fehlende Vereinheitlichung und Transparenz bei der Datenerhebung sowie der Einsatz mangelhaft erfasster Daten bzw. deren ad-hoc Surrogaten bedingt durch die mangelnde Verfügbarkeit an (insbesondere annotierten) Forschungsdaten.
                </p>
            <p>Die Arbeitsweise mit Referenzdaten, wie sie in den Naturwissenschaften und Teilen der Geisteswissenschaft Anwendung findet, möchte neben der Evaluation, Verifikation gerade die Vergleichbarkeit der Forschungsergebnisse stützen. Das setzt voraus, dass diese Daten in ihrer Qualität diesen Ansprüchen genügen müssen und durch entsprechende Normungen Interpretationsspielräume definiert sind.</p>
            <p>Trotz des wissenschaftlichen Anspruches der Digital Humanities auf Objektivität und dem Bemühen Referenzdaten zu schaffen, werden unterschiedliche Interpretationen, die auf Grundlage von mangelhaften Referenz- und Trainingsdaten entstehen, möglich. Solch ein „hinzunehmendes Übel“ wird erkannt und mit „Pragmatismus“, mit der „Flexibilität“ oder „Austauschbarkeit von Konzepten im Konkreten der Texte“ gerechtfertigt<ref target=""ftn6"" n=""6""/>, eine Vergleichbarkeit der so entstandenen Texte bzw. Forschungsergebnisse damit aber wesentlich behindert. Um eine Vergleichbarkeit im Sinne von 
                    <hi rend=""italic"">Reproducible Science</hi> zu erreichen, ist somit der Gegenstand der Digital Humanities um die Fragen der Objekterstellung zu erweitern.
                </p>
            <p>2. Mit dem Begriff OCR wird üblicherweise der Gesamtprozess der automatischen Texterfassung bestehend aus den Teilaufgaben Bildvorverarbeitung, Struktur- und Texterkennung sowie gegebenenfalls (automatisierter) Nachkorrektur bezeichnet. Damit eine OCR vorgenommen werden kann, sind entsprechende Erkennungsmodelle notwendig. Diese werden durch sogenanntes Training unter Nutzung von Ground Truth induziert. Generische Modelle werden vom Anbieter der OCR-Software zur Verfügung gestellt, domänenspezifische Modelle müssen trainiert werden. Das Training dient immer der Verbesserung der Endergebnisse des Erkennungsprozesses. Somit sollten bei einem universellen Anspruch Ground-Truth-Daten sowohl spezifische als auch allgemeine Normungen enthalten, damit sowohl generische als auch spezifische Modelle trainiert werden können. Ziel ist es, die unterschiedlichen Bedürfnisse und Schwerpunkte in den Digital Humanities zu bedienen.</p>
            <p>3. Um diesem sehr breiten Anspruch gerecht zu werden, reicht eine Akklamation, dass diese oder jene Sammlung von Daten als Ground Truth bezeichnet und genutzt werden kann, nicht aus. Es bedarf hingegen eines Ground-Truth-Konzepts. Dieses Konzept dokumentiert sowohl dessen inneren Aufbau als auch dessen Erstellung. Damit können im Bereich der Texterfassung beispielsweise folgende Punkte im Umgang mit Ground Truth erreicht werden:</p>
            <list type=""ordered"">
               <item>(weitere) Reduktion bzw. Standardisierung der Freiheitsgrade bei der Transkription (z. B. langes s, Ligaturen, Zeilenumbruch)</item>
               <item>weitergehende Operationalisierung der Überprüfbarkeit der Validität
                        <lb/>der Transkription
                    </item>
               <item>Ergänzung von (weiteren) Anweisungen zum Umgang mit koordinatenbasierten Phänomenen</item>
            </list>
            <p>Illustriert werden kann ein solches Konzept am Beispiel der 
                    <hi rend=""italic"">OCR-D Ground-Truth-Guidelines</hi>. Um die Freiheitsgrade von Interpretationen (Punkt A) zu normieren werden drei Level von Erfassungsgraden angeboten. Die einzelnen Level sollen nachvollziehbare Interpretationsentscheidungen sowohl festlegen als auch dokumentieren und damit die Möglichkeit der maschinellen Überprüfbarkeit eröffnen.
                </p>
            <p>Tabelle : Beispiel der Anwendung der Level bei der Ligatur ct.</p>
            <table rend=""rules"">
               <row>
                  <cell rend=""normal"">Zeichen</cell>
                  <cell rend=""normal"">Level 1</cell>
                  <cell rend=""normal"">Level 2</cell>
                  <cell rend=""normal"">Level 3</cell>
               </row>
               <row>
                  <cell rend=""normal"">
                     <figure>
                        <graphic n=""in_table"" width=""2.06375cm"" height=""2.4341666666666666cm"" rend=""inline"" url=""BOENIG_Matthias_Ground_Truth__Grundwahrheit_oder_Ad_Hoc_L_su-image2.jpeg""/>
                     </figure>
                  </cell>
                  <cell rend=""normal"">ct
                            <lb/>Die Ligatur wird in zwei einzelne Zeichen aufgespalten.
                        </cell>
                  <cell rend=""normal"">ct
                            <lb/>Die Ligatur wird aufgespalten und mit einer zusätzlichen Annotation, dass es sich um eine Ligatur handelt, im PAGE-Format versehen.
                            <lb/>textStyle{offset:0; length:2;ligatur:true;}
                        </cell>
                  <cell rend=""normal"">&#xEEC5;
                            <lb/>Die Ligatur wird als 
                            <hi rend=""bold"">ein</hi> Zeichen interpretiert und mit dem entsprechenden Unicode-Zeichen wiedergegeben.
                        </cell>
               </row>
            </table>
            <p>Damit werden Anweisung und Richtlinien weitgehend operationalisiert und eine computergestützte Validierung umsetzbar (Punkt B). Das entsprechende Ground-Truth-Korpus liegt im XML-basierten PAGE-Format<ref target=""ftn7"" n=""7""/> vor. 
                    Das PAGE-Format hat sich im Rahmen des EU-Projekts IMPACT<ref target=""ftn8"" n=""8""/> sowie durch seine Verbreitung im Rahmen von Wettbewerben bei wissenschaftlichen Konferenzen (z.B. ICDAR, ICFHR, DAS) als de-facto Standard für XML-basierter Ground Truth etabliert. 
                    Mit Hilfe von Schematron<ref target=""ftn9"" n=""9""/>-Regeln kann nun, wie das Beispiel zeigt, geprüft werden, nach welchem Level die „ct“-Ligatur kodiert ist:
                </p>
            <figure>
               <graphic n=""1003"" width=""16.002cm"" height=""5.695597222222222cm"" rend=""inline"" url=""BOENIG_Matthias_Ground_Truth__Grundwahrheit_oder_Ad_Hoc_L_su-image3.jpeg""/>
            </figure>
            <p>Die Wahl des PAGE-Formates ermöglicht im Unterschied zum TEI-basierten Basisformat des DTA (DTABf)<ref target=""ftn10"" n=""10""/> eine unkomplizierte Lösung für die Repräsentation von koordinatenbasierten Phänomenen (Punkt C).
                </p>
            <figure>
               <graphic n=""1004"" width=""16.002cm"" height=""4.4943888888888885cm"" rend=""inline"" url=""BOENIG_Matthias_Ground_Truth__Grundwahrheit_oder_Ad_Hoc_L_su-image4.jpeg""/>
            </figure>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Fazit</head>
            <p>Ground Truth stellt im Texterkennungsprozess und im dazu betrachteten Forschungsprozess in den Digital Humanities eine entscheidende Rolle dar. Ergebnis dieser beiden Prozesse sind immer Publikationen, die als Forschungsdaten in neuen Forschungszusammenhängen nachgenutzt werden. Gelingt es den Digital Humanities nicht, ein Referenzsystem mit Konventionen zur Prüfung, Dokumentation, Verzeichnung und Aufbewahrung ihrer Daten in Institutionen sowie spezifischen Publikationsformen aufzubauen, ist die Vergleichbarkeit der Forschungsergebnisse nicht immer gegeben. Dabei bietet der Aufbau so genannter Forschungsdatenrepositorien erste positive Entwicklungen in Richtung dokumentierter Forschungsprozesse.<ref target=""ftn11"" n=""11""/>
            </p>
            <p>Der Gegenstand der Digital Humanities ist nicht nur auf das digitale Objekt zu beschränken, sondern auch auf dessen Erstellung zu erweitern. Die Erstellung ist ein Prozess, der von den Digital Humanities zu dokumentieren ist, da nur so eine Referenzierbarkeit und Reproduzierbarkeit der Forschungsergebnisse möglich wird. Der kritische Umgang mit diesen Daten stellt eine Aufgabe der Wissenschaft dar. </p>
            <p>Daher täten die Digital Humanities gut daran, in einen aktiven Dialog mit digitalisierenden Einrichtungen und Förderern einzutreten um gemeinsame Standards und Richtlinien zur Dokumentation zu etablieren, die transparent Auskunft über die Provenienz eines digitalen Objekts geben sowie klar über Möglichkeiten sowie Einschränkungen zu dessen Nachnutzbarkeit informieren. Entscheidungen, Kriterien, Daten und Ergebnisse sollten, im Unterschied zur bisherigen, in diesem Beitrag kritisierten Praxis, zukünftig möglichst transparent, operationalisierbar sowie validierbar digital zur Verfügung gestellt werden. </p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" rend=""footnote text""> Vgl. dazu These 1.1: „Die Digital Humanities bereichern die traditionellen Geisteswissenschaften konzeptionell und methodisch - ihre Werkzeuge und Verfahren ergänzen das „Wie“ unserer Praxis um 
                            <hi rend=""italic"">eine empirisch ausgerichtete Epistemologie.</hi>“ [Thesenpapier des Fachverbands „Digital Humanities im deutschsprachigen Raum“ 2014]
                        </note>
            <note id=""ftn2"" n=""2"" rend=""footnote text""> siehe [Schnell, Hill, Esser 2011: 4]</note>
            <note id=""ftn3"" n=""3"" rend=""footnote text""> siehe Fußnote 2</note>
            <note id=""ftn4"" n=""4"" rend=""footnote text""> OCR-D: Koordinierungsprojekt zur Weiterentwicklung von Verfahren der Optical Character Recognition (OCR), http://www.ocr-d.de/.</note>
            <note id=""ftn5"" n=""5"" rend=""footnote text""> „Es werden weitere korpusbasierte Analysen angestrebt. Diese erfordern aber eine Voraussetzung: Im Zuge der ‚digitale[n] Wende‘ [Schöch 2014: 130] ist es weiterhin wünschenswert und erforderlich, dass immer mehr literarische Texte digital zur Verfügung stehen oder diese durch leichte und praktikable Verfahren der Texterkennung (OCR, optical character recognition) digitalisierbar gemacht werden können.“ [Mihm 2016: 200]</note>
            <note id=""ftn6"" n=""6"" rend=""footnote text""> „Dass die von uns einbezogenen Online-Repositorien hinsichtlich der editionsphilologischen Textqualität variieren ist ein hinzunehmendes Übel, dem wir zum einen pragmatisch (Wahl der bestmöglichen verfügbaren Ausgabe; Ziel, die Fehlermarge unter 2% zu halten), zum anderen unter Hinweis auf die flexible Struktur des Korpus (Austausch durch eine qualitativ hochwertigere Version ist möglich) begegnen. Durch die nahtlose Dokumentation des Korpus wird zudem die nötige Transparenz gewährleistet um auch Nachnutzern flexible Kontrolle der Daten zu ermöglichen.“ [Herrmann-Wolf, Lauer 2016: 159]</note>
            <note id=""ftn7"" n=""7"" rend=""footnote text""> Page Analysis and Ground Truth Elements, siehe http://www.primaresearch.org/publications/ICPR2010_Pletschacher_PAGE und https://github.com/PRImA-Research-Lab/PAGE-XML.</note>
            <note id=""ftn8"" n=""8"" rend=""footnote text""> Vgl. https://www.digitisation.eu/tools-resources/image-and-ground-truth-resources/.</note>
            <note id=""ftn9"" n=""9"" rend=""footnote text""> ISO/IEC-Standard 19757-3:2006</note>
            <note id=""ftn10"" n=""10"" rend=""footnote text""> Deutsches Textarchiv, DTA-Basisformat, http://www.deutschestextarchiv.de/doku/basisformat/.</note>
            <note id=""ftn11"" n=""11"" rend=""footnote text""> Vgl. z.B. https://rdmorganiser.github.io/</note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Herrmann-Wolf, J. Berenike / Lauer, Gerhard</hi> (2016): „Aufbau und Annotation des Kafka/Referenzkorpus“ in: Burr Elisabeth (ed): 
                        <hi rend=""italic"">DHd 2016 : Modellierung, Vernetzung, Visualisierung : die Digital Humanities als fächerübergreifendes Forschungsparadigma</hi> : Konferenzabstracts : Universität Leipzig 7. bis 12. März 2016. [Duisburg]: nisaba 158-160.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Mihm, Melanie</hi> (2016): „Weibliches Erzählen im Expressionismus? Eine Stilometrie von Mela Hartwigs Prosa“ in: Burr Elisabeth (ed): 
                        <hi rend=""italic"">DHd 2016 : Modellierung, Vernetzung, Visualisierung : die Digital Humanities als fächerübergreifendes Forschungsparadigma</hi> : Konferenzabstracts : Universität Leipzig 7. bis 12. März 2016. [Duisburg]: nisaba 198-200.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Schöch, Christof</hi> (2014): „Corneille, Molière et les autres. Stilometrische Analysen zu Autorschaft und Gattungszugehörigkeit im französischen Theater der Klassik“, in: Schöch, Christof / Schneider, Lars (eds.): 
                        <hi rend=""italic"">Literaturwissenschaft im digitalen Medienwandel</hi> (=Philologie im Netz Beiheft 7) 130-157 http://web.fu-berlin.de/phin/beiheft7/b7t08.pdf [letzter Zugriff 25.September 2017].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Schnell, Rainer / Hill, Paul B. / Esser, Elke</hi> (2011): Methoden der empirischen Sozialforschung. 9., aktualisierte Aufl. München: Oldenbourg
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Thesenpapier des Fachverbands „Digital Humanities im deutschsprachigen Raum“ (DHd)</hi> (2014): „Digital Humanities 2020“, vorgestellt im März 2014 auf der ersten Jahrestagung des Verbandes in Passau. http://dig-hum.de/digital-humanities-2020.
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,ground truth;ocr;referenzdaten;standardisierung,German,bearbeitung;datenerkennung;transkription
10699,2018 - University of Cologne,University of Cologne,Kritik der digitalen vernunft,2018,DHd,DHd,,Cologne,,Germany,https://dhd2018.uni-koeln.de/,Universal Morphology zwischen Sprachtechnologie und Sprachwissenschaft: Sprachressourcen für Kaukasussprachen,,Christian Chiarcos;Kathrin Donandt;Maxim Ionov;Monika Rind-Pawlowski;Hasmik Sargsian;Jesse Wichers Schreur,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Hintergrund</head>
            <p>Das Projekt `Linked Open Dictionaries' (LiODi, 2015-2020) ist eine vom Bundesministerium für Bildung und Forschung (BMBF) finanzierte eHumanities-Forschungsgruppe, die daran arbeitet, einen nutzerorientierten Zugang zu LOD-Technologien in den Sprachwissenschaften zu entwickeln und diesen in Einzelstudien zum Sprachkontakt im Kaukasus zu demonstrieren. Ein wichtiges Element dafür sind Lehnwortuntersuchungen, und in morphologisch reichen Sprachen ist es möglich, dass eine flektierte Form Gegenstand der Entlehnung war. Diese automatisch gestützt generieren und identifizieren zu können, erfordert einen morphologischen Generator, der auch über unvollständige Daten hinweg generalisieren, und beispielsweise Paradigmen komplettieren kann. Hierfür stellt die Universal Morphology derzeit Standardressourcen bereit, auf die hin Softwareimplementierungen optimiert werden, etwa im Rahmen aktueller SIGMORPHON Shared Tasks (Cotterell et al., 2016).</p>
            <p>
               <hi rend=""bold"">Universal Morphology</hi> (<hi rend=""italic"">Unimorph, </hi>
               <ref target=""http://unimorph.github.io/"">http://unimorph.github.io/</ref>) ist ein aktuelles Communityproject zur Erfassung und automatischen Generierung der Flexionsmorphologie unterschiedlichster Sprachen. Ziel ist sowohl die Entwicklung von Vollformenwörterbüchern, deren Einsatz zur Annotation, weshalb Unimorph auf Kompatibilität mit den Universal Dependencies (
                    <ref target=""http://universaldependencies.org/"">http://universaldependencies.org/</ref>) angelegt ist, aber auch der Aufbau einer Referenzressource zur Entwicklung morphologischer Analyse- und Generierungskomponenten innerhalb der Sprach
                    <hi rend=""italic"">technologie</hi>. Die sprach
                    <hi rend=""italic"">wissenschaftliche </hi>Nutzung jedoch steht bislang aus und bestimmt daher den Fokus unseres Beitrages. Unimorph-Ressourcen und -Technologien sind dabei eine höchst willkommene Ergänzung unserer Arbeit, ihre praktische Anwendung des Schemas auf das Kaukasusgebiet erweist sich jedoch als problembehaftet.
                </p>
            <p>Der 
                    <hi rend=""bold"">Kaukasus</hi> ist für die Diversität seiner Sprachen bekannt, die oftmals eurozentristische Ansichten traditioneller Sprachwissenschaften infrage gestellt haben, und sich daher sehr gut zur Prüfung von linguistischen Modellen mit universellem Anspruch eignen. Viele Kaukasussprachen sind bedroht, die meisten (mit Ausnahme von Georgisch, Armenisch und Albanisch/Udi) wurden erst in jüngerer Vergangenheit verschriftlicht. Allen gemeinsam ist ein großer Lehnwortschatz (u.a. aus dem Iranischen, Türkischen, Russischen) und Lehnbeziehungen untereinander. Alle Kaukasussprachen sind sprachtechnologisch schlecht erschlossen, hier betrachten wir daher aktuelle Ansätze zur Schaffung von sprachübergreifenden (`universellen‘) morphologischen Annotationen im Rahmen des 
                    <hi rend=""italic"">Unimorph</hi>-Projektes. Wir berichten Ergebnisse zu unserer Arbeit zu Batsbi (nakh-dagestanisch), Mingrelisch (kartvelisch), Khinalug (nakh-dagestanisch) und Armenisch (indoeuropäisch). Auf dieser Basis diskutieren wir behutsame Erweiterungen von Unimorph, um dessen Anwendbarkeit für die kaukasischen Sprachen im Besonderen und für Sprachdokumentationsdaten im Allgemeinen zu ermöglichen.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Unimorph für Sprachdokumentation im Kaukasus?</head>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Schema und Schemaerweiterungen</head>
               <p>Unimorph verwendet ein 
                        <hi rend=""bold"">TSV-Format</hi>, d.h., eine Liste von tab-separierten Einträgen für jeweils Wortform, Lemma und Unimorph-Tags. Letztere sind 
                        <hi rend=""italic"">nicht qualifizierte</hi> Merkmale, durch Semikolon getrennt und 
                        <hi rend=""italic"">unsortiert</hi>. Der Eintrag für deutsch „(ich) treffe (dich)“ wäre beispielsweise
                    </p>
               <p>
                  <figure>
                     <graphic n="""" width="""" height="""" rend=""inline"" url=""CHIARCOS_Christian_Universal_Morphology_zwischen_Sprachtechn-gloss1.gif""/>
                  </figure>
               </p>
               <p>Der Eintrag für mingrelisch 
                        <hi rend=""italic"">kešerxvaduk</hi> (`Ich werde dich treffen') besitzt folgende Glosse:
                    </p>
               <p>
                  <figure>
                     <graphic n="""" width="""" height="""" rend=""inline"" url=""CHIARCOS_Christian_Universal_Morphology_zwischen_Sprachtechn-gloss2.gif""/>
                  </figure>
               </p>
               <p>In Unimorph wird diese Analyse wie folgt repräsentiert:</p>
               <p>
                  <figure>
                     <graphic n="""" width="""" height="""" rend=""inline"" url=""CHIARCOS_Christian_Universal_Morphology_zwischen_Sprachtechn-gloss3.gif""/>
                  </figure>
               </p>
               <p>Das mingrelische Verb kongruiert mit beiden syntaktischen Argumenten: dem Subjekt (1S, Nominativ) und dem Objekt (2S, Dativ), für die 
                        <hi rend=""bold"">zusammengesetzte Merkmale</hi> gebildet werden, die ein Argument mit dessen Person, Numerus usw. zusammenstellt; z.B. werden hier ArgNo1S für „Nominativargument=1. Person Singular“ und ArgDa2S für „Dativargument=2. Person Singular“ aufgeführt. Ein methodisches Problem ist, dass das Unimorph-Schema dieselbe Information hierbei in unterschiedlicher Weise ausdrückt, wie im Vergleich deutlich wird: mingrelisch ArgNo1S entspricht deutsch 1;SG. Da der Zusammenhang zwischen Kasus und grammatischen Rollen für das Deutsche nicht explizit definiert ist, gibt es keine Möglichkeit, diese automatisch als äquivalent zu interpretieren. Leider erlaubt es Unimorph zudem nicht, herkömmliche Terminologie zu verwenden, in der beide Argumente bzgl. ihrer syntaktischen Rollen (`Subjekt' und `Objekt') beschrieben werden, sondern zieht stattdessen die Kasusmorphologie heran. Dies ist insofern problematisch, als der Kasus im Verb nicht morphologisch realisiert ist, und Argumente im Satz nicht (pro)nominal realisiert werden müssen. Konventionell werden statt dessen grammatische Rollen verwendet.
                    </p>
               <p>Ähnlich zu (mehreren Argumenten von) Verben gibt es Nomina mit 
                        <hi rend=""bold"">mehrfacher Kodierung derselben Merkmalskategorie</hi> (v.a. Kasus), die mit Unimorph nicht behandelt werden können. In der sog. 
                        <hi rend=""italic"">Suffixaufnahme</hi> spezifizieren adnominale Elemente 
                        <hi rend=""italic"">neben</hi> ihrem inhärenten Kasus auch Agreement-Merkmale ihres Kopfnomens, z.B. durch Wiederholung von dessen Kasusmorphologie. Dies wurde ursprünglich für Georgisch beschrieben, gilt aber als verbreitet im Kaukasus. Bedauerlicherweise kann diese Information in Unimorph nicht positional kodiert werden, sondern erfordert eine Erweiterung des Label-Inventars. Deshalb schlagen wir die Einführng numerischer Indizes in der nominalen Morphologie vor, wobei der inhärente Kasus nicht bezeichnet wird, der Kasus des direkten Kopfes durch Anhängen von -1 an das Feature-Label, der Kasus von dessen Kopf durch -2, usw.
                    </p>
               <p>Diese 
                        <hi rend=""bold"">numerischen Indizes</hi> sind auch auf die verbale Domäne übertragbar. Gegeben etablierte Hierarchien grammatischer Rollen bzw. der zugeordneten Kasus, kann die bisherige Verbundmarkierung multipler Argumente durch eine Indizierungsstrategie ersetzt werden, die sich auf diese bezieht, und bei der das höchstrangige Element (z.B. das Subjekt) unbezeichnet bleibt, während andere Argumente nach ihrer Stellung im Ranking gekennzeichnet werden. Eine alternative Repräsentation des mingrelischen 
                        <hi rend=""italic"">kešerxvaduk</hi> wäre also</p>
               <p>
                  <figure>
                     <graphic n="""" width=""10cm"" height="""" rend=""inline"" url=""CHIARCOS_Christian_Universal_Morphology_zwischen_Sprachtechn-gloss4.gif""/>
                  </figure>
               </p>
               <p>Dies korrigiert auch die Asymmetrie zwischen zusammengesetzten und individuellen Merkmalen. Auch die Zuschreibung mehrerer Merkmale einer Kategorie kann nominal und verbal einheitlich gehandhabt werden, und die Vergleichbarkeit über Sprachen hinweg wird vereinfacht. </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Datenformat und Alternativen</head>
               <p>Ein zweites Problem ist das in Unimorph verwendete, nicht erweiterbare TSV-
                        <hi rend=""bold"">Format</hi>, das gegenüber den in der Sprachdokumentation üblichen Softwarelösungen (FLEx, Toolbox, ELAN) aber nur 
                        <hi rend=""italic"">stark eingeschränkte </hi>Informationen bereitstellt: Im Vergleich zur wörterbuchgestützten Interlinearglossierung stellen Morphem-Inventorien in unvollständigen und weniger gut interpretierbaren Repräsentationen ein Akzeptanzproblem dar. Daher sollte Unimorph nicht als eigenständiger Formalismus verstanden werden, sondern als Austauschformat zwischen reichen und hochwertigen Sprachressourcen auf der einen Seite und morphologischen Generatoren auf der anderen. 
                    </p>
               <p>Allerdings sind 
                        <hi rend=""italic"">Format und Speicherort</hi> festgeschrieben, so dass zugrundeliegende Ressourcen an anderen Orten gespeichert und gepflegt werden müssen, und Ergänzungen aus der Sprachdokumentationsarbeit womöglich nicht eingepflegt werden. Wir schlagen daher vor, das jetzige Format nur bei Bedarf zu generieren. Der Schlüssel hierzu liegt darin, die Quellformate gemäß W3C-Standards zur Ressourcentransformation auf einheitliche RDF-Datenstrukturen nach lemon (
                        <ref target=""https://www.w3.org/2016/05/ontolex/"">https://www.w3.org/2016/05/ontolex/</ref>) zu mappen und mit Hilfe der Anfragesprache SPARQL das derzeitige Tabellen-Format zu erzeugen. Das Repository enthält dann für jede Sprache die (a) 
                        <hi rend=""italic"">vollständigen</hi> Daten, und (b) ein standardisiertes Mapping auf lemon. Die TSV-Generierung ist nicht ressourcenspezifisch. Der Gebrauch von RDF-Technologien für Datenkonversion und Abfrage kann so die Entwicklung einer technischen Infrastruktur für Unimorph ermöglichen, die es erlaubt, über die Grenzen des TSV-Formats hinauszuwachsen, wovon SprachwissenschaftlerInnen, ForscherInnen und NLP-IngenieurInnen, die mit 
                        <hi rend=""italic"">low-resource</hi>-Sprachen arbeiten, profitieren könnten.
                    </p>
               <p>Die Integration mit gängigen Annotationswerkzeugen kann hierbei auf von uns entwickelten RDF-Konvertern für FLEx, Toolbox und weitere Formate aufsetzen (Chiarcos et al., 2017, 
                        <ref target=""https://github.com/acoli-repo/LLODifier"">https://github.com/acoli-repo/LLODifier</ref>). 
                    </p>
               <p>
                  <hi rend=""bold"">Zusammenfassend</hi> plädieren wir für die Einführung numerischer Indizes für verschiedene Argumente polyvalenter Verben und rekursive Merkmale in der Nominalflexion in Unimorph. Für die bessere Integration von existierenden Ressource aus der Sprachdokumentation insgesamt schlagen wir zudem eine Erweiterung der unterstützten Formate und einen einheitlichen Zugriff auf diese auf Basis von RDF-Technologien vor, so dass das jetzige Unimorph-Format nicht mehr von den zugrundeliegenden, reicheren Quelldaten separiert wird, sondern bedarfsabhängig daraus generiert wird. Sind beide Mängel behoben, steht einer sprachwissenschaftlichen Nutzung von Unimorph hinsichtlich der Sprachkontaktforschung im Kaukasus nichts mehr entgegen.
                    </p>
            </div>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Chiarcos, Christian / Ionov, Maxim / Rind-Pawlowski, Monika / Fäth, Christian / Wichers Schreur, Jesse / Nevskaya, Irina</hi> (2017): “LLODifying linguistic glosses” in: 
                        <hi rend=""italic"">Proceedings of the First International Conference on Language, Data and Knowledge (LDK 2017), Galway, Ireland, June 2017</hi>. Springer (Lecture Notes in Artificial Intelligence (LNAI)), 89-103 https://doi.org/10.1007/978-3-319-59888-8_7 [letzter Zugriff 14. Januar 2018]
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Cotterell, Ryan / Kirov, Christo / Sylak-Glassman, John / Yarowsky, David / Eisner, Jason / Hulden, Mans</hi> (2016). “The SIGMORPHON 2016 Shared Task Morphological Reinflection” in: 
                        <hi rend=""italic"">Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, Berlin, Germany, August, 2016</hi>. Association for Computational Linguistics, 10-22 http://anthology.aclweb.org/W16-2002.pdf [letzter Zugriff 14. Januar 2018]
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Sylak-Glassman / John</hi> (2016). “The composition and use of the universal morphological feature schema (UniMorph schema)”. Technical report, Department of Computer Science, Johns Hopkins University, working draft, v.2, https://unimorph.github.io/doc/unimorph-schema.pdf [letzter Zugriff 14. Januar 2018]
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,community-standards;morphologie(generierung);schnittstellenprobleme (nlp vs. sprachwissenschaft);sprachdokumentation;universal morphology,German,annotieren;methoden;modellierung;sprache;text
10701,2018 - University of Cologne,University of Cologne,Kritik der digitalen vernunft,2018,DHd,DHd,,Cologne,,Germany,https://dhd2018.uni-koeln.de/,Professionalisierung der Ausbildung von Geisteswissenschaftlern in der Digitalisierung von Texten,,Michael Dahnke,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Motivation</head>
            <p>
                    »Angesichts der steigenden Sichtbarkeit der Digital Humanities, auch und gerade bei universitären Schwerpunktsetzungen, ist die Frage, wie sie am sinnvollsten gelehrt werden sollen, von steigender Bedeutung« […] »Um diese Bemühungen längerfristig in der Community zu verankern, wurde auf der ersten Jahreskonferenz der Digital Humanities der deutschsprachigen Länder im März 2014« […] »eine Arbeitsgruppe der DHd gegründet. Die Proponenten der Arbeitsgruppe schlugen vor,« […] »die bisher losen Diskussionen stärker auf ein »Referenzcurriculum« zu fokussieren.«
                    <ref target=""https://dig-hum.de/ag-referenzcurriculum-digital-humanities"">(https://dig-hum.de/ag-referenzcurriculum-digital-humanities)</ref>
            </p>
            <p>
                    Diesem Anliegen fühlt sich der als Dozent für die Vermittlung von Digitalisierungskompetenz an der Universitäsbibliothek Würzburg arbeitende Autor verpflichtet. Er engagiert sich in der 
                    <ref target=""https://dig-hum.de/ag-referenzcurriculum-digital-humanities"">
                  <hi rend=""italic"">AG Referenzcurriculum Digital Humanities,</hi>
               </ref> ist Mitglied des Würzburger Arbeitskreis Digitale Editionen und unterstützt das Editionsprojekt 
                    <ref target=""http://kallimachos.de/kallimachos/index.php/Narragonien"">
                  <hi rend=""italic"">Narragonien digital.</hi>
               </ref> 
                    Er plädiert mit seinem Beitrag dafür, die DH-Ausbildung bezüglich der Bilddigitalisierung und des OCR (Optical Character Recognition) stärker zu kanonisieren. Mit seiner eigenen 
                    <ref target=""https://www.bibliothek.uni-wuerzburg.de/fileadmin/ub/pdf-Dateien/Infoblaetter/handzettelModulvorstellung.pdf"">Lehrveranstaltung</ref>
               <ref target=""https://www.bibliothek.uni-wuerzburg.de/fileadmin/ub/pdf-Dateien/Infoblaetter/handzettelModulvorstellung.pdf"">
                  <hi rend=""italic"">Bilddigitalisierung und OCR für Geisteswissenschaftler</hi>
               </ref>, deren Schwerpunkt auf der Erstellung der Digitalisate und dem OCR liegt, bietet er eine Referenz, die er hiermit zur Diskussion in der Community stellt.
                </p>
            <p>Das Ziel der Lehrveranstaltung ist die Vermittlung von Kenntnissen des gesamten Digitalisierungsprozesses für MA-, BA- und LA-Studentinnen und Studenten aller geisteswissenschaftlichen Fachrichtungen. Diese nachfolgend als Zielgruppe Bezeichneten sollen in die Lage versetzt werden, selbständig strukturiert ausgezeichnete, digitale Volltexte zu erzeugen und die Ergebnisse der Erstellung derselben beurteilen zu können. Diese Kenntnisse sind wichtig, weil Projekte häufig auch dadurch gefährdet sind, dass Vertretern der Zielgruppe die mangelnde Qualität der ihnen vorliegenden Digitalisate zu spät bewusst wird. Darum müssen sie von Beginn an Digitalisate auf ihre Brauchbarkeit für die automatische Texterkennung beurteilen können. Strukturiert ausgezeichnete, digitale Volltexte sind unabdingbar beispielsweise für Topic Modeling oder Sentiment Analysis auf größeren Textcorpora und als Zwischenstufe für die Erstellung digitaler Editionen.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Ablauf</head>
            <p>Kurz gefaßt sind die sechs Arbeitschritte dafür</p>
            <list type=""ordered"">
               <item>die Sensibilisierung für juristische Aspekte der Bilddigitalisierung,</item>
               <item>die Suche nach vorhandenen oder die Erstellung von eigenen Digitalisaten,</item>
               <item>deren Vorverarbeitung,</item>
               <item>das OCR,</item>
               <item>die Anreicherung der Digitalisate und des generierten Rohtextes – im Idealfall einer diplomatische Transkription – mit Metadaten, sowie</item>
               <item>die inhaltliche Auszeichnung des generierten Rohtextes.</item>
            </list>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>2.1. Rechtliche Grundlagen der Bilddigitalisierung</head>
               <p>Um den Teilnehmern den typischen Arbeitsablauf der Textdigitalisierung möglichst stringent und ohne thematische Abschweifungen vorzuführen, werden sie zuerst intensiv mit den juristischen Grundlagen der Bilddigitalisierung vertraut gemacht. Dazu gehören</p>
               <list type=""ordered"">
                  <item>die Vorstellung des UrhG beziehungsweise speziell der § 60d und 60g UrhG in der novellierten Fassung des UrhWissG 2017,</item>
                  <item>die Unterscheidung zwischen Immaterial- und Materialgüterrecht und was daraus für die Digitalisierung zweidimensionaler Objekte folgt,</item>
                  <item>die Persönlichkeitsrechte des Urhebers und weiterer Betroffener sowie</item>
                  <item>der Umgang mit Werken, die unter der Creative Commons Lizenz stehen und die Möglichkeit, diese selbst zu benutzen.</item>
               </list>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>2.2. Suche nach vorhandenen Digitalisaten beziehungsweise deren Erstellung</head>
               <p>Am Anfang der Transformation vom gedruckten zum digitalen Corpus steht die Suche nach möglicherweise bereits vorhandenen Digitalisaten. Diese Suche setzt neben nicht DH-spezifischen Kenntnissen der Erschließung das Wissen um Metadaten zu digitalen Bildformaten voraus. Die Vertreter der Zielgruppe müssen in dieser Situation wissen, dass beispielsweise die Chancen eines erfolgreichen OCR mit einem JPEG mit 72 dpi deutlich geringer sind als mit einem unkomprimierten TIFF, True Color und 300 dpi. Sollte er schließlich feststellen, dass ihm Digitalisate in der gewünschten Form nicht zugänglich sind, bedarf er der Kenntnisse zu digitalen Bildformaten genauso, um im nächsten Schritt erfolgreich den Scan selbst durchzuführen oder nach seinen Vorgaben durchführen zu lassen.</p>
               <p>Ausgehend von den skizzierten Anforderungen werden den Vertretern der Zielgruppe in der Veranstaltung die Grundlagen der Bilddigitalisierung nahe gebracht. Vertieft wird hier auf das menschliche Sehen und die Farbreproduktion, die Entstehung digitaler Bilder (Rastergraphik, optische und interpolierte Auflösung, Farbtiefe), Farbräume, Color-Management-Systeme, verschiedene Graphikspeicherformate, Speichermedien und verschiedene Scannertypen eingegangen. Auch für die Berücksichtigung konservatorischer Aspekte werden die Teilnehmer sensibilisiert.</p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>2.3. Aufbereitung der Digitalisate für das OCR</head>
               <p>Die Forschung im Bereich OCR, insbesondere auf Inkunabeln und Wiegendrucken, sowie die eigene Praxis des Autors belegen die Bedeutung einer vorherigen Aufbereitung der Digitalisate für das OCR, der darum entsprechend Platz in der Veranstaltung eingeräumt wird (Springmann 2015: 9). Als Tätigkeiten sind hier in der Reihenfolge ihrer Ausführung die Bereinigung und anschließende Binarisierung der Digitalisate, deren Segmentierung in einzelne Textabschnitte und schließlich einzelne Textzeilen sowie die Transkription (›ground truth‹) einer Anzahl der Textzeilen zu nennen. Die gesamte Aufbereitung der Digitalisate für das OCR führen die Vertreter der Zielgruppe in der Veranstaltung selbst an ausgewähltem Trainingsmaterial durch. Nach der Teilnahme an der Veranstaltung sollen sie auch diesen Arbeitsschritt selbständig erledigen und Arbeitsergebnisse anderer in diesem Bereich beurteilen können.</p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>2.4. OCR</head>
               <p>Entsprechend der zunehmenden Spezialisierung des Digitalisierungszentrums der UB Würzburg ist es erstens wünschenswert, in der Lehrveranstaltung besonders auf das Training eigener Modelle beispielsweise mit 
                    <ref target=""https://github.com/tmbdev/ocropy"">
                     <hi rend=""italic"">OCRopus</hi>
                  </ref> einzugehen. Zweitens soll ein Arbeitsablauf für die Digitalisierung eigener Texte vorgestellt werden, der von den Vertretern der Zielgruppe selbständig mit möglichst geringem technischen Aufwand realisierbar ist. Diese Form der Digitalisierung von Texten soll als handhabbares Mittel zum Zweck wahrgenommen werden.
                    </p>
               <p>Schließlich wird in diesem Zusammenhang auf die Frage nach dem Zeitpunkt der Normalisierung des mit dem OCR erstellten Textes eingegangen. Soll bereits mit dem OCR ein normalisierter Text erstellt werden und wenn ja, nach welchen Regeln? Ist also beispielsweise von der verwendeten OCR-Software das Schaft-s bereits automatisch als Rund-s zu lernen und anschließend zu transkribieren? Oder soll das Ergebnis des OCR graphisch so dicht wie möglich am Original bleiben und normalisierende Eingriffe erst hinterher erfolgen?</p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>2.5. Auszeichnung/Anreicherung</head>
               <p>Nach dem OCR wird erst die Notwendigkeit der Auszeichnung sowohl der Digitalisate mit Metadaten als auch des Rohtextes erläutert, die die Teilnehmer dann auch selbst vornehmen sollen. Für den extrahierten Rohtext gilt das in zweifacher Hinsicht: Erstens sind ihm Metadaten hinzuzufügen, welche die spätere, eindeutige Identifikation des Werkes und dessen Auffindbarkeit ermöglichen. Zweitens muss der Text strukturiert mit inhaltsbezogenen Elementen angereichert werden.</p>
               <p>Konkret sind bei der digitalen Repräsentation eines Romans beispielsweise die Figuren, Orte, Zeitpunkte und gegebenenfalls weitere signifikante Entitäten im Text für das spätere, automatisierte Retrieval zu kodieren. Andere Anforderungen stellen digitalierte Transkriptionen gesprochener Sprache und wiederum andere die Erstellung einer Urkundenedition (Vogeler 2015). Für die visuelle Präsentation, beispielsweise auf einem Webportal, sind textstrukturierende Merkmale wie die Einteilung nach Kapiteln, Abschnitten, Fußnoten etc. zu kennzeichnen. Dem unterschiedlichen Kenntnisstand der Teilnehmer geschuldet muss hier vor der Vorstellung der TEI Guidelines zuvor zweifelsohne XML dargestellt werden. Wie ausführlich daneben 
                    <ref target=""http://dublincore.org/"">Dublin Core</ref> und bibliotheksspezifische Formate (MARC21) thematisiert werden, ist noch nicht entschieden. Wieviel Zeit für weiterführende Themen wie 
                    <ref target=""http://www.datacommunitydc.org/blog/2013/04/a-survey-of-stochastic-and-gazetteer-based-approaches-for-named-entity-recognition"">Named Entity Recognition</ref>, 
                    <ref target=""http://www.persid.org/downloads/PI-intro-2010-09-22.pdf"">PID</ref> und Normdaten wie GND bleibt, muß ebenfalls die Praxis weisen.
                    </p>
            </div>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Forschungsbezug und Weiterentwicklung</head>
            <p>Die eine Stärke der Würzburger Lehrveranstaltung ist die Praxisorientierung, der nach der zweitägigen Einführung noch stärker mit einer drei Tage dauernden Übung Rechnung getragen wird. Bei dieser werden die Vertreter der Zielgruppe mit vorbereiteten Scans selbständig die genannten Arbeitsschritte von der Bereinigung und anschließenden Binarisierung über die Segmentierung und Transkription, dem OCR bis zum anschließenden Auszeichnen beziehungsweise der Anreicherung des digitalen Corpus mit den nötigen Metadaten vornehmen.</p>
            <p>Unverzichtbar für die gesamte Ausbildung im DH-Bereich ist neben dem Praxisbezug die Orientierung am neuesten Stand der Forschung in allen Teilbereichen. Dem wird bei der skizzierten Lehrveranstaltung erstens durch die Forschung einzelner Mitglieder des Digitalisierungszentrums als die Veranstaltung verantwortende Abteilung Rechnung getragen (1. Reul/Wick/Springmann/Puppe: 2017. 2. Springmann: 2016. 459–462). Zweitens ist die enge Zusammenarbeit des Digitalisierungszentrums mit dem Lehrstuhl für Informatik VI der Universität Würzburg zu nennen.</p>
            <p>Denkbare Erweiterungen für eine zukünftig breiter angelegte Lehrveranstaltung der beschriebenen Art sind die Vorstellung a) des OCR von Handschriften, beispielsweise in der Kooperation mit einer 
                    <ref target=""https://transkribus.eu/wikiDe/index.php/Hauptseite"">Transkribus</ref> anwendenden Institution, idealerweise der 
                    <ref target=""https://transkribus.eu/Transkribus/"">
                  <hi rend=""italic"">Digitalisierung und elektronische Archivierung – DEA</hi>
               </ref> der Universität Innsbruck, und b) des Einsatzes virtueller Forschungsumgebungen zur Herstellung digitaler Ressourcen.
                </p>
            <p>Aus Sicht des Autors ist nach der erfolgreichen Durchführung die kritische Diskussion mit den Autors ähnlicher oder gleicher Veranstaltungen von anderen Institutionen unverzichtbar. Ausgehend von
                    <ref target=""http://cceh.uni-koeln.de/digitale-geisteswissenschaften-studiengange-2011/"">http://cceh.uni-koeln.de/digitale-geisteswissenschaften-studiengange-2011/</ref> befragt er aktuell die Mitarbeiter einschlägiger Institutionen nach deren Angeboten im Bereich der Digitalisierung von Texten. Er hofft mit seinem Beitrag wie dem AG Treffen an der DHd2018 auf einen fruchtbaren Meinungsaustausch.
                </p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Corbach, Almuth</hi>: 
                        <hi rend=""italic"">Bestandsschonendes Digitalisieren von schriftlichem Kulturgut</hi>. In: Digital und analog. Die beiden Archivwelten. 46. Rheinischer Archivtag. Ratingen 21.-22. Juni 2012.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Jannidis, Fotis / Hubertus Kohle / Malte Rehbein </hi>[Hrsg.]: 
                        <hi rend=""italic"">Digital Humanities. Eine Einführung</hi>. Springer-Verlag GmbH Deutschland, 2017.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Kneißl, Michael</hi>: 
                        <hi rend=""italic"">Scannen wie die Profis : Text- und Bildvorlagen perfekt digitalisieren</hi>. München: DTV. (2)2002.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Loewenheim, Ulrich / Adolf Dietz / Gerhard Schricker</hi>: 
                        <hi rend=""italic"">Urheberrecht</hi>. Kommentar. München: Beck. (4)2010.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Reul, Christian / Christoph Wick / Uwe Springmann / Frank Puppe</hi>: 
                        <hi rend=""italic"">Transfer Learning for OCRopus Model Training on Early Printed Books</hi>. In: 
                        <hi rend=""italic"">Zeitschrift für Bibliothekskultur</hi>. Bd. 5, Nr. 1 (2017).
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Springmann, Uwe</hi>: 
                        <hi rend=""italic"">A high accuracy OCR method to convert early printings into digital text. A Tutorial</hi>. 
                        <hi rend=""italic"">Center for Information and Language Processing (CIS)</hi>. LMU. München. 2015. S. 9.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Springmann, Uwe</hi>: 
                        <hi rend=""italic"">OCR für alte Drucke</hi>. 
                        <hi rend=""italic"">Informatik-Spektrum</hi>. 39(6):459–462. 2016.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Vogeler, Georg</hi>: 
                        <hi rend=""italic"">Die Text Encoding Initiative (TEI) als Werkzeug des Urkundeneditors – Erfahrungen und Desiderate</hi>. In: Fees, Irmgard Prof. Dr.; Hotz, Benedikt; Schönfeld, Benjamin (Hrsg.): 
                        <hi rend=""italic"">Papsturkundenforschung zwischen internationaler Vernetzung und Digitalisierung. Neue Zugangsweisen zur europäischen Schriftgeschichte</hi>. Göttingen. 2015.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Weitzmann, John H. / Paul Klimpel</hi>: Rechtliche Rahmenbedingungen für Digitalisierungsprojekte von Gedächtnisinstitutionen. Berlin: Zuse Institute Berlin. digiS – Servicestelle Digitalisierung Berlin. (3)2016.</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,digitalisierung ocr metadaten transkription tei,German,bilder;bilderfassung;daten;metadaten;transkription
10762,2018 - University of Cologne,University of Cologne,Kritik der digitalen vernunft,2018,DHd,DHd,,Cologne,,Germany,https://dhd2018.uni-koeln.de/,Annotation and beyond – Using ATHEN. Annotation and Text Highlighting Environment,,Markus Krug;Ngoc Duyen Tanja Tu;Lukas Weimer;Isabella Reger;Leonard Konle;Fotis Jannidis;Frank Puppe,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div/>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Goals of the workshop</head>
            <p>The workshop presents ATHEN
                    <ref target=""ftn1"" n=""1""/> (<hi rend=""bold"">A</hi>nnotation and 
                    <hi rend=""bold"">T</hi>ext 
                    <hi rend=""bold"">H</hi>ighlighting 
                    <hi rend=""bold"">En</hi>vironment), an extensible desktop-based annotation environment which supports more than just regular annotation. Besides being a general purpose annotation environment, ATHEN supports indexing and querying support of your data as well as the ability to automatically preprocess your data with Meta information. It is especially suited for those who want to extend existing general purpose annotation tools by implementing their own custom features, which cannot be fulfilled by other available annotation environments. On the according gitlab, we provide online tutorials, which demonstrate the use of specific features of ATHEN.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Related Work</head>
            <p>We compare ATHEN to three web-based and four desktop applications in 12 categories by adapting most criteria defined by Neves and Leser (Neves / Leser 2012) to compare different annotation tools:</p>
            <list type=""ordered"">
               <item>Availability and up-to-dateness of the documentation</item>
               <item>Active development at the present time</item>
               <item>Source code for download</item>
               <item>Complexity of system requirements</item>
               <item>Interoperability by supporting certain formats</item>
               <item>Support of different annotation layers</item>
               <item>Support of NLP-preprocessing to speed up manual annotation</item>
               <item>Support of visualization</item>
               <item>Support of self-learning systems to speed up manual annotation</item>
               <item>Support of querying annotated data</item>
               <item>Possibility to do an inter-annotator-agreement, this is important for projects, in which more than one annotator labels the same documents</item>
               <item>Extensibility </item>
            </list>
            <p>We explicitly do not want to compare subjective features like usability or how the annotations are presented.</p>
            <figure>
               <graphic n=""1001"" width=""16.002cm"" height=""8.225013888888888cm"" rend=""inline"" url=""KRUG_Markus_Annotation_and_beyond___Using_ATHEN-image1.png""/>
               <head>Table 1: Comparison of three web-based and four desktop applications with ATHEN in twelve categories. No tool excels in every category.</head>
            </figure>
            <p>All of the listed tools have an accessible documentation, either web-based or as a PDF, available for download. Besides UAM (O'Donnell 2008), every other application is listed as open source, so at least extensions based on code level can be made. WebAnno (Yimam et al. 2013) is the only application having a tutorial supporting a new developer to make changes in their project. ATHEN stands out in the sense that extensions to its UI can be made at runtime, therefore easing the process of adding functionality to it. WebAnno supports the largest number of formats and comes with a machine learning based automatic annotation, however lacks integrated NLP-preprocessing. CATMA (Meister 2017) is the only project that has a very good visualization component and also supports TEI-XML (Wittern et al 2009), the unspoken standard of text processing. Being a standalone web application, CATMA itself does not support NLP-preprocessing. ATHEN comes with the support of the execution of UIMA analysis engines, accessible from web repositories or a local repository, giving the user a chance to integrate her custom-made annotators. Four tools, ATHEN, UAM, MMAX2 (Müller / Strube 2006) and CATMA feature an integrated query language which helps to analyze existing corpora. Most tools allow the annotation of user-defined annotation schemas earning therefore the title “generic annotation tool”. Alongside UAM, ATHEN supports the annotation based on queries, while UAM defines its own language, ATHEN supports the annotation using Apache UIMA Ruta (Kluegl et al. 2016) rules. Three of the listed tools, MMAX2, Knowtator (Ogren 2006) and WordFreak (Morton / LaCivita 2003) are currently no longer in active development. </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Brief technological description of ATHEN</head>
            <p>ATHEN is a Java-based desktop application with the vision to be extensible. Therefore, it makes use of the flexible plugin architecture of the eclipse 
                    <hi rend=""bold"">R</hi>ich-
                    <hi rend=""bold"">C</hi>lient-
                    <hi rend=""bold"">P</hi>latform (RCP)
                    <ref target=""ftn2"" n=""2""/>. Internally, it is built around Apache UIMA, which means incoming data is automatically converted into the UIMA specific Common Analysis Subject (CAS) architecture. Working with UIMA allows the integration of standalone analysis engines, which can be used to preprocess data and speed up manual annotation. The use of Apache Lucene enables ATHEN to create an index comprising documents, as well as their annotations, which results in queries that can answer questions based on text and meta information in real time. With the ability to execute Apache UIMA Ruta one can even create queries of far higher complexity. On top of that, ATHEN features OWL-Support, which allows the definition of an ontological annotation schema in a machine- readable format. Using Apache UIMA internally allows ATHEN to even address more complicated input. Currently ATHEN supports the annotation of image regions, based on user defined polygons.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Program of the workshop</head>
            <p>The program is split into four sections:</p>
            <list type=""ordered"">
               <item>Introduction to ATHEN and distinguishing from other existing annotation environments.</item>
               <item>Working with ATHEN, which contains the definition of scenarios and annotating sample documents.</item>
               <item>Utility of ATHEN (beyond regular annotation), which addresses the following topics:
                       <list type=""ordered"">
                     <item>Defining an annotation schema using OWL</item>
                     <item>Preprocessing texts based on Apache UIMA Analysis Engines</item>
                     <item>Creating and executing queries based on Apache Lucene</item>
                     <item>Annotating images with ATHEN</item>
                  </list>
               </item>
               <item>Extending ATHENs functionalities and adapting it to your needs (developer specific)</item>
            </list>
            <p>The first section is a presentation which shows the main differences between the existing annotation tools. The second section defines an ordinary annotation scenario and it is used to introduce the participants to the general-purpose annotation view of ATHEN. Afterwards, for tasks to which ATHEN has special support (annotating character references and their coreferences, annotating direct speech and their speaker) an introduction to the special purpose views of ATHEN is given.</p>
            <p>The third panel introduces the participants to the functionality of ATHEN beyond regular text annotation. It starts with the definition of an OWL ontology (and its utilization for texts). This is centered on relation detection of character references, as well as an attribution of those references.</p>
            <p>To speed up manual annotation it is helpful to have it preprocessed with existing tools. The task definition is then changed from pure annotation to an application with a consecutive correction of the output of the automatic engines. In this context, Nappi, a submodule of ATHEN is presented and it is shown how to define, execute and integrate custom analysis engines. </p>
            <p>The next part is dedicated towards extracting knowledge from annotated data, for this purpose, an Apache Lucene Index is created using ATHEN and is queried in a live fashion. This feature allows rapid insight into an existing corpus and enables the user to answer their own hypothesis.</p>
            <p>The tutorial continues with the presentation of how images can be annotated with polygon-based annotations to show, that ATHEN is not only limited to textual resources.</p>
            <p>The last part is directed towards Java developers who are interested in developing their own annotation component.</p>
            <p>Each section starts with a set of slides which introduce the features in focus and presents the participants with one or more tasks that can be fulfilled by using ATHEN.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Requirements</head>
            <p>The participants need their own laptops with an active internet connection. The number of participants is limited to 15 to 20. The last section requires knowledge of Java. Data which is necessary for the tutorials will be hosted on our own server and will be made accessible for download.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Research projects</head>
            <p>ATHEN is mainly developed in the context of the project Kallimachos at the University of Wuerzburg. Its main purpose was to support the annotation process of DROC (
                    <hi rend=""bold"">D</hi>eutscher 
                    <hi rend=""bold"">RO</hi>man 
                    <hi rend=""bold"">C</hi>orpus). Currently automatic creation of literary interaction networks, automatic genre detection of novels and sentiment analysis in literary novels are in the focus of interest.
                </p>
            <p>An extension to ATHEN was made in the project “Redewiedergabe” to manually annotate different forms of speech, thought and writing representation (STWR). These annotations will then be used to train an automatic recognizer for STWR.</p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"">
               <ref target=""https://gitlab2.informatik.uni-wuerzburg.de/kallimachos/Athen"">https://gitlab2.informatik.uni-wuerzburg.de/kallimachos/Athen</ref>
            </note>
            <note id=""ftn2"" n=""2"" rend=""footnote text"">A web version of ATHEN with its major features is available at: 
                            <ref target=""https://webathen.informatik.uni-wuerzburg.de"">https://webathen.informatik.uni-wuerzburg.de</ref>
            </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Kluegl, Peter / Toepfer, Martin / Beck, Philip-Daniel / Fette, Georg / Puppe, Frank</hi> (2016): “UIMA Ruta: Rapid Development of Rule-based Information Extraction Applications“, in: 
                        <hi rend=""italic"">Natural Language Engineering 22.1</hi> 1-41.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Meister, Jan Christoph / Gius, Evelyn / Jacke, Janina / Petris, Marco</hi>: 
                        <hi rend=""italic"">CATMA 5.0.</hi>
                  <ref target=""http://catma.de/"">http://catma.de/</ref> [Accessed September 22, 2017].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Morton, Thomas / LaCivita, Jeremy</hi> (2003): “WordFreak: an open tool for linguistic annotation“, in: 
                        <hi rend=""italic"">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Demonstrations-Volume 4 </hi>17-18.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Müller, Christoph / Strube, Michael </hi>(2006): “Multi-level annotation of linguistic data with MMAX“, in
                        <hi rend=""italic"">: Corpus technology and language pedagogy: New resources, new tools, new methods 3</hi> 197-214.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Neves, Mariana / Leser, Ulf </hi>(2012): “A survey on annotation tools for the biomedical literature“, in: 
                        <hi rend=""italic"">Briefings in Bioinformatics 15.2</hi> 327-340.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">O'Donnell, Mick</hi> (2008): “Demonstration of the UAM CorpusTool for text and image annotation“, in: 
                        <hi rend=""italic"">Proceedings of the 46th annual meeting of the Association for computational linguistics on human language technologies: Demo session</hi> 13-16.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Ogren, Philip V.</hi> (2006): “Knowtator: a Protégé plug-in for annotated corpus construction“, in: 
                        <hi rend=""italic"">Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume: demonstrations</hi> 273-275.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Stenetorp, Pontus / Pyysalo, Sampo / Topić, Goran / Ohta, Tomoko / Ananiadou, Sophia / Tsujii, Jun’ichi</hi> (2012): “BRAT: a Web-based Tool for NLP-Assisted Text Annotation“, in : 
                        <hi rend=""italic"">Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics</hi> 102-107.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Wittern, Christian / Ciula, Arianna / Tuohy, Conal</hi> (2009): “The making of TEI P5“, in: 
                        <hi rend=""italic"">Literary and Linguistic Computing 24.3</hi> 281-296.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Yimam, Seid Muhie / Gurevych, Iryna / de Castilho, Richard Eckart / Biemann, Chris</hi> (2013): “WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations“, in: 
                        <hi rend=""italic"">Proceedings of ACL-2013, demo session, Sofia, Bulgaria</hi> 1-6
                        <hi rend=""italic"">.</hi>
               </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,annotation;extensibility;nlp;preprocessing;query,German,annotieren;bearbeitung;metadaten;programmierung;software;visualisierung
10865,2019 - Johannes Gutenberg University;Goethe University,Johannes Gutenberg University;Goethe University,multimedial & multimodal,2019,DHd,DHd,Johannes Gutenberg-Universität Mainz (Johannes Gutenberg University of Mainz);Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt),Frankfurt & Mainz,,Germany,https://dhd2019.org/,OCR Nachkorrektur des Royal Society Corpus,,Carsten Klaus;Peter Fankhauser;Dietrich Klakow,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading"">
            <head>Einleitung</head>
            <p>Linguistische Analysen historischer Texte stellen Forscher oftmals vor große Herausforderungen. Im Gegensatz zur Digitalisierung moderner Dokumente kann es bei jahrhundertealten Texten zu Schwierigkeiten kommen. Diese weisen oftmals eine geringere Qualität auf, sodass es beim Einlesen zu Fehlern kommt. Solche können schwerwiegende Störfaktoren für weitere Analysen sein. In diesem Beitrag beschreiben wir den
                    <hi rend=""bold"">Noisy Channel </hi>
               <hi rend=""bold"">Spell Checker</hi>, ein Verfahren zur automatisierten Korrektur von Optical Character Recognition (OCR) induzierten Rechtschreibfehlern in historischen Texten, genauer dem 
                    <hi rend=""bold"">Royal Society Corpus</hi>.
                </p>
            <p>Beim Royal Society Corpus (RSC) handelt es sich um eine Sammlung wissenschaftlicher Texte von 1665 bis 1869, veröffentlicht im Journal
                    <hi rend=""italic"">Philosophical Transactions of the Royal Society of London. </hi>Das Korpus umfasst ungefähr 10.000 Dokumente mit insgesamt 35.000.000 Tokens. Die Texte wurden mithilfe von Optical Character Recognition digitalisiert, bedingt durch das alte Material der Dokumente wurden jedoch Worte falsch erkannt und somit Rechtschreibfehler eingestreut. Diese sollen in einer Nachkorrektur berichtigt werden. (UdS Fedora Commons o.J.)
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading"">
            <head>State of the Art </head>
            <p>Das Korpus wird einer strikten Versionskontrolle unterzogen. Fortschritte bzgl. Formatierung oder Fehlerkorrektur werden in aufsteigenden 
                    <hi rend=""italic"">corpusBuild</hi> Versionen festgehalten. Derzeit wird das Royal Society Corpus durch einen 
                    <hi rend=""bold"">Pattern</hi>-basierten Ansatz bereinigt (Knappen, 2017). Hierbei werden Ersetzungsregeln auf die Texte angewendet um Fehler mit ihrer richtigen Form auszutauschen, wie beispielsweise 
                    <hi rend=""italic"">tbe</hi> → 
                    <hi rend=""italic"">the</hi>. Der große Nachteil dieses Verfahrens ist jedoch, dass nur ein Bruchteil der induzierten OCR Fehler abgedeckt wird, was in einer geringen Fehlererkennung resultiert. Im Folgenden erläutern wir unseren Ansatz, welcher mit einem statistischen Lernverfahren deutlich bessere Ergebnisse erzielt.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading"">
            <head>Methodik</head>
            <p>Der 
                    <hi rend=""italic"">Noisy Channel Spell Checker</hi> basiert auf dem 
                    <hi rend=""bold"">Noisy Channel Model </hi>(Shannon, 1948). Ein potentiell fehlerhaftes Wort 
                    <hi rend=""bold italic"">w</hi>
               <hi rend=""italic""> </hi>wird wie folgt korrigiert: Aus einer Vorauswahl an geeigneten Kandidaten 
                    <hi rend=""bold italic"">c</hi> aus 
                    <hi rend=""bold italic"">C</hi> wird abgeschätzt welcher am ehesten als Korrektur 
                    <hi rend=""bold italic"">ŵ</hi> in Frage kommt.
                </p>
            <p>
               <figure>
                  <graphic url=""147_final-35d7dd4107cdde71e841941e11703323.png""/>
               </figure>
            </p>
            <p>Das Noisy Channel Model besteht zum einen aus dem 
                    <hi rend=""bold"">Sprachmodell </hi>
               <hi rend=""bold italic"">P(c)</hi> und zum anderen dem 
                    <hi rend=""bold"">Fehlermodell</hi>
               <hi rend=""bold italic"">P(w|c)</hi>. Es werden hierbei zwei intuitive Gedanken kombiniert: Das Sprachmodell schätzt die Wahrscheinlichkeit des Kandidaten in seinem Wortkontext ab. Hochfrequentierte Worte sind demnach sehr wahrscheinlich. Das Gegengewicht hierzu bildet das Fehlermodell. Diese Verteilung gibt an wie sicher 
                    <hi rend=""bold italic"">w</hi> eine fehlerhafte Variante von 
                    <hi rend=""bold italic"">c</hi> ist, schätzt also ab, wie wahrscheinlich einzelne Korrekturschritte von
                    <hi rend=""bold italic"">w</hi> nach 
                    <hi rend=""bold italic"">c</hi> sind. 
                    <hi rend=""color(#000000)bold"">λ</hi>
               <hi rend=""color(#000000)""> </hi>
               <hi rend=""color(#000000)"">ist ein frei wählbarer Parameter, mithilfe dessen man das Sprachmodell gewichten kann. </hi>
               <hi rend=""color(#000000)"">(Jurafsky 2016: </hi>
               <hi rend=""color(#000000)"">61-73</hi>
               <hi rend=""color(#000000)"">)</hi>
            </p>
         </div>
         <div type=""div1"" rend=""DH-Heading"">
            <head>Training des Modells</head>
            <p>Die Besonderheit unseres Ansatzes besteht darin, dass Sprach-, sowie Fehlermodell 
                    <hi rend=""bold"">korpusspezifisch</hi> trainiert werden. Es sind keine aufwändigen Trainingsdatenannotationen notwendig, denn es werden lediglich die Korpusdateien verwendet.
                </p>
            <list type=""unordered"">
               <item>Das 
                        <hi rend=""bold"">Sprachmodell</hi> wurde mithilfe der aktuellsten 
                        <hi rend=""italic"">corpusBuild</hi> Version des Royal Society Corpus trainiert. Diese Texte sind durch die Patterns bereits best möglich bereinigt worden. Somit wurde versucht das Rauschen innerhalb der Verteilung zu reduzieren. 
                    </item>
               <item>Zum Trainieren des 
                        <hi rend=""bold"">Fehlermodells</hi> wurden die bereits erwähnten Patterns als Wissensbasis hinzugezogen. Die Idee war hier aus der Korrektur durch die Patterns eine Wahrscheinlichkeitsverteilung zu erzeugen, also das Fehlerverhalten im Korpus zu generalisieren. Anhand eines Beispiels lässt sich dies veranschaulichen: Gegeben die Ersetzungsregel 
                        <hi rend=""bold italic"">fuch → such. </hi>Diese wird in folgende Sequenz von edit Operationen aufgebrochen:
                        <hi rend=""bold italic"">f|s </hi>
                  <hi rend=""bold italic"">+</hi>
                  <hi rend=""bold italic""> u|u </hi>
                  <hi rend=""bold italic"">+</hi>
                  <hi rend=""bold italic""> c|c </hi>
                  <hi rend=""bold italic"">+</hi>
                  <hi rend=""bold italic""> h|h</hi>. Der Trainingsprozess erfasst nun wie oft edit Operationen angewendet wurden und leitet daraus eine Verteilung ab.
                    </item>
            </list>
         </div>
         <div type=""div1"" rend=""DH-Heading"">
            <head>Resultate und Diskussion</head>
            <p>Als Testmenge haben wir 26 Dokumente aus dem Korpus extrahiert. Diese wurden eigens korrigiert um einen Gold Standard zu erhalten. Als Evaluationsmetriken wählten wir 
                    <hi rend=""italic"">Precision </hi>(Anteil der validen Korrekturen), 
                    <hi rend=""italic"">Recall </hi>(Abdeckung der einzelnen Fehler) und daraus den 
                    <hi rend=""italic"">F1-Score </hi>(harmonisches Mittel aus Pre. und Rec.). Um die Ergebnisse unserer Arbeit zu vergleichen, haben wir zwei weitere Methoden auf die Testdaten angewendet. Dies waren zum einen die 
                    <hi rend=""bold"">Pattern</hi>
               <hi rend=""bold"">s</hi> und zum anderen nutzten wir als Referenzkorrektur für das Noisy Channel Model eine Implementierung von 
                    <hi rend=""bold"">Peter Norvig</hi> (Norvig, 2009). Die Ergebnisse sind in Abbildung 1 aufgetragen.
                </p>
            <p>
               <figure>
                  <graphic url=""147_final-28a241507022c765c8b8182f40f73f0c.png""/>
                  <head>Abbildung 1: Resultate einzelner Korrekturmethoden angewendet auf den Testdatensatz</head>
               </figure>
            </p>
            <p>
               <hi rend=""color(#000000)"">Man kann erkennen, dass </hi>
               <hi rend=""color(#000000)"">die Pattern</hi>
               <hi rend=""color(#000000)"">korrektur </hi>
               <hi rend=""color(#000000)"">(gelb)</hi>
               <hi rend=""color(#000000)""> die beste Precision </hi>
               <hi rend=""color(#000000)"">erziel</hi>
               <hi rend=""color(#000000)"">t</hi>
               <hi rend=""color(#000000)"">.</hi>
               <hi rend=""color(#000000)""> Dies ist ein typisches Verhalten regelbasierte</hi>
               <hi rend=""color(#000000)"">r</hi>
               <hi rend=""color(#000000)""> Systeme. Im Gegensatz dazu decken die beiden </hi>
               <hi rend=""color(#000000)"">anderen Verfahren</hi>
               <hi rend=""color(#000000)""> eine größere Menge an Fehlern ab, dies wird am höheren Recall deutlich. </hi>
               <hi rend=""color(#000000)"">Besonders Norvigs Variante </hi>
               <hi rend=""color(#000000)"">(blau)</hi>
               <hi rend=""color(#000000)""> ist hier führend, jedoch tendiert diese auch zur Überkorrektur von richtig erfassten Wörtern. Wir waren bestrebt, dass unser </hi>
               <hi rend=""color(#000000)"">Spell Checker</hi>
               <hi rend=""color(#000000)""> </hi>
               <hi rend=""color(#000000)"">(rot)</hi>
               <hi rend=""color(#000000)""> dies </hi>
               <hi rend=""color(#000000)"">weitestgehend </hi>
               <hi rend=""color(#000000)"">vermeidet, </hi>
               <hi rend=""color(#000000)"">indem es</hi>
               <hi rend=""color(#000000)""> </hi>
               <hi rend=""color(#000000)"">Precision und Recall möglichst balanciert. Es werden also viele OCR Rechtschreibfehler korrigiert und gleichzeitig </hi>
               <hi rend=""color(#000000)"">wird </hi>
               <hi rend=""color(#000000)"">die Rate an Falsch Positiven </hi>
               <hi rend=""color(#000000)"">gering gehalten</hi>
               <hi rend=""color(#000000)"">. </hi>
               <hi rend=""color(#000000)"">Hierbei w</hi>
               <hi rend=""color(#000000)"">ar </hi>
               <hi rend=""color(#000000)"">das Optimieren der Gewicht</hi>
               <hi rend=""color(#000000)"">u</hi>
               <hi rend=""color(#000000)"">ng </hi>
               <hi rend=""color(#000000)"">λ </hi>
               <hi rend=""color(#000000)"">des Language Models</hi>
               <hi rend=""color(#000000)""> </hi>
               <hi rend=""color(#000000)"">ein essentieller Bestandteil der Arbeit, </hi>
               <hi rend=""color(#000000)"">sodass unser Modell </hi>
               <hi rend=""color(#000000)"">schlussendlich </hi>
               <hi rend=""color(#000000)"">einen F-Score von </hi>
               <hi rend=""color(#000000)bold"">0.61</hi>
               <hi rend=""color(#000000)bold"">2 </hi>
               <hi rend=""color(#000000)"">erzielte. </hi>
               <hi rend=""color(#000000)"">Bei der Überlegung unseren Ansatz auf andere historische, unaufbereitete Texte anzuwenden </hi>
               <hi rend=""color(#000000)"">empfiehlt es sich das Fehlerverhalten in diesen Texten bestmöglich zu generalisieren</hi>
               <hi rend=""color(#000000)"">. Deshalb sollte bereits eine Wissensbasis in Form von Ersetzungspatterns vorliegen um das Error Model korpusspezifisch zu trainieren, </hi>
               <hi rend=""color(#000000)"">das heißt genauso wie in diesem Beitrag beschrieben.</hi>
            </p>
         </div>
         <div type=""div1"" rend=""DH-Heading"">
            <head>Zusammenfassung</head>
            <p>Im Vergleich zur derzeitigen pattern-basierten Methode verbesserte der 
                    <hi rend=""italic"">Noisy Channel </hi>
               <hi rend=""italic"">Spell Checker</hi> die Korrekturqualität um mehr als das Doppelte. Es werden nun Fehler berichtigt, die die Patterns nicht einmal als solche erkennen. Die Hauptmotivation zum Aufbau des Royal Society Corpus sind Untersuchungen der diachronischen Entwicklung von wissenschaftlichem Englisch (UdS Fedora Commons o.J.). Die Bereinigung der Texte macht es möglich, dass diese Analysen in Zukunft weitaus genauer und verlässlicher werden.
                </p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Jurafsky Daniel / Martin James H. (2016)</hi>: <hi rend=""italic"">""Spelling Correction and the Noisy Channel""</hi> In: Speech and Language Processing, 3. Edition, S. 61-73.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Kermes, Hannah / Degaetano-Ortlieb, Stefania / Khamis, Ashraf / Knappen, Jörg / Teich, Elke (2016)</hi>: <hi rend=""italic"">""The Royal Society Corpus: From Uncharted Data to Corpus""</hi>, in: Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA).
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Knappen, Jörg / Fischer, Stefan / Kermes, Hannah / Teich, Elke / Fankhauser, Peter (2017)</hi>: <hi rend=""italic"">""The Making of the Royal Society Corpus""</hi>, in ListLang@NoDaLiDa.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Norvig, Peter (2008)</hi>: <hi rend=""italic"">""Natural Language Corpus Data: Beautiful Data""</hi>. [online] <ptr target=""http://norvig.com/ngrams/""/> [letzter Zugriff 08. November 2017].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Shannon, Claude E. (1948)</hi>: <hi rend=""italic"">""A Mathematical Theory of Communication""</hi>, in Bell System Technical Journal.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">UdS Fedora Commons Repository (o.J.)</hi>: <hi rend=""italic"">""The Royal Society Corpus (RSC)""</hi>, <ptr target=""https://fedora.clarin-d.uni-saarland.de/rsc/""/>. [letzter Zugriff 29. März 2018].
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,historical corpus;misspellings;noisy channel model;ocr;royal society,German,bereinigung;daten;kollaboration;modellierung;programmierung;software
10881,2019 - Johannes Gutenberg University;Goethe University,Johannes Gutenberg University;Goethe University,multimedial & multimodal,2019,DHd,DHd,Johannes Gutenberg-Universität Mainz (Johannes Gutenberg University of Mainz);Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt),Frankfurt & Mainz,,Germany,https://dhd2019.org/,Ein unscharfer Suchalgorithmus für Transkriptionen von arabischen Ortsnamen,,Magdalena Scherl;Martin Unold;Timo Homburg,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Einleitung</head>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Motivation</head>
               <p>Digitale Ortsverzeichnisse (Gazetteers) beinhalten Informationen über Orte sowie deren geographische Lage. Eine der grundlegendsten Aufgaben im Umgang mit solchen Ortsverzeichnissen ist die Suche nach Ortsnamen. Diese Suche kann sehr schwierig sein für Ortsnamen, die in verschiedenen Transliterations- oder Transkriptionsvarianten vorliegen, wie es oft bei arabischen Ortsnamen der Fall ist. In diesen Fällen reicht eine reine Volltextsuche nicht aus. Hier können unscharfe String-Matching-Algorithmen eine bessere Trefferquote für Suchen erreichen.</p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Zielsetzung</head>
               <p>Unser Ziel war es, einen Suchalgorithmus zu entwickeln, der in der Lage ist, arabische Ortsnamen in verschiedenen Transliterationen und Transkriptionen zu identifizieren. Einerseits sollte der Algorithmus fehlertolerant sein, sodass er einen Suchbegriff findet, selbst wenn er etwas anders geschrieben wurde als im Ortsverzeichnis hinterlegt. Andererseits sollte er genau genug sein, um nur tatsächliche Transliterations- und Transkriptionsvarianten einzuschließen. Zum Beispiel sollte die Suche nach ""Agaga"" den Ort ""Ajaja"" finden, da es sich um verschiedene Transliterationen des selben arabischen Wortes handelt, aber nicht ""Dagaga"", da dies ein ganz anderer Ort ist. Um diese beiden Ziele zu erreichen, haben wir einen Algorithmus mit einer modifizierten gewichteten Levenshtein-Distanz (Levenshtein 1965) entwickelt. Eine weitere Eigenschaft unseres Suchalgorithmus ist, dass er für andere Anwendungsfälle als arabische Schrift leicht angepasst werden kann. Wir haben daher auch eine Version für Keilschriftsprachen implementiert und auf einem sumerischen Wörterbuch getestet.</p>
            </div>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Forschungsstand</head>
            <p>Die gewichtete Levenshtein Distanz wurde bereits für Autokorrektur (Kukich 1992), für die Korrektur von Fehlern bei der Optical Character Recognition (OCR) (Lasko 2001, Mihov 2002) und für die automatische Spracherkennung (Ziolko 2010, Zgank 2012) genutzt. Um die Kosten für die Editieroperationen zu bestimmen, schlägt Weigel (1995) einen iterativen überwachten Lernalgorithmus vor. Lasko (2001) beschreibt die Verwendung einer probabilistischen Substitutionsmatrix und Schulz / Mihov (2002) schlagen die Implementierung eines endlichen Zustandsautomaten vor, um die Performanz des Levenshtein-Algorithmus zu verbessern.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Arabische Schrift</head>
            <p>Variationen in der Schreibweise von arabischen Toponymen sind sehr häufig, da es mehrere Transliterationsstandards und verschiedene gebräuchliche Transkriptionsschemata gibt (Brockelmann 1953, Schlott-Kotschote 2004, UNGEGN 2016, Pedersen 2008). Insbesondere die Darstellung jener arabischen Buchstaben, die im lateinischen Alphabet keine direkte Entsprechung haben, variiert hier teilweise beträchtlich. Während einige Standards hierfür diakritische Zeichen verwenden, setzen andere Standards auf die Verwendung von Kombinationen aus zwei Buchstaben. Eine andere Quelle der Variation ist die fehlende Vokalisierung in der arabischen Schrift. Besonders regionale Variationen der Aussprache und Dialektdiversität führen dazu, dass arabische Vokale in der lateinischen Schreibweise unterschiedlich wiedergegeben werden. Zu Abweichungen führen auch unterschiedliche Traditionen der Transkription, die sich entweder eher an der englischen oder an der französischen Aussprache orientieren. Ein weiteres Problem, das zu Variationen führen kann, sind Wortgrenzen und divergierende Ansätze in der Zusammen- und Getrenntschreibung, insbesondere bei der Verwendung des Artikels ""al"".</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Keilschriftsprachen</head>
            <p>Die Entwicklung von Software für die Verbesserung der Bearbeitbarkeit von Keilschriftsprachen traf in der Vergangenheit auf ein reges Interesse in der Digital Humanities Community.</p>
            <p>Homburg (2016, 2017, 2018) zeigten, dass Fortschritte in der Erstellung einer Natural Language Processing Pipeline und in der Erstellung von State-Of-The-Art semantischen Wörterbüchern für verschiedene Keilschriftsprachen in Entwicklung sind. Homburg (2015) entwickelte eine auf einem Präfixbaum De La Briandais (1959) basierende Eingabemethode für Keilschriftsprachen, die auf der DHd 2015 präsentiert wurde. State Of The Art Eingabemethoden wie Sogou 
                Pinyin<ref target=""ftn1"" n=""1""/> für Chinesisch oder Google Japanese 
                Input<ref target=""ftn2"" n=""2""/> (Krueger 2000) für Japanisch beinhalten jedoch prädiktive Algorithmen, welche es erlauben die Korrektheit von Texteingaben in ihrem jeweiligen Kontext einzubeziehen und mit Fuzzy Search Algorithmen ebenfalls eine Korrektur von Tippfehlern vorzunehmen. Für die Eingabe von Keilschrift wurden solche Algorithmen bisher noch nicht erprobt, obwohl diese die Eingabe auch durch Einblendung von Zusatzinformationen ernorm vereinfachen kann und mehr relevante Suchergebnisse angezeigt werden können. Für Keilschriftsprachen im Speziellen ist eine Fuzzy Search für die Unterscheidung gerade auch der verschiedenen Dialekte und Transliterationen der Keilschriftarten interessant, da in diesen unter anderem Vokalverschiebungen und Variationen durch verschiedene Transliterationskonventionen auftreten können. Beispiele hierfür sind die Unterscheidungen von diakritischen Zeichen vs. einer numerischen Annotation (ù vs. u2), Transliterationsunterschiede wie die Verwendung von sh vs. sz und sprachliche Entwicklungen über die Zeit hinweg, in denen z.B. endende Konsonanten weggefallen sind (sogenannte Mimation).
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Ansatz</head>
            <p>Wir verwendeten ein modifiziertes Levenshtein Distanz Maß, welches speziell für die arabische Schrift angepasst wurde. Der Quellcode des Projektes ist unter der GPLv2 Lizenz in unserem Gitlab freigegeben 
                worden.<ref target=""ftn3"" n=""3""/> Die Kosten für die Editieroperationen wurden hierbei durch ein überwachtes Lernverfahren ermittelt. Wir verwendeten eine Substitutionsmatrix sowie eine Matrix für Lösch- sowie Einfügeoperationen, um die jeweiligen Kosten der Überführung von einer Transliteration in die nächste zu bestimmen.
                </p>
            <p>
               <figure>
                  <graphic url=""182_final-bb8153a7080fb1af903a522f00ecd98a.png""/>
               </figure>Abbildung 1: Beispielwerte aus der Substitutionsmatrix.
                </p>
            <p>Für die Matrix für Löschungen und Einfügungen haben wir zwei unterschiedlichen Ansätze verfolgt: Im ersten Ansatz (Levenshtein1) wurden die Lösch- sowie Einfügekosten für jeden Buchstaben ohne Betrachtung des Buchstabenkontexts ermittelt. Im zweiten Ansatz (Levenshtein2) wurden die Lösch- und Einfügekosten in Abhängigkeit des voranstehenden Buchstabens ermittelt.</p>
            <p>
               <figure>
                  <graphic url=""182_final-eb0029ec4914f0b09a21d88d11317d80.png""/>
               </figure>Abbildung 2: Beispielwerte aus der Matrix für Löschungen und Einfügungen (Levenshtein2). Reihen repräsentieren den zu löschenden bzw. den einzufügenden Buchstaben; Spalten repräsentieren den voranstehenden Buchstaben.
                </p>
            <p>Desweiteren wurden spezielle Anpassungen für die arabische Schrift wie z.B. diakritische Zeichen (ī), welche typisch für die gegebenen Transliterationen sind, in das erweiterte Alphabet aufgenommen. Außerdem waren Kombinationen aus zwei Buchstaben zu berücksichtigen, die ein arabisches Phonem repräsentieren (z.B. sh). Da die klassische Levenshtein Distanz nicht aus Buchstabenkombinationen errechnet werden kann, musste der Algorithmus auf diese angepasst werden. In einer vereinfachten Version (Levenshtein1Simple und Levenshtein2Simple) wurden die Buchstabenkombinationen im Vorhinein durch einen Index ersetzt, sodass eine klassische Berechnung über den originären Levenshtein Algorithmus erfolgen konnte. Dieser vereinfachte Ansatz wies eine deutlich höhere Performanz auf.</p>
            <p>
               <figure>
                  <graphic url=""182_final-fb5700ffa1243a1d53a1457f2d7a4309.png""/>
               </figure>Abbildung 3: Berechnung der Levenshtein Distanz für ein Beispiel Wortpaar mit Levenshtein2. Buchstabenkombinationen werden durch einen modifizierten Algorithmus berücksichtigt.
                </p>
            <p>
               <figure>
                  <graphic url=""182_final-133df5793c8187a7712107c0486249e9.png""/>
               </figure>Abbildung 4: Berechnung der Levenshtein Distanz für ein Beispiel Wortpaar mit Levenshtein2Simple. Buchstabenkombinationen werden vorab auf einen Index gematcht.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Experimente und Ergebnisse</head>
            <p>Die arabische Version des Suchalgorithmus wurde auf zwei Wörterbüchern getestet. Das erste Wörterbuch beinhaltete Toponyme von archäologischen Fundorten in Syrien, im Irak und in der Türkei, welche aus dem TEXTELSEM Repositorium des i3Mainz stammten 
                (tts_arch)<ref target=""ftn4"" n=""4""/>. Das zweite Wörterbuch beinhaltete syrische Toponyme aus GeoNames 
                (geo_SY)<ref target=""ftn5"" n=""5""/>. Zusätzlich wurde die Übertragbarkeit des Suchalgorithmus auf andere Sprachen auf einem sumerischen Wörterbuch getestet, das aus dem ""Semantic Dictionary for Ancient Languages"" extrahiert wurde 
                (sum)<ref target=""ftn6"" n=""6""/>. Alle Wörterbücher wurden in ein Trainings- sowie ein Testkorpus aufgeteilt. Gemessen wurde die Mean Average Precision (MAP) bei einer Rückgabe der Ergebnisse in Form eines Rankings. Da die durchgeführten Tests so konzipiert waren, dass jeweils nur ein Ergebnis als zutreffend gewertet wurde, genügte für jedes Suchwort die Berechnung eines Präzisionswertes, der anschließend über alle Testsuchwörter gemittelt wurde. Die Ergebnisse unserer Tests sind in Tabelle 1 festgehalten. Sie zeigen, dass unser Algorithmus in der Lage war, Toponyme mit einer Präzision zwischen 90% und 95% abhängig vom Wörterbuch zu finden. Verglichen mit einem ungewichteten Levenshtein Distanzmaß kann unser Ansatz somit eine Verbesserung der Präzision zwischen 9 Prozentpunkten auf dem sumerischen Wörterbuch und 27 Prozentpunkten auf dem TEXTELSEM Wörterbuch erreichen.
                </p>
            <table rend=""frame"" id=""Table2"">
               <row>
                  <cell>Datenset</cell>
                  <cell>MAP bei Volltextsuche</cell>
                  <cell>MAP bei ungewichteter Levenshtein Distanz</cell>
                  <cell>MAP bei eigenem Algorithmus (beste Version)</cell>
                  <cell>Algorithmus</cell>
               </row>
               <row>
                  <cell>tts_arch</cell>
                  <cell>0.24</cell>
                  <cell>0.63</cell>
                  <cell>0.90</cell>
                  <cell>Levenshtein2Simple</cell>
               </row>
               <row>
                  <cell>geoSY_xs</cell>
                  <cell>0.01</cell>
                  <cell>0.81</cell>
                  <cell>0.95</cell>
                  <cell>Levenshtein1</cell>
               </row>
               <row>
                  <cell>Sum</cell>
                  <cell>0.01</cell>
                  <cell>0.83</cell>
                  <cell>0.92</cell>
                  <cell>Levenshtein2Simple</cell>
               </row>
            </table>
            <p>Tabelle 1: Testergebnisse. Die Tests zeigen, dass die Levenshtein2Simple Version des Algorithmus im allgemeinen Fall eine bessere Präzision sowie die beste Performanz aufweisen konnte.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Zusammenfassung</head>
            <p>Unsere Version der gewichteten Levenshtein Distanz erwies sich als ein vielversprechender Ansatz für die Verbesserung von Suchergebnissen in digitalen Gazetteeren. Zusätzlich konnten wir durch die Anwendung des Algorithmus auf das sumerische Keilschriftwörterbuch die Übertragbarkeit des Algorithmus auf andere Sprachen demonstrieren. Obwohl die vorgeschlagene Adaption des Levenshtein Algorithmus für sumerische Keilschrift erfolgreich war, könnten in anderen Fällen möglicherweise neue Probleme auftreten. Da der Algorithmus bisher nur Kombinationen aus zwei Buchstaben berücksichtigt, würde er nicht für Transliterationen funktionieren, die auch Kombinationen aus mehr als zwei Buchstaben enthalten, beispielsweise für die Transliteration des kyrillischen Alphabets, die Kombination wie ``shtsh'' für den kyrillischen Buchstaben Щ enthält. Für Fälle wie diesen müsste der Ansatz weiterentwickelt werden. Darüber hinaus wäre zu überlegen, inwieweit die Performanz des Algorithmus weiter verbessert werden könnte. Durch die Verwendung eines Burkhard-Keller-Baumes konnte die Performanz immerhin so weit gesteigert werden, dass die Suchzeit auf einem Testkorpus mit über 35.000 Einträgen auf unter eine halbe Sekunde im Durchschnitt reduziert wurde. Für die Verwendung mit größeren Wörterbüchern könnte jedoch eine weitere Verbesserung der Performanz wünschenswert sein. Als Möglichkeit hierfür wäre etwa die Verwendung eines Levenshtein-Automaten nach Schulz / Mihov (2002) zu prüfen, der als besonders effiziente Umsetzung des Levenshtein-Algorithmus gilt.</p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" rend=""footnote text"">
               <ptr target=""https://pinyin.sogou.com/""/>
            </note>
            <note id=""ftn2"" n=""2"" rend=""footnote text"">
               <ptr target=""https://www.google.co.jp/ime/""/>
            </note>
            <note id=""ftn3"" n=""3"" rend=""footnote text"">
               <ptr target=""https://gitlab.rlp.net/mscherl/FuzzySearch""/>
            </note>
            <note id=""ftn4"" n=""4"" rend=""footnote text"">
               <ptr target=""http://www.higeomes.org/""/>
            </note>
            <note id=""ftn5"" n=""5"" rend=""footnote text"">
               <ptr target=""http://www.geonames.org/""/>
            </note>
            <note id=""ftn6"" n=""6"" rend=""footnote text"">
               <ptr target=""https://situx.github.io/SemanticDictionary/""/>
            </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Brockelmann, C. / Fischer, A. / Heffening, W. / Taeschner, F. (1935)</hi>: <hi rend=""italic"">Die Transliteration der arabischen Schrift in ihrer Anwendung auf die Hauptliteratursprachen der islamischen Welt. Denkschrift dem 19. Internationalen Orientalistenkongreß in Rom.</hi>
               </bibl>
               <bibl>
                  <hi rend=""bold"">De La Briandais, R. (1959)</hi>: <hi rend=""italic"">File searching using variable length keys</hi>. In: Papers presented at the the March 3-5, 1959, western joint computer conference. pp. 295–298. ACM.
					</bibl>
               <bibl>
                  <hi rend=""bold"">Homburg, T. (2017)</hi>: <hi rend=""italic"">Postagging and semantic dictionary creation for hittite cuneiform</hi>. In: DH2017 .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Homburg, T. (2018)</hi>: <hi rend=""italic"">Semantische Extraktion auf antiken Schriften am Beispiel von Keilschriftsprachen mithilfe semantischer Wörterbücher</hi>. In: Dhd2018 .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Homburg, T., Chiarcos, C. (2016)</hi>: <hi rend=""italic"">Word segmentation for akkadian cuneiform</hi>. In: LREC 2016 .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Homburg, T., Chiarcos, C., Richter, T., Wicke, D. (2015)</hi>: <hi rend=""italic"">Learning cuneiform the modern way</hi>, http://gams.uni-graz.at/o:dhd2015.p.55 .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Krueger, M.H., Neeson, K.D. (2000)</hi>: <hi rend=""italic"">Japanese text input method using a limited roman character set</hi>, uS Patent 6,098,086 
					</bibl>
               <bibl>
                  <hi rend=""bold"">Kukich, K. (1992)</hi>: <hi rend=""italic"">Techniques for automatically correcting words in text</hi>. ACM Computing Surveys 24,4 .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Lasko, T.A., Hauser, S.E. (2001)</hi>: <hi rend=""italic"">Approximate string matching algorithms for limited-vocabulary ocr output correction</hi>, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.1064&rep=rep1&type=pdf .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Levenshtein, V.I. (1966)</hi>: <hi rend=""italic"">Binary codes capable of correcting deletions, insertions, and reversals</hi>. Soviet Physics Doklady 10,8 .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Pedersen, T.T. (2008)</hi>: <hi rend=""italic"">Transliteration of arabic</hi>, http://transliteration.eki.ee/pdf/Arabic_2.2.pdf .
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Schlott-Kotschote, A. (2004)</hi>: <hi rend=""italic"">Transkription arabischer Schriften. Vorschläge für eine einheitliche Umschrift arabischer Bezeichnungen.</hi>
               </bibl>
               <bibl>
                  <hi rend=""bold"">Schulz, K.U., Mihov, S. (2002)</hi>: <hi rend=""italic"">Fast string correction with levenshtein automata</hi>. International Journal on Document Analysis and Recognition 5 .
					</bibl>
               <bibl>
                  <hi rend=""bold"">UNGEGN Working Group, R.S. (2016)</hi>: <hi rend=""italic"">Arabic. report on the current state of united nations romanization systems for geographical names</hi>. version 4.0,<ptr target=""http://www.eki.ee/wgrs/rom1_ar.pdf""/> .
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Weigel, A., Baumann, S., Rohrschneider, J. (1995)</hi>: <hi rend=""italic"">Lexical postprocessing by heuristic search and automatic determination of the edit costs</hi>. In: Proceedings of the Third International Conference on Document Analysis and Recognition .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Zgank, Kacic (2012)</hi>: <hi rend=""italic"">Predicting the acoustic confusability between words for a speech recognition system using levenshtein distance</hi>, http://eejournal.ktu.lt/index.php/elt/article/download/2628/1917 .
					</bibl>
               <bibl>
                  <hi rend=""bold"">Ziółko, B., Gałka, J., Skurzok, D., Jadczyk, T. (2010)</hi>: <hi rend=""italic"">Modified weighted levenshtein distance in automatic speech recognition</hi>, http://www.dsp.agh.edu.pl/_media/pl:bziolko_kkzmbm2010final.pdf .
					</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,arabische schrift;gazeteer;keilschrift;levenshteindistanz;machine learning;nlp;ortsnamen,German,karte;methoden;software;sprache;text;transkription
10909,2019 - Johannes Gutenberg University;Goethe University,Johannes Gutenberg University;Goethe University,multimedial & multimodal,2019,DHd,DHd,Johannes Gutenberg-Universität Mainz (Johannes Gutenberg University of Mainz);Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt),Frankfurt & Mainz,,Germany,https://dhd2019.org/,Der TextImager als Front- und Backend für das verteilte NLP von Big Digital Humanities Data,,Wahed Hemati;Alexander Mehler;Tolga Uslu;Giuseppe Abrami,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p style=""text-align:left; "">Immer mehr Disziplinen benötigen Natural Language Processing (NLP) Werkzeuge, um automatische Textanalysen auf verschiedenen Ebenen der Sprache durchzuführen. Die Anzahl der NLP-Werkzeuge wächst rasant<hi rend=""superscript"">1</hi>. Auch die Anzahl der frei oder anderweitig zugänglichen Ressourcen wächst. Angesichts dieser wachsenden Zahl an Werkzeugen und Ressourcen ist es schwierig, den Überblick zu behalten; gleichzeitig ist ein Computational-Linguistic-Framework, das große Datenmengen aus verschiedenen Quellen verarbeiten kann, noch nicht etabliert. Ein solches Framework sollte in der Lage sein, Daten verteilt zu verarbeiten und gleichzeitig eine standardisierte Programmier- und Modellschnittstellebereitzustellen. Darüber hinaus sollte es modular und leicht erweiterbar sein, um die ständig wachsende Palette neuer Ressourcen und Tools zu integrieren. Das Framework muss offen genug für Erweiterungen Dritter sein, wobei jede Erweiterung für die gesamte Community zugänglich bleibt. Das Framework sollte es zudem Dritten ermöglichen, den Zugang zu ihren Erweiterungen zu beschränken, wenn dies beispielsweise durch Urheber-recht, geistiges Eigentum oder Datenschutz erforderlich ist. Um diesen Anforderungen gerecht zu werden, haben wir den TextImager (Hemati 2016, Mehler et al. 2018) um ein verteiltes Serversystem mit Cluster-Computing-Funktionen auf der Basis von UIMA (Ferrucci andLally 2004) weiterentwickelt. </p>
         <p style=""text-align:left; "">UIMA ist ein Framework zur Verwaltung von Datenflüssen zwischen Komponenten. Es bietet standardisierte Interfaces zur Erstellung von Komponenten an. Dabei können die Komponenten einzeln oder im Verbund in einer Pipeline-Struktur ausgeführt werden. UIMA bietet weitgehende Möglichkeiten der sequenziellen Ordnung von NLP-Werkzeugen und verspricht, auch in Zukunft von der Community weiterentwickelt zu werden: Prozess-Management auf der Basis von UIMA erscheint nach derzeitigem Stand daher als erste Wahl im Bereich von NLP und DH. </p>
         <p style=""text-align:left; "">TextImager bietet eine Vielzahl von UIMA-basierten NLP-Komponenten an, darunter unter anderen einen Tokenizierer, einen Lemmatisierer, einen Part-Of-Speech-Tagger, einen Named-Entity-Parser und einen Dependency Parser, und zwar für eine Vielzahl von Sprachen, darunter Deutsch, Englisch, Französisch und Spanisch. Dieses Spektrum an Werkzeugen besteht allerdings nicht ausschließlich aus Eigenentwicklungen, sondern wird maßgeblich um Entwicklungen Dritter erweitert, wozu unter anderem die Tool-Palette von Stanford CoreNLP (Manning 2014), OpenNLP (OpenNLP 2010) und DKpro (Eckart de Castilho 2014) zählen. </p>
         <p style=""text-align:left; "">In Zeiten von Big Data wird es immer relevanter, Daten schnell zu verarbeiten. Ausdiesem Grund ist TextImager als Multi-Server- und zugleich als Multi-Instanz-Clusteraufgebaut, um das verteilte Verarbeiten von Daten zu ermöglichen. Dafür setzt TextImager auf UIMAs Cluster-Management-Dienste UIMA-AS<hi rend=""superscript"">2</hi> und UIMA-DUCC<hi rend=""superscript"">3</hi> auf. </p>
         <figure>
            <graphic n=""1001"" width=""16.002cm"" height=""9.381086111111111cm"" url=""218_final-b2fe77dd0ebb1a61bf6b757024e4779b.png"" rend=""inline""/>
            <head>
               <lb/>Abbildung 1
				</head>
         </figure>
         <p style=""text-align:left; "">Abbildung 1 zeigt eine schematische Darstellung von TextImager. Jede NLP-Komponente läuft als UIMA-AS Webservice auf dem Computing-Cluster des TextImager. Dabei können mehrere Instanzen einer Komponente instanziiert (s. Abbildung1, Service Instances) werden und dennoch über eine Webservice-Schnittstelle (s. Abbildung1, UIMA AS Services) angesprochen werden. Dazu wird das Java Messaging Service (JMS) verwendet, das die Kommunikation zwischen verschiedenen Komponenten einer verteilten Anwendung ermöglicht. JMS implementiert ein Point-to-Point-Kommunikationssystem. Dieser Kommunikationstyp basiert auf dem Konzept der message queues (Warteschlangen), senders (Sender) und receivers (Empfänger). Jedem Dienst ist eine Eingabewarteschlange und eine Ausgabewarteschlange zugeordnet. Um mehrere Instanzen einer Komponente zu verteilen, verbinden sich die Instanzen mit der gleichen Service-Eingangswarteschlange. Die Instanzen erhalten aus dieser Warteschlange Arbeitseinheiten. Nach der Verarbeitung wird das Ergebnis an eine Ausgabewarteschlange zurückgegeben. Die Ausgabewarteschlange eines Dienstes kann an eine Eingabewarteschlange eines anderen Dienstes angeschlossen werden, um eine Pipeline zu erstellen. Aufgrund dieser Ein- und Ausgabewarteschlangen-Systematik kann jeder Service Arbeitseinheiten asynchron bearbeiten. Durch diese Architektur ist TextImager eine Multi-Server-, Multi-Service- und Multi-Service-Instanz-Architektur.</p>
         <p style=""text-align:left; "">Darüber hinaus bietet TextImager ein Toolkit, das es jedem Entwickler ermöglicht, einen eigenen TextImager-Cluster aufzusetzen und Services im TextImager-System hinzuzufügen. Entwickler können den Zugriff auf die Dienste einschränken, wenn dies wie oben beschrieben erforderlich ist, was mittels die Integration des ResourceManagers (Gleim 2012) und des AuthorityManagers (Gleim 2012) realisiert wird.</p>
         <p style=""text-align:left; "">Durch Freigabe des Quellcodes des TextImager und die Bereitstellung von Leitlinien für dessen Erweiterung wollen wir es Dritten ermöglichen, ihre NLP-Software über die Webservices von TextImager zu vertreiben, so dass die gesamte wissenschaftliche Gemeinschaft davon profitiert. </p>
         <p style=""text-align:left; "">Installationsanweisungen und Beispiele für die Einrichtung eines TextImager-Servers finden Nutzer in folgendem GitHub-Repository: https://github.com/texttechnologylab/textimager-server.</p>
         <p style=""text-align:left; "">Der Beitrag erörtert die Möglichkeiten und Grenzen des NLP von Big Data, stellt den TextImager als Werkzeug für diesen Bereich zur Diskussion und zeigt anhand von drei Nutzungsszenarien Einsatzmöglichkeiten in den DH auf.</p>
      </body>
      <back>
         <div type=""notes"">
            <note place=""foot"" id=""ftn1"" n=""1""> https://github.com/topics/nlp</note>
            <note place=""foot"" id=""ftn2"" n=""2""> https://uima.apache.org/doc-uimaas-what.html</note>
            <note place=""foot"" id=""ftn3"" n=""3""> https://uima.apache.org/doc-uimaducc-whatitam.html</note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">de Castilho, Richard Eckart / Gurevych, Iryna (2014):</hi>
                  <hi rend=""italic"">A broad-coverage collection of portable NLP components for building shareable analysis pipelines</hi>, 
                        in: Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT 1–11.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Ferrucci, David / Lally, Adam (2004):</hi>
                  <hi rend=""italic"">UIMA: an architectural approach to unstructured information processing in the corporate research environment.</hi>, 
                        in: 
                        Natural Language Engineering 10(3-4) 327–348.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Gleim, Rüdiger / Mehler, Alexander / Ernst, Alexandra (2012):</hi>
                  <hi rend=""italic"">SOA implementation of the ehumanities desktop</hi>, 
                        in: Proceedings of the Workshop on Service-oriented Architectures (SOAs) for the Humanities: Solutions and Impacts.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Mehler, Alexander / Hemati, Wahed / Gleim, Rüdiger / Baumartz, Daniel (2018):</hi>
                  <hi rend=""italic"">VienNA: Auf dem Weg zu einer Infrastruktur für die verteilte interaktive evolutionäre Verarbeitung natürlicher Sprache</hi>,
						in Forschungsinfrastrukturen und digitale Informationssysteme in der germanistischen Sprachwissenschaft , H. Lobin, R. Schneider, and A. Witt, Eds., Berlin: De Gruyter, 2018, vol. 6. 
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Hemati, Wahed / Uslu, Tolga / Mehler, Alexander (2016):</hi>
                  <hi rend=""italic"">Textimager: a distributed uima-based system for nlp</hi>, 
                        in: Proceedings of the COLING 2016 System Demonstrations. FederatedConference on Computer Science and Information Systems.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Manning, Christopher / Surdenu, Mihai / Bauer, John / Finkel, Jenny / Bethard, Steven / McClosky, David (2014):</hi>
                  <hi rend=""itliac"">The stanford CoreNLP natural language processing toolkit</hi>, 
                        in: Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations 55–60.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">OpenNLP (2010):</hi>
                  <hi rend=""italic"">Apache OpenNLP</hi>, in: <ref target=""http://opennlp.apache.org"">http://opennlp.apache.org</ref> [letzter Zugriff 05. Oktober 2018]
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,big data;nlp;visualisierung,German,annotieren;programmierung;strukturanalyse;veröffentlichung;visualisierung;webentwicklung
10942,2019 - Johannes Gutenberg University;Goethe University,Johannes Gutenberg University;Goethe University,multimedial & multimodal,2019,DHd,DHd,Johannes Gutenberg-Universität Mainz (Johannes Gutenberg University of Mainz);Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt),Frankfurt & Mainz,,Germany,https://dhd2019.org/,Texterkennung mit Ocropy – Vom Bild zum Text,,Robert Nasarek;Andreas Müller,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Das OCR-Programm ocropy</head>
            <p style=""text-align:left; "">Die optische Zeichenerkennung (engl. Optical Character Recognition – OCR) von historischen Texten weißt oftmals niedrige Erkennungsraten auf. Mit einem gekonnten Preprozessing und ocropy (auch ocropus), einem modular aufgebauten Kommandozeilenprogramm auf Basis eines neuronalen long short-term memory Netzes, ist es möglich, deutlich bessere Ergebnisse zu erzielen. (Springmann 2015, S. 3; Vanderkam 2015) Ocropy ist in Python geschrieben und enthält u. a. Module zur Binarisierung (Erzeugung einer Rastergrafik), zur Segmentierung (Dokumentaufspaltung in Zeilen), zur Korrektur fehlerhafter Erkennungstexte, zum Training neuer Zeichen und natürlich zur Erkennung von Dokumenten (siehe Abbildung 1). Ein bedeutender Vorteil dabei ist, dass jedes Modul eine Reihe von nachvollziehbaren Einstellungsmöglichkeiten hat, um auf die individuellen Herausforderungen jedes Dokumentes einzugehen. Zusätzlich besteht die Möglichkeit ocropy auf die Erkennung einer bestimmten Schriftart, bzw. eines Zeichensatzes zu trainieren.</p>
            <figure>
               <graphic n=""1001"" width=""9.992422222222222cm"" height=""12.2cm"" url=""264_final-4b6e208c0125ad6b7e1118e2bbf7cd93.png"" rend=""inline""/>
               <head>Abbildung 1. Überblick zum Prozessablauf der Texterkennung mit den grundlegenden Software-Modulen</head>
            </figure>
            <p style=""text-align:left; "">Die Benutzung von ocropy als Kommandozeilenprogramms setzt jedoch den Umgang mit einer Consolen-Umgebung und eine grundlegende Kenntnis von Bash-Kommandos voraus: Für viele potenzielle NutzerInnen stellt dies eine erste Einstiegshürde dar, denn der NutzerInnenanteil von Linuxderivaten beträgt nur 3% (statista 2018), wobei die Gruppe an ShelluserInnen noch kleiner sein dürfte. Im Workshop wird diese Hürde abgebaut, indem alle Schritte „from zero to recognised textfile“ nachvollziehbar und zum Mitmachen aufzeigt wird. Insgesamt werden sechs Themengebiete behandelt, damit die TeilnehmerInnen des Workshops alle benötigten Informationen erhalten, um selbstständig Frakturschriften (oder andere Schriftarten) durch ocropy erkennen zu lassen.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Ubuntu in der VirtualBox</head>
            <p style=""text-align:left; "">Für bisher ausschließliche NutzerInnen des Betriebssystem Windows oder Mac OS, ist es unverhältnismäßig, allein wegen ocropy Linux als Zweit- oder sogar Hauptsystem zu installieren. Durch die Verwendung einer 
                    <ref target=""https://www.virtualbox.org/"">VirtualBox</ref> und des Linux-Derivats 
                    <ref target=""https://www.ubuntu.com/"">Ubuntu</ref> kann dieser Schritt umgangen werden. Mit Hilfe einer virtuellen Maschine lässt sich ein Betriebssystem innerhalb eines anderen Betriebssystems emulieren. Das bringt den Vorteil mit sich, keine größeren Änderungen am System vornehmen zu müssen und die Software in einem geschützten virtuellen Rahmen testen zu können. Das Einrichten einer virtuellen Maschine ist daher für die meisten NutzerInnen das Fundament (und vielleicht auch der Einstieg) in Unix-basierte Entwicklerumgebungen. Dabei sind diverse kleinere Einstellungen zu beachten, vom Einschalten der Virtualisierung im BIOS bis hin zur Installation von gemeinsam genutzten Ordnern zwischen Host und Gast. Ubuntu als „Einstiegslinux“ eignet sich hervorragend für die ersten Schritte, da es eine hohe Benutzerfreundlichkeit aufweist und trotzdem alle wichtigen Features mitbringt, die benötigt werden.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Repositorien für brauchbare Digitalisate</head>
            <p style=""text-align:left; "">OCR-Software erzielt bessere Ergebnisse mit hochauflösenden und fehlerfreien Digitalisaten. Bilddateien sollten mindestens eine Auflö<ref target=""http://www.zvdd.de/startseite/"">
                  <hi rend=""italic"">zentrale Verzeichnis digitalisierter Drucke</hi>
               </ref> oder das 
                    <ref target=""https://www.digitale-sammlungen.de/"">
                  <hi rend=""italic"">Münchener DigitalisierungsZentrum</hi>
               </ref> bieten exzellente Anlaufstellen zur Beschaffung digitalisierter Drucke; aber auch Sammlungen wie das 
                    <ref target=""http://digitale.bibliothek.uni-halle.de/"">
                  <hi rend=""italic"">Verzeichnis der im deutschen Sprachbereich erschienen Drucke des 16. - 19. Jahrhunderts</hi>
               </ref> der Universität- und Landesbibliothek Sachsen-Anhalt verfügen über frei zugängliche Digitalisate mit einer Auflösung bis zu 600 DPI. 
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Installation von ocropy</head>
            <p style=""text-align:left; "">Ocropy ist nicht in den nativen Quellen von den bekanntesten Linux-Derivaten enthalten, sondern muss von 
                    <ref target=""https://github.com/tmbdev/ocropy"">Github</ref> heruntergeladen und über ein Script installiert werden. Dabei ist die Version der Programmiersprache Python 2.7 zu beachten und die Abhängigkeiten einiger benötigter Module. Im Workshop wird die Installation begleitet und ein bereits auf Drucke des 18. Jahrhundert trainiertes Erkennungsmodul zur Verfügung gestellt.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Preprocessing mit ScanTailor</head>
            <p style=""text-align:left; "">Eine Texterkennung ist nur so gut wie das Preprocessing des Digitalisates. Bilder, Initiale oder Flecken im Bild stören die Texterkennung und müssen entfernt werden. Darüber hinaus benötigt ocropy binarisierte (schwarz/weiß gerasterte) oder normalisierte Graustufenbilder zur Verarbeitung. Obwohl ocropy mit dem Modul ocropus-nlbin eine eigene Lösung zur Binarisierung von Bilddateien anbietet, hilft dies nicht in Bezug auf Nicht-Text-Elemente, wie Bilder oder schräge Spaltenlinien. Bearbeitungssoftware wie Gimp beinhaltet zwar alle benötigten Funktionen, ist jedoch in Bezug auf die serielle Verwendung bei Textdigitalisaten ineffizent. Im Workshop wird die Software 
                    <ref target=""http://scantailor.org/"">ScanTailor</ref> als passgenaues Preprocessing-Tool zur Vorbereitung der Digitalisate favorisiert. ScanTailor ist wie dafür gemacht gescannte Texte in eine einheitliche Form zu bringen und beinhaltet (zum Teil vollständig automatisierte) Funktionen wie 
                </p>
            <list type=""unordered"">
               <item>der Aufsplittung von Spalten oder Seiten</item>
               <item>das Ausrichten der Seite</item>
               <item>des Auswählens des Inhalts</item>
               <item>der Möglichkeit Bereich zu füllen</item>
               <item>der Entzerrung gekrümmter Seiten und</item>
               <item>der Anpassung des Schwellwertes (threshold) bei der Binarisierung.</item>
            </list>
            <p style=""text-align:left; "">Außerdem werden Hinweise zu den grundlegenden Eigenschaften eines guten Eingangsbildes gegeben, z. B. in Bezug auf Schwellwert oder DPI-Zahl.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Entwicklung einer Pipeline zur Texterkennung</head>
            <p style=""text-align:left; "">Die ocropy-Module funktionieren am effizientesten innerhalb einer Pipeline. Ausgehend von der Konvertierung unpassender Dateiformate der Roh-Digitalisate bis hin zur Erstellung einer Korrektur-HTML für die Verbesserung der falsch erkannten Zeichen bietet die Linux-Shell zusammen mit ocropy und dem Programm 
                    <ref target=""https://www.imagemagick.org/script/index.php"">ImageMagick</ref> alle benötigten Werkzeuge. So lassen sich auch große Mengen an Bilddateien stapelweise verarbeiten. In einem Script werden Befehle zur Bildkonvertierung, Zeilenauftrennung, Texterkennung und Textkonvertierung in Reihe geschaltet, um eine stapelhafte Verarbeitung zu ermöglichen. Der Workshop bietet zwei vorgefertigte Scripte zum Gebrauch an und erklärt ihren Ablauf, um eventuelle Anpassungen an die eigenen Bedürfnisse vornehmen zu können.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Training unbekannter Schriftarten</head>
            <p style=""text-align:left; "">Die eigentliche Stärke von ocropy ist die Möglichkeit Erkennungsmodule für Schriftarten zu trainieren. Die dazu bereitgestellte Ground Truth Data bestimmt maßgeblich die Leistungsfähigkeit der Erkennungsmodule. Dabei stellt sich die Frage, wie eine gute Ground Truth im wörtlichen Sinne auszusehen hat? Wie „schmutzig“ dürfen die Daten sein? Sind abgeschnittene Serifen, fehlende Bögen oder i-Punkte ein Problem? Welche Zeichen sollten verwendet werden, um Abbreviationen oder Abkürzungszeichen zu kodieren? Darüber hinaus trainiert ocropy sich nicht permanent besser, sondern baut das neurale Netz zeitweise mit negativen Auswirkungen für die Erkennungsraten um (siehe Abbildung 2). Im Workshop wird ein Script zur Identifikation des besten Trainingsmoduls vorgestellt, um das Beste aus ocropy herauszuholen.</p>
            <figure>
               <graphic n=""1002"" width=""16.002cm"" height=""10.341680555555556cm"" url=""264_final-39449017b82e85831a89c8f063ec7e8c.png"" rend=""inline""/>
               <head>
                  <lb/>Abbildung 2. Trainingsprozess von ocropus-rtrain mit Ground Truth von Zedlers Universallexikon. training = Ground Truth anhand derer das Modul trainiert wurde, testing = unbekanne Ground Truth zum Test der Performance.
					</head>
            </figure>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Ablauf</head>
            <p style=""text-align:left; "">Der Workshop richtet sich vorrangig an Anfänger und leicht fortgeschrittene NutzerInnen im Umgang mit Linux und der Console. Es werden keine Vorkenntnisse in Bash oder Python benötigt und alle im Kurs vorgestellte Software, 
                    <ref target=""https://github.com/rnsrk/occu-toolkit"">Scripte und Daten</ref> stehen frei zur Verfügung. Der Workshop möchte alle an Interessierten da abholen, wo sie stehen und versucht durch ein schrittweises Vorgehen an die Vorzüge der Consolen-Benutzung und kommandozeilenbasierte Software heranzuführen. Teilnehmer sollten ihr eigenes Notebook mitbringen, auf dem sie auch Administrator-Rechte besitzen. Des Weiteren wird ein Internetzugang benötigt, um fehlende Software oder Abhängigkeiten herunterladen zu können. Größere Softwarepakete (VirtualBox, Ubuntu) werden auch auf USB-Sticks zur Verfügung gestellt, sollten aber nach Möglichkeit vorher selbstständig heruntergeladen werden. Es können je nach Erfahrungsstand der TeilnehmerInnen mit Console und Linux 20 bis 25 Personen betreut werden. Der Workshop dauert drei bis vier Stunden.
                </p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">ImageMagick (2018):</hi>
                  <hi rend=""italic"">Convert, Edit, Or Compose Bitmap Images @ ImageMagick</hi>, 
                        URL: 
                        <ref target=""https://www.imagemagick.org/"">https://www.imagemagick.org/</ref>, [zuletzt besucht am 14.10.2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">MDZ (2018):</hi>
                  <hi rend=""italic"">Münchner DigitalisierungsZentrum</hi>, 
                        Bayerische Staatsbibliothek, München, URL: 
                        <ref target=""https://www.digitale-sammlungen.de/"">https://www.digitale-sammlungen.de/</ref>, [zuletzt besucht am 12.10.2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">ocropy (2018):</hi>
                  <hi rend=""italic"">Python-based tools for document analysis and OCR</hi>, 
                        URL: 
                        <ref target=""https://github.com/tmbdev/ocropy"">https://github.com/tmbdev/ocropy</ref>, [zuletzt besucht am 14.10.2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">ScanTailor (2018):</hi>
                  <hi rend=""italic"">ScanTailor</hi>, 
                        <ref target=""http://scantailor.org/"">http://scantailor.org/</ref>, [zuletzt besucht am 14.10.2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Springman, Uwe (2015):</hi>
                  <hi rend=""italic"">Ocrosis. A high accuracy OCR method to convert early printings into digital text, Center for Information and Language Processing (CIS)</hi>, 
                        Ludwig-Maximilians-University, Munich, URL: 
                        <ref target=""http://cistern.cis.lmu.de/ocrocis/tutorial.pdfm"">http://cistern.cis.lmu.de/ocrocis/tutorial.pdfm</ref> [zuletzt besucht am 14.10.2018].
                    </bibl>
               <bibl style=""text-align:left; "">statista, Marktanteile der führenden Betriebssysteme in Deutschland von Januar 2009 bis Juli 2018, URL: https://de.statista.com/statistik/daten/studie/158102/umfrage/marktanteile-von-betriebssystemen-in-deutschland-seit-2009/, [zuletzt besucht am 10.10.2018].</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Vanderkam, Dan (2015):</hi>
                  <hi rend=""italic"">Extracting text from an image using Ocropus</hi>, 
                        URL: http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html, [zuletzt besucht am 10.10.2018].</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">VD (2018):</hi>
                  <hi rend=""italic"">Digitale Sammlungen des 16. bis 19. Jahrhunderts</hi>, 
                        Universitäts- und Landesbibliothek Sachsen-Anhalt, Halle (Saale), URL: 
                        <ref target=""http://digitale.bibliothek.uni-halle.de/"">http://digitale.bibliothek.uni-halle.de/</ref>, [zuletzt besucht am 14.10.2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">VirtualBox (2018):</hi>
                  <hi rend=""italic"">Oracle VM VirtualBox</hi>, 
                        URL: 
                        <ref target=""https://www.virtualbox.org/"">https://www.virtualbox.org/</ref>, [zuletzt besucht am 14.10.2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Ubuntu (2018):</hi>
                  <hi rend=""italic"">The leading operating system for PCs, IoT devices, servers and the cloud | Ubuntu</hi>, 
                        URL: 
                        <ref target=""https://www.ubuntu.com/"">https://www.ubuntu.com/</ref>, [zuletzt besucht am 14.10.2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">ZVDD (2018):</hi>
                  <hi rend=""italic"">Zentrales Verzeichnis Digitalisierter Drucke</hi>, 
                        Georg August Universität Göttingen, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Göttingen, URL: 
                        <ref target=""http://www.zvdd.de/"">http://www.zvdd.de/</ref>, [zuletzt besucht am 12.10.2018].
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,imagepreprocessing;ocr;ocropy;virtualmachine,German,bilder;datenerkennung;text;übersetzung;umwandlung
10943,2019 - Johannes Gutenberg University;Goethe University,Johannes Gutenberg University;Goethe University,multimedial & multimodal,2019,DHd,DHd,Johannes Gutenberg-Universität Mainz (Johannes Gutenberg University of Mainz);Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt),Frankfurt & Mainz,,Germany,https://dhd2019.org/,Vom gedruckten Werk zu elektronischem Volltext als Forschungsgrundlage. Erstellung von Forschungsdaten mit OCR-Verfahren,,Matthias Boenig;Elisa Herrmann;Volker Hartmann,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div>
            <head>EINLEITUNG</head>
            <p>In den vergangenen 30 Jahren ist ein beträchtlicher Teil des in Deutschland gedruckten Materials aus der Zeit von 1500 bis ca. 1850 in mehreren, durch die Deutsche Forschungsgemeinschaft (DFG) geförderten Kampagnen in den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD16, VD17, VD18) zunächst nachgewiesen und seit 2006 digitalisiert worden. Zusätzlich vorliegender Volltext wird mittlerweile auf breiter disziplinärer Front als Schlüssel zu einer ganzen Reihe von geistes- und kulturwissenschaftlichen Forschungsfragen gesehen und gilt zunehmend als elementare Voraussetzung für die Weiterentwicklung der transdisziplinär arbeitenden Digital Humanities. Deshalb werden bereits an verschiedenen Stellen OCR-Verfahren angewendet; viele dieser Unternehmungen haben allerdings noch sehr starken Projektcharakter. Die informationswissenschaftliche Auseinandersetzung mit OCR kann an der großen Zahl wissenschaftlicher Studien und Wettbewerbe ermessen werden, die Möglichkeiten zur Verbesserung der Textgenauigkeit sind in den letzten Jahrzehnten enorm gestiegen. Der Transfer der auf diesem Wege gewonnenen, oftmals sehr vielversprechenden Erkenntnisse in produktive Anwendungen ist jedoch häufig nicht gegeben: Es fehlt an leicht nachnutzbaren Anwendungen, die eine qualitativ hochwertige Massenvolltextdigitalisierung aller historischen Drucke aus dem Zeitraum des 16. bis 19. Jahrhundert ermöglichen. 
                    <lb/>Auf dem DFG-Workshop „Verfahren zur Verbesserung von OCR-Ergebnissen“ (Deutsche Forschungsgemeinschaft 2014) im März 2014 formulierten Expertinnen und Experten daher folgende Desiderate um die Weiterentwicklung von OCR-Verfahren zu ermöglichen. Es bestehe eine dringende Notwendigkeit für freien Zugang zu historischen Textkorpora und lexikalischen Ressourcen zum Training von vorhandener Software zur Texterkennung bestehe. Ebenso müssen Open-Source-OCR-Engines zur Verbesserung der Textgenauigkeit weiterentwickelt werden, wie auch Anwendungen für die Nachkorrektur der automatisch erstellten Texte. Daneben sollten Workflow, Standards und Verfahren der Langzeitarchivierung mit Blick auf zukünftige Anforderungen an den OCR-Prozess optimiert werden. Als zentrales Ergebnis dieses Workshops stand fest, dass eine koordinierte Fördermaßnahme der DFG notwendig ist. Die „Koordinierte Förderinitiative zur Weiterentwicklung von Verfahren der Optical Character Recognition (OCR)“, kurz OCR-D, begann im September 2015 und versucht seitdem einen Lückenschluss zwischen Forschung und Praxiseinsatz, indem für die Entwicklungsbedarfe Lösungen erarbeitet und der aktuelle Forschungsstand zur OCR mit den Anforderungen aus der Praxis zusammengebracht werden. 
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>ARBEITEN IM PROJEKT OCR-D</head>
            <p>Das Vorhaben hat zum Ziel, einerseits Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einen optimalen Workflow sowie eine möglichst weitreichende Standardisierung von OCR-bezogenen Prozessen und Metadaten zu erzielen, andererseits die vollständige Transformation des schriftlichen deutschen Kulturerbes in digitale Forschungsdaten in (xml-strukturierter Volltext) konzeptionell vorzubereiten. Am Ende des Gesamtvorhabens (d.h. unter Einschluss der Modulprojektphase) sollte ein in allen Aspekten konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des schriftlichen deutschen Kulturerbes stehen und eine Dokumentation, die Antworten auf die damit verbundenen technischen, informationswissenschaftlichen und organisatorischen Probleme und Herausforderungen gibt sowie Rahmenbedingungen formuliert.</p>
            <p>Das Projekt ist in zwei Phasen geteilt: In der ersten Phase hat das Koordinierungsgremium von OCR-D Bedarfe für die Weiterentwicklung von OCR-Technologien analysiert und sich intensiv mit den Möglichkeiten und Grenzen der Verfahren zur Text- und Strukturerkennung auseinandergesetzt. Zahlreiche Gespräche mit ExpertInnen aus Forschungseinrichtungen und Bibliotheken sowie Sichtung vorhandener Werkzeuge aber auch Betrachtung vorhandener Textsammlungen sowie aktueller und geplanter Digitalisierungsvorhaben mündeten in der Erkenntnis, dass der Lückenschluss zwischen Wissenschaft und Praxis das primäre Desiderat im Bereich der Textdigitalisierung darstellt. Zudem hat sich im Lauf der ersten Projektphase eine technologische Wende auf dem Gebiet der Zeichenerkennung vollzogen - an die Stelle traditioneller Verfahren der Mustererkennung, die auf einer Segmentierung von Textabschnitten in Zeilen, Wörter und schließlich einzelne Glyphen basieren, die anschließend aufgrund charakteristischer Merkmale (z.B. Steigung an Kanten) erkannt werden, ist eine zeilenorientierte Sequenzklassifizierung auf Basis statistischer Modelle, insbesondere verschiedener Arten neuronaler Netze (sog. 
                    <hi rend=""italic"">Deep Learning</hi>), getreten. Grund für diesen Technologiewechsel ist die vielfach nachgewiesene Überlegenheit segmentierungsfreier Erkennungsverfahren bezüglich der resultierenden Textgenauigkeit. Diese Überlegenheit gilt insbesondere für schwierige, historische Vorlagen. Dieser Technologiewandel hat sich bisher nicht oder nur äußerst begrenzt auf die Digitalisierungspraxis ausgewirkt. Der Grund dafür liegt vor allem in den bisher bestehenden Hürden beim Einsatz verfügbarer OCR-Lösungen auf Basis neuronaler Netze. Ohne weitreichende projektspezifische Anpassungen ist ein produktiver Einsatz derzeit nicht möglich. Das betrifft unter anderem die Erstellung passender Erkennungsmodelle, die durch das Trainieren eines neuronalen Netzes auf Basis ausgewählter Ground-Truth-Daten generiert werden. Dafür sind zum einen hochqualitativer und umfangreicher Ground Truth aber auch Erfahrungen bzgl. freier Parameter wie z.B. Anzahl der Trainingsschritte, Lernrate, Modelltiefe unabdingbar. Aus OCR-D heraus ist daher ein Datenset mit Trainings- und Ground-Truth-Daten entstanden, welches für Trainings und Qualitätsanalysen im Vorhaben selber genutzt wird aber auch durch andere Forschungsprojekte nachgenutzt werden kann. Neben der Qualität der Zeichenerkennung sind es vor allem Umfang und Korrektheit der strukturellen Annotationen, die die Utilität eines Volltexts für wissenschaftliche Kontexte determinieren. Auch im Bereich der automatischen Layouterkennung (OLR) gab es innerhalb des bisherigen Projektzeitraums vielversprechende Forschungsergebnisse durch den Einsatz innovativer statistischer Verfahren. Der Übertrag in die Praxis in Form nachnutzbarer Software ist hier jedoch noch nicht gegeben. Kommerzielle OCR-Lösungen ignorieren diesen Bereich weitestgehend und bieten nur minimale Strukturinformationen auf Seitenebene (Text, Tabelle, Abbildung etc.) an. Tiefergehende strukturelle Auszeichnungen (Kapitelstruktur, Bildunterschriften, Inhaltsverzeichnisse) werden daher manuell erfasst und in METS/MODS repräsentiert. Eine Verknüpfung zwischen Struktur und Volltext findet, obwohl technisch möglich, in vielen Digitalisierungsvorhaben nicht statt. Für die philologische, editorische oder linguistische Wissenschaftspraxis bedeutet das eine massive Einschränkung die bspw. eine sinnvolle Transformation in hochstrukturierte Formate wie TEI verhindert. 
                </p>
            <p>Die Erkenntnisse dieser Bedarfsanalyse mündeten in einem OCR-D-Funktionsmodell, welches den Rahmen für die Modulprojekt-Ausschreibung der DFG im März 2017 bot. Vor diesem Hintergrund wurden acht Modulprojekte bewilligt die seit 2018 an Lösungen zur Bildvorverarbeitung, Layouterkennung, Textoptimierung (inkl. Nachkorrektur), zum Modelltraining und zur Langzeitarchivierung der OCR-Daten arbeitet. Die Entwicklungen schöpfen dabei das Potential innovativer Methoden für den gesamten Bereich der automatischen Texterkennung für die Massenvolltextdigitalisierung von historischen Drucken aus. Sie werden anschließend nahtlos in den OCR-D-Workflow zur optimierten OCR-basierten Texterfassung integriert. Das so entstehende OCR-D-Softwarepaket steht damit Kultureinrichtungen wie Forschenden für die automatische Texterkennung als Open-Source-Software zur Verfügung.</p>
            <p>Die meisten Arbeiten werden im Sommer 2019 abgeschlossen sein, aber bereits Anfang des Jahres wird die Alpha-Version einen Einblick in die zu erwartende Gesamtlösung bieten können.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>ZIEL DES WORKSHOPS</head>
            <p>Der Workshop soll neben der Vorstellung des Projektes und der Software die Gelegenheit bieten selber die Software zu testen und zugleich über Optimierungen und Anforderungen seitens der Wissenschaft an diese Technologien zu diskutieren. Teilnehmende erhalten somit einen exklusiven Einblick in die Entwicklungsarbeit und haben die Möglichkeit proaktiv auf die Arbeiten Einfluss zu nehmen, die Ihren späteren Forschungsalltag begleiten und verbessern soll.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>PROGRAMM</head>
            <p>Der Workshop gliedert sich in drei Abschnitte:</p>
            <list type=""unordered"">
               <item>Vorstellung des Projekts OCR-D, des Ground-Truth-Datensets und der Guidelines (30min)</item>
               <item>Demonstration der Eigenentwicklung und eines Test-Workflows (120min)</item>
               <item>Diskussion zu Anforderungen und Optimierungen aus Sicht der Digital Humanities (30min)</item>
            </list>
            <p>Der erste Abschnitt stellt die Hintergründe zum Vorhaben vor und geht auf Besonderheiten der Volltextdigitalisierung von historischen Beständen ein. Anschließend wird das Trainings- und Ground-Truth-Datenset präsentiert, das im Rahmen von OCR-D auf- und weiter ausgebaut wird. Besonders die dazu entwickelten Guidelines geben Hinweise für eine spätere Nachnutzung und die Erstellung eigener Ground-Truth-Daten in anderen Projekten. Der Fokus des Workshops liegt auf dem zweiten Abschnitt, in welche der derzeitige Entwicklungsstand präsentiert wird. Die benötigten Test-Dateien werden auf 
                GitHub<hi rend=""superscript"">1</hi> veröffentlicht. Abgerundet wird der Workshop durch eine Diskussionsrunde zu Anforderungen aus der Wissenschaft heraus an OCR-Techniken und die dafür eingesetzte Software.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>VORAUSSETZUNG</head>
            <p>Teilnehmende benötigen einen eigenen Laptop mit Internetanbindung und Ubuntu 18.04 als Betriebssystem. Alternativ kann auch Windows/Mac OSX mit der Software VirtualBox verwendet werden. Die VM wird den Teilnehmenden vom OCR-D-Projekt vor Ort zur Verfügung gestellt. Die Anzahl der Teilnehmenden ist auf 20-25 begrenzt. Python- und Linux-Kommandozeilen-Kenntnisse sind wünschenswert</p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" place=""foot""> OCR-D Git-Hub: https://github.com/OCR-D/</note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Deutsche Forschungsgemeinschaft (2014): </hi> 
                        Workshop 
                        <hi rend=""italic"">Verfahren zur Verbesserung von OCR-Ergebnissen</hi>. 
                        Protokoll zu den Ergebnissen und Empfehlungen des Workshops. 
                        <ref target=""http://www.dfg.de/download/pdf/foerderung/programme/lis/140522_ergebnisprotokoll_ocr_workshop.pdf"">
                     <hi style=""font-size:10pt"">http://www.dfg.de/download/pdf/foerderung/programme/lis/140522_ergebnisprotokoll_ocr_workshop.pdf</hi>
                  </ref> [Zuletzt abgerufen 07.01.2019]
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,digitalisierung;forschungsdaten;ocr,German,einführung;forschungsprozess;infrastruktur;text;umwandlung
10952,2019 - Johannes Gutenberg University;Goethe University,Johannes Gutenberg University;Goethe University,multimedial & multimodal,2019,DHd,DHd,Johannes Gutenberg-Universität Mainz (Johannes Gutenberg University of Mainz);Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt),Frankfurt & Mainz,,Germany,https://dhd2019.org/,Automatic Font Group Recognition in Early Printed Books,,Nikolaus Weichselbaumer;Mathias Seuret;Saskia Limbach;Vincent Christlein;Andreas Maier,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Introduction</head>
            <p style=""text-align:left; "">Early modern books were printed with a large variety of different fonts. In the first decades after Gutenberg’s invention, every printer had to start out by cutting his own punches and casting his own fonts. This diversity was somewhat standardised with the advent of an organised font trade in the 16th century. However, this was a very long process and one that was not completed before the 19th century. Only then did industrialised mass production make fonts as stable and - at least for text fonts - predictable as we know them today. The diversity of fonts is one of the cornerstones of analytical bibliography: with detailed descriptions of the individual fonts we can identify the printer of almost any given incunabula or at least narrow the possible candidates down to a very small group. </p>
            <p style=""text-align:left; "">For OCR, however, this is a major drawback. Most OCR-models are trained to work with one of three different training sets, based on either just modern antiqua-fonts or on 19th-century standard Fraktur or on all fonts that ever existed. Specialised OCR-models, e. g. for Rotunda or Textura, almost don’t exist as they would be very difficult to apply. One reason for this is that metadata for digitised books usually does not include the the font group or even the font of the main text face. Therefore, these models would - at the moment - only be applicable if the font is recognised manually. Given the vast amount of digital copies rendered by the large-scale digitisation projects like those for VD16, VD17 and VD18, this is out of the question. </p>
            <p style=""text-align:left; "">Our project addresses this problem in two ways. Firstly, we will create a tool that can identify font groups automatically, i.e. fonts which are similar to each other and thus can be used jointly for training an OCR model (Christlein / Weichselbaumer 2016). Secondly, we will create OCR-models for various font groups. In this way, we hope to significantly improve the recognition rates of OCR for early printed books. In this paper, we will present and discuss first results on automatic font group recognition. </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Basis</head>
            <p style=""text-align:left; "">Fortunately, we have outstanding Ground Truth data: the Gesamtkatalog der Wiegendrucke (GW) (Staatsbibliothek zu Berlin 2019a) and its side project, the Typenrepertorium der Wiegendrucke (TW) (Staatsbibliothek zu Berlin 2019b). Both were initiated by Konrad Haebler at the turn of the last century and are still maintained today at the Berlin State Library. The GW provides us with bibliographical data for all known incunabula editions (books printed in the 15th century) as well as some 15,000 digital copies from all over the world. The corresponding records in the TW list over 6,000 fonts used for these books and later editions. This Ground Truth data was painstakingly collected in over a century and was recently - thankfully - converted to a database by the Berlin State Library (Eisermann / Duntze 2014). </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Method</head>
            <p style=""text-align:left; "">In a first step, we have also accumulated a large body of material from our collaboration partners: The University Library of Cologne, the Herzog-August Library in Wolfenbüttel, the University Library of Heidelberg, the University Library of Erlangen, the Berlin State Library, the Göttingen State Library, the Stuttgart State Library and the Bavarian State Library. We will also be able to work soon - for the first time - with digitised copies from the British Library, which is currently scanning its large incunabula collection. All in all, we have over a million images which provide a sound basis for our goals.</p>
            <p style=""text-align:left; "">For evaluation purposes, random pages were taken out from the labelled data. We have two such subsets: validation and test. The validation data is used for tuning the classification method and evaluating it during its development, while the test data is kept until the end for an unbiased method evaluation on never seen, never used data.</p>
            <p style=""text-align:left; "">A deep convolutional neural network (CNN) is used for the font recognition. To have both a robust and proven network architecture, we used one inspired by a residual network with 50 layers, also known as ResNet50 (He et al. 2016). A typical ResNet50 has layers with a large amount of neurons (from 64 to 512 in the convolutional layers), which can lead to overfitting the training data. To avoid this pitfall, we restrict the layers to having 96 neurons, with a penultimate fully-connected layer of 384 neurons. The training is done by stochastic gradient descent on batches of 64 samples, with a constant momentum of 0.9 and initial learning rate and weight decay of respectively 0.01 and 0.0005. The learning rate and weight decay are divided by 10 every 300,000 samples. The training is stopped after processing a million samples because the error stagnates.</p>
            <p style=""text-align:left; "">The CNN has a receptive field of 224x224 pixels, which is insufficient for processing a whole page image at once. To identify the font of a page, we present 25 random crops from this page to the CNN, and average the results of the last linear layer (not the softmax output), then the class with maximum average value is taken. Crops full of text contain between 15 and 500 characters, depending on image resolution and text size. Typically, if a crop is misclassified (e.g., if it did not contain text), it will have little impact on the average result as the CNN is likely to produce results will low confidences.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Evaluation</head>
            <p style=""text-align:left; "">We used, as training data, 280 pages with a median resolution of 2 megapixels from the 15th century containing text with fonts from four different groups: Antiqua, Bastarda, Rotunda, and Textura. </p>
            <figure>
               <graphic n=""1001"" width=""8.942916666666667cm"" height=""9.36625cm"" url=""279_final-18cc8dfeb4225c81d21b9c4dd2131089.jpg"" rend=""inline""/>
            </figure>
            <p style=""text-align:left; "">This means the CNN does not have the ability to answer something else. It is however useful to investigate what happens when pages with other fonts are given to the CNN. So as a test we provided 100 images of Fraktur - a font very closely connected to Bastarda. In addition to that, we also provided 30 images of each Greek, Hebrew and Italic - fonts that are rather different to the others. The results obtained on the test data for the four base fonts (15 pages each), as well as on the other pages, are given in the following confusion matrix:</p>
            <figure>
               <graphic n=""1002"" width=""16.002cm"" height=""7.611352777777777cm"" url=""279_final-a362a69c091dc64db482c9f7d6ebae11.png"" rend=""inline""/>
            </figure>
            <p style=""text-align:left; "">Rows correspond to fonts, and columns to results given by the CNN. We can see that the test pages with the fonts known by the CNN are well classified, with an accuracy of 93%. The other fonts are more spread, but mostly classified as Bastarda.</p>
            <p style=""text-align:left; "">This is a very significant result. Not only does the network recognize the connection between Bastarda and Fraktur but it also perceives the significant difference between Bastardas and the other three groups. After all, from the very beginning of typography the font group Bastarda differed considerably from other font groups. This is especially true for Texturas and Rotundas which have very uniform characteristics: Textura letters are upright, narrow and stand on crooked feet and Rotunda letters are small, curved and reveal a great contrast between thick and thin strokes. In contrast, Bastardas show much more variety - letters tend to slope forward and have flourished ascenders, yet many of them do not have these characteristics. Therefore it is very plausible that the CNN would categorize ‘unknown’ fonts as Bastardas. </p>
            <p style=""text-align:left; "">This matches what can be seen from the data produced by the penultimate fully-connected layer of the CNN. As it has 384 dimensions, a t-Distributed Stochastic Neighbor Embedding (t-SNE) can be applied for visualization purpose. In the figure below, the dots correspond to individual random crops from test images. We can see five main areas. The one at the bottom corresponds to crops with little or no text, and therefore the CNN produces similar values regardless of the type group of the page. The points between this cluster and the center of the graphics might correspond to crops with text content, but not in a quantity large enough for identifying the script. Then, we have three well defined clusters for Antiqua, Rotunda, and Textura. Finally, the part corresponding to Bastarda is well spread and significantly less dense than for the other type groups. Thus, the CNN produces more variability in its penultimate layer for the Bastarda than for the other type groups, and a more important area of the feature space is considered, by the CNN, as corresponding to Bastarda. This could also explain why unseen type groups are frequently classified as belonging to the Bastarda. </p>
            <figure>
               <graphic n=""1003"" width=""16.002cm"" height=""15.545152777777778cm"" url=""279_final-d769443edebb072b04572db8501d5069.png"" rend=""inline""/>
            </figure>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Outlook</head>
            <p style=""text-align:left; "">These results show that it is feasible to recognise font groups automatically. The authors are currently working on improving accuracy further and to expand the scope of the recognition tool from the 15th to the 16th-18th century. At the same time preliminary steps are taken to recognise not only font groups but exact fonts. This feature would not only make it much quicker to date and identify the printer of early modern books based on their fonts but also make this procedure much more accessible to scholars who are not highly specialised in analytical bibliography.</p>
            <p style=""text-align:left; "">The source code used in this paper is available on github (
                    <ref target=""https://github.com/seuretm/typegroups-classification-projection"">https://github.com/seuretm/typegroups-classification-projection</ref>). Please note that the exact same results cannot be obtained due to the randomness of initial parameters, and that the data is currently not publicly available.
                </p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Christlein, Vincent / Weichselbaumer, Nikolaus (2016):</hi>
                  <hi rend=""italic"">Automatische Typenbestimmung in historischen Drucken.</hi> 
                        Poster at DHd2016, Abstract online: http://dhd2016.de/boa-2.0.pdf [Access date 9 October 2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Eisermann, Falk / Duntze, Oliver (2014):</hi>
                  <hi rend=""italic"">Auf der Spur der seltsamen Typen. Das digitale Typenrepertorium der Wiegendrucke</hi>, 
                        in: Bibliotheksmagazin 3: 41–48 https://www.bsb-muenchen.de/fileadmin/imageswww/pdf-dateien/bibliotheksmagazin/BM2014-3.pdf [Access date 9. October 2018].
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">He et al. (2016):</hi> 
                        https://ieeexplore.ieee.org/abstract/document/7780459.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Staatsbibliothek zu Berlin (2019a):</hi> 
                        https://www.gesamtkatalogderwiegendrucke.de [Access date 11 January 2019]. 
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Staatsbibliothek zu Berlin (2019b):</hi> 
                        https://tw.staatsbibliothek-berlin.de [Access date 11 January 2019].
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,buchgeschichte;mustererkennung;neuronales netz;ocr;schriftart,English,bilder;literatur;methoden;werkzeuge
10968,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,Nachlass Ludwig Wittgenstein: Softwaretechnologien und computerlinguistische Methoden der Software-Infrastruktur um die FinderApp WiTTFind,,Maximilian Hadersbeck;Florian Babl;Marcel Eisterhues;Ines Röhrer;Sebastian Still;Sabine Ullrich;Florian Landes;Matthias Lindinger,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""DH-Heading"" type=""div1"">
            <head>Die Infrastuktur und das Projekt</head>
            <p>Seit 2010 kooperieren das Wittgenstein Archiv der Universität Bergen und das Centrum für Informations- und Sprachverarbeitung der Ludwig-Maximilians Universität München in der Forschungsgruppe „Wittgenstein Advanced Search Group“ (WAST). Die Forschungsgruppe entwickelt Web-Frontends (FinderApps) und spezielle Suchwerkzeuge, die sich gut für die Forschung und Lehre im Bereich der Digital Humanities eignen. Ihre erste Suchmaschine, die FinderApp WiTTFind (wittfind.cis.lmu.de, siehe Abb. 1), die den von der UNESCO zum Weltkulturerbe (im Jahr 2017) erhobenen (Schmidt 2018) Nachlass von Ludwig Wittgenstein durchsucht, gewann im Jahre 2014 der EU-Open-Humanity Award. Der Preis zeichnet Gruppen aus, die herausragende Technologie im Bereich der Humanities entwickelt haben. Die in der Forschergruppe programmierte FinderApp WiTTFind erlaubt es, mit hochqualifizierten, computerlinguistisch orientierten Suchwerkzeugen Nachlasstrans-kriptionen zu durchsuchen. Die Transkriptionen entstammen der 
                    <hi rend=""italic"">Bergen Normalized Edition</hi>, die die Grundlage der Wittgenstein Edition bildet. Neben den gefundenen Treffern der Suchmaschine, werden in den Suchergebnissen von WiTTFind die Faksimile-Extrakte aus den Originaldokumenten angezeigt. So kann der Nutzer die „Aura“ der gefundenen Textstelle im Original studieren und nicht nur den transkribierten Text sehen.
                </p>
            <p>
               <figure>
                  <graphic url=""107_final-f897daa699cd4943461d46e6bf0dc126.png""/>
                  <head> Abbildung 1: WiTTFind (<ref target=""http://wittfind.cis.lmu.de/"">http://wittfind.cis.lmu.de</ref>)</head>
               </figure>
            </p>
            <p>Damit derNutzer auch den seitenweisen Kontext des Suchtreffers im Original studieren kann, wurde am CIS eine weitere WEB-Applikation entwickelt, der doppelseitige Reader. Dieser Reader ermöglicht es, vom Suchtreffer direkt an die entsprechende Stelle im entsprechenden Dokument des Originals zu springen. Im doppelseitigen Lesemodus kann der Nutzer in den Faksimile des originalen Dokuments blättern. Eine symmetrische Autovervollständigung gibt während der Suchanfrage einen statistischen und lexikalischen Zugang zu den Wörtern, die in der Edition vorkommen. Im Zentrum der Suche steht die selbstprogrammierte C++ Suchmaschine wf, die mit Hilfe von Vollformlexika (WiTTlex), verbessertem POS-Tagging und weiteren Metainformationen regelbasiertes Suchen erlaubt. Zum Aufspüren semantisch ähnlicher Textpassagen in der Edition gibt es das NLP-Tool WiTTSim.</p>
            <p>Die thematisch getrennten Aufgaben innerhalb der Infrastruktur der WAST-Tools (siehe Abb. 2) werden über REST-API’s von einzelnen Microservices realisiert, deren zentrale Datenhaltung über eine mongo Datenbank realisiert wird. Die Oberflächen der FinderApps werden mit HTML5, Javascript und Bootstraptechniken für WEB-Browser programmiert und möglichst browserunabhängig gehalten. </p>
            <p>
               <figure>
                  <graphic url=""107_final-3e09113c66b34dd483a67a95e3bda3a6.png""/>
                  <head> Abbildung 2: Infrastruktur der WAST-Tools (<ref target=""http://gitlab.cis.lmu.de/"">http://gitlab.cis.lmu.de</ref>)</head>
               </figure>
            </p>
            <p>Alle Programme, Schnittstellen und Entwicklungen werden dokumentiert (siehe Abb. 3) und Tutorials für Anschlussprojekte entwickelt. So ist gewährleistet, dass die Tools und Suchmaschinen nachhaltig verwendet und auch für die Forschung und Lehre eingesetzt werden können. Als Versionskontrollsystem wird git verwendet.</p>
            <p>
               <figure>
                  <graphic url=""107_final-093084038ce6bf919787e0a17a7159fb.png""/>
                  <head> Abbildung 3: Dokumentation der WAST-Tools: <ref target=""http://wittfind.cis.uni-muenchen.de/wast/infrastruktur/index.html"">http://wittfind.cis.uni-muenchen.de/wast/infrastruktur/index.html</ref>
                  </head>
               </figure>
            </p>
            <p>Bei der Entwicklung der Infrastruktur der WAST-Tools wurden die strengen Vorgaben des EU-Open-Humanity Awards eingehalten: Forderungen nach Open-Source, interdisziplinäre Öffnung und Nachhaltigkeit. Diese Offenheit ermöglichte es weitere FinderApps für andere Wissenschaftsbereiche zu implementieren: GoetheFind (Faust-I und Faust-II Edition, Deutsches Textarchiv Berlin (XML-TEIP5, DTA Basis Format)), HistoFind (Briefwechsel Erzherzog Leopold Wilhelms an Kaiser Ferdinand III. aus dem Reichsarchiv Stockholm; Kooperation mit Historikern) und den OdysseeReader (Schreibprozess der zur Logisch-Philosophischen-Abhandlung führte; Kooperation mit Philosophen).</p>
            <p>In diesem Workshop werden die verwendeten Softwaretechnologien und computerlinguistischen Methoden im konkreten Einsatz vorgestellt. Den Teilnehmer*innen wird ein Debian-10 Container mit allen notwendigen Programmen, Tools und Dokumentation der gesamten Softwareinfrastruktur zur Verfügung gestellt. Innerhalb dieses Containers können die Teilnehmer*innen die einzelnen Tools der WAST-Projektgruppe kennenlernen und bekommen von den Projektmitarbeiter*innen kleine Aufgaben gestellt, die sie dann mit ihnen bearbeiten. So können sie die Arbeitsweise der WAST Infrastruktur konkret kennenlernen.</p>
         </div>
         <div rend=""DH-Heading"" type=""div1"">
            <head>Im Workshop werden folgende Datenformate, Tools und Programmierkonzepte vorgestellt und geübt</head>
            <p>Gitlab Projektmanagement und Continuous Integration, XML TEI-P5 Edition CISWAB, Faksimilestrukturierung und Texterkennung, lexikalische Arbeit, WEB-Oberfläche der FinderApps und Einsatz mit Micorservices, doppelseitiger Faksimilereader mit MongoDB, NLP-Tools zur semantischen Ähnlichkeitssuche, Vorstellung und Programmierung einer regelbasierten Suchmaschine und die Erstellung eines Dokumentationssystems mit Sphinx.</p>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Voraussetzungen an die Kursteilnehmer*innen</head>
               <p>Programmierkenntnisse (Grundkenntnisse): LINUX (Arbeit mit der UNIX-Shell), Python, XML, HTML, git, javascript, POS-Tagging.</p>
               <p>Da beim Workshop einige Entwickler der WAST-Tools anwesend sein werden, gibt es die Möglichkeit auch vertieft in die jeweilige Thematik einzusteigen.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Gitlab Projektmanagement und Continuous Integration (Hadersbeck, Still)</head>
               <p>Im gesamten Projekt wird als Versionierungssystem git verwendet. Die Projektrepositories werden auf zwei unterschiedlichen Rechnern ausgerollt: Dem preview-Server für Tests und einem Projektserver für die offizielle Onlineversion. Es wird das in der Praxis bewährte „git branching model“ kombiniert mit einer „continuous integration“ Technik eingesetzt. Mit einer Feedbackapp können Nutzer Fehler melden oder Implementierungswünsche äußern, die in Issues innerhalb der Projektrepositories bearbeitet werden.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>XML TEI-P5 Edition CISWAB (Hadersbeck)</head>
               <p>Als Datenbasis für das WiTTFind Projekt wird die „Bergen Nachlass Edition“ (BNE) verwendet, die sich an den Richtlinien der Text Encoding Initiative (TEI-P5) orientiert. Im Workshop werden die wichtigen TEI-XML-Elemente der BNE vorgestellt.</p>
               <p>   </p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Faksimilestrukturierung und Erkennung (Eisterhues, Landes)</head>
               <p>Da in den FinderApps neben den gefunden Textstellen auch die zugehörigen Faksimileextrakte aus der Edition dargestellt werden, sind Kenntnisse der Bildkoordinaten der Textstellen nötig. Diese Koordinaten werden mit Hilfe einer Kette von Bildverarbeitungstools ermittelt. Da bei Manuskripten und bei manuellen Änderungen in Dokumenten die automatische Zeichenerkennung unbrauchbare Ergebnisse liefert, wurden eigene Strategien entwickelt, die die Informationen aus der BNE nutzen. Im Workshop werden die eingesetzten Tools und Optimierungsstrategien vorgestellt.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Lexikalische Arbeit (Lokale Grammatiken, Semantik) (Röhrer)</head>
               <p>Zur lemmatisierten Suche, Partikelverberkennung und semantischen Wortfeldern wurden spezielle Projektlexika entwickelt (Röhrer 2017). Die Lexika enthalten alle Wörter der zu durchsuchenden Edition und sind mit grammatischen Angaben und zum Teil mit zusätzlichen semantischen Informationen versehen. Diese Lexika und ein nachgestelltes optimiertes Part-of-Speech Tagging ist die Grundlage für die computerlinguistischen Methoden, die bei der regelbasierten Suche im Nachlass von Ludwig Wittgenstein eingesetzt werden.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Regelbasierte Suchmaschine (Babl)</head>
               <p>Im Zentrum der FinderApps steht die Suchmaschine wf, ein multithreaded C++ Programm, das viele Anfragemöglichkeiten zur Suche implementiert: Einwort und Mehrwortsuche (mit internem Rankingverfahren) und reguläre Ausdrücke kombiniert mit linguistischen Anfragen (Morphologische Eigenschaften, POS-Tags, semantische und syntaktische Tags). Für das Rankingverfahren wird für jeden Suchtreffer die Relevanz zur Suchanfrage berechnet. Die Qualität für jeden Suchtreffer, die Distanz zwischen den einzelnen Wörtern und unterschiedlichen Belohnungs- und Bestrafungsparametern, gehen in die Berechnung der Relevanz ein. Die Treffer werden dann nach dieser sortiert und auf der Website ausgegeben. Durch dieses neuartige Ranking kann nun auch nach verschiedenen Wörtern gesucht werden, die im Text nicht direkt hintereinander stehen müssen.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>NLP-Tool Semantische Ähnlichkeitssuche (Ullrich)</head>
               <p>Zur Extraktion von semantisch ähnlichen Bemerkungen wurde das Analysetool WiTTSim (Ullrich 2018) entwickelt, welches anhand von semantischen und syntaktischen Features ähnliche Texte identifiziert. Da die enorm hohe Anzahl von etwa 100.000 Features in Kombination mit den zu vergleichenden 54.000 Bemerkungen eine effiziente Suche unmöglich macht, wurde ein semantisches Clustering-Verfahren vorgeschaltet (Ullrich 2019), welches durch Dimensionsreduktion und Gruppierung der Texte die Rechenzeit der Ähnlichkeitssuche um den Faktor 100 beschleunigt.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>WEB-Oberfläche der FinderApps und Micorservices (Hadersbeck, Still)</head>
               <p>Zur Arbeit mit WiTTFind wird dem User eine WEB-basierte FinderApp zur Verfügung gestellt, die über REST-APIs und „internet microservices“ mit den WAST-Tools kommuniziert. HTML5, Javascript und Bootstrap-css erlauben den Aufbau der WEB-page, die nahezu browserunabhängig die Schnittstelle zum Anwender darstellt. </p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Doppelseitiger Faksimilereader und MongoDB (Lindinger)</head>
               <p>Der doppelseitiger Faksimilereader ist eine komplett eigenständige Anwendung mit Suchschlitz und Investigate Mode zur gleichzeitigen Betrachtung von Faksimile und Transkription. Außerdem gibt es zahlreiche weitere Features, die es den Nutzern sehr bequem erlauben, die gefunden Treffer der Suchmaschine im Kontext einer doppelseitigen Darstellung der Faksimile zu sehen und gleichzeitig durch die Dokumente der Forschungsdomäne zu blättern. Sämtliche Informationen bzgl. Edition und Faksimile sind in einer MongoDB gespeichert und werden über HTTP-Schnittstellen abgefragt.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Dokumentationssystem Sphinx (Babl) (siehe Abb.2)</head>
               <p>Für jedes Teilprojekt der Wittgenstein Advanced Search Tools (WAST) wird im entsprechenden Gitlab Ordner eine README.md Datei erstellt, das in einer Dokumentation, die alle Projekte umspannt mithilfe der Software Sphinx zusammengefasst und online auf ansprechende Art und Weise darstellt. Die Dokumentation hilft, neuen Studierenden einen schnelleren Einstieg in das Projekt zu finden und ermöglicht es, das gesamte WAST-Projekt schnell nach bestimmten Fachbegriffen zu durchsuchen. </p>
            </div>
         </div>
         <div rend=""DH-Heading"" type=""div1"">
            <head>Programm des Workshops (ganztages Workshop)</head>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Überblick/Einführung/Vorstellungsrunde </head>
               <p>Digitaler Zugang zum Nachlass von Ludwig Wittgenstein, das Projekt WAST (Dr. Max Hadersbeck)</p>
               <p>Fragen/ Diskussion/ gewünschte Schwerpunkte der Teilnehmer*innen des Workshops</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>
                  <hi rend=""bold"">WAST-Spezialthemen</hi> (jeweils ca. 15 Min. Theorie / 20 Min. Praxis)</head>
               <list type=""unordered"">
                  <item> Gitlab Projektmanagement und Continuous Integration mit git production / testing server (Hadersbeck, Still) 
      </item>
                  <item> XML TEI-P5 Edition CISWAB (Hadersbeck): Bergen Normalized Edition und xslt-Transformationen und Investigate-Mode von WiTTFind</item>
                  <item> Faksimilestrukturierung und OCR Erkennung (Eisterhues, Landes) </item>
                  <item> Lexikalische Arbeit (Röhrer): Lemmatisierte Suche, Lexika, Lokale Grammatiken, Query Beispiele</item>
                  <item> WEB-Oberfläche der FinderApps und Microservices (Hadersbeck, Still): Flask server, Javascript</item>
                  <item> Doppelseitiger Faksimilereader und mongodb (Lindinger) </item>
                  <item> NLP-Tool Semantische Ähnlichkeitssuche (Ullrich): NLP-Python Libraries, Funktionalitäten</item>
                  <item> Regelbasierte Suchmaschine (Babl): Programmierung C++, make/cmake, client-server Programmierung mit C++</item>
                  <item> Dokumentationssystem Sphinx (Babl): Markdown, Sphinx Installation, 2HTML, 2PDF</item>
               </list>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Arbeitsgruppen: Diskussionen/Spezialfragen</head>
               <p>Je nach Interesse der Teilnehmer*innen unter der Leitung der einzelnen Dozent*innen.</p>
            </div>
         </div>
         <div rend=""DH-Heading"" type=""div1"">
            <head>Kurzbiographie der Dozent*innen</head>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Florian Babl (CIS)</head>
               <p>Bachelorarbeit: Entwicklung eines Rankingverfahrens der Suchtreffer für die FinderApp WiTTfind im Nachlass Ludwig Wittgensteins </p>
               <p>Forschungsschwerpunkte: verschiedene Rankingalgorithmen und ihre Funktionalität mit dem Ziel der Rankingverbesserung.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Marcel Eisterhues (CIS)</head>
               <p>Forschungsschwerpunkte: Der momentane Forschungsschwerpunkt ist die automatische Seitensegmentierung von handgeschriebenen Texten.</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Max Hadersbeck (CIS)</head>
               <p>Projektleiter und Dozent am CIS</p>
               <p>Forschungsschwerpunkte: Digitaler Zugang zum Nachlass von Ludwig Wittgenstein, FinderApp WiTTFind, Wittgenstein Advanced Search Tools, Programmierung: C++, Python, XML</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Florian Landes (Kommission für bayerische Landesgeschichte bei der Bayerischen Akademie der Wissenschaften) </head>
               <p>Bachelorarbeit: Optical Character Recognition (OCR) – Optische Zeichenerkennung (OZE) Ein Werkzeug zur Verknüpfung von digitaler Edition und Faksimile? Semiautomatische Ermittlung von Bildkoordinaten für WiTTFind</p>
               <p>Forschungsschwerpunkte: OCR, OZE, Bavarikonprojekt Ortsnamen des Regierungsbezirks Schwaben</p>
               <p> <lb/> <lb/> </p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Ines Röhrer (CIS)</head>
               <p>Masterarbeit: Lexikon, Syntax und Semantik - computerlinguistische Untersuchungen zum Nachlass Ludwig Wittgensteins</p>
               <p>Forschungsschwerpunkte: Digitales Speziallexikon WiTTLex für den Nachlass von Ludwig Wittgenstein</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Sebastian Still (CIS)</head>
               <p>Masterarbeit: Ludwig Wittgenstein: 100 Jahre Traktatus. Der Odyssee-Reader, ein web-basiertes Tool zur textgenetischen Suche im Traktatus</p>
               <p>Forschungsschwerpunkte: moderne Frontend Programmierung, NLP (Backend)</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Sabine Ullrich (CIS)</head>
               <p>Masterarbeit: Clustering zur Verbesserung der Performanz einer Ähnlichkeitssuche</p>
               <p>Forschungsschwerpunkte: Natural Language Processing, Data Mining, semantische Ähnlichkeitserkennung im Nachlass von Ludwig Wittgenstein</p>
            </div>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Babl, Florian </hi>(2019): 
                        <hi rend=""italic"">Entwicklung eines Rankingverfahrens der Suchtreffer für die FinderApp WiTTFind im Nachlass Ludwig Wittgensteins</hi>. Bachelor‘s thesis. LMU.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Landes, </hi>
                  <hi rend=""bold"">Florian</hi> (2019): 
                        <hi rend=""italic"">Optical Character Recognition (OCR) – Optische Zeichenerkennung (OZE). Ein Werkzeug zur Verknüpfung von digitaler Edition und Faksimile? Semiautomatische Ermittlung von Bildkoordinaten für WiTTFind</hi>, Bachelorarbeit, LMU.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Lindinger, Matthias</hi> (2013): 
                        <hi rend=""italic"">Highlighting von Treffern des Suchmaschinentools </hi>
                  <hi rend=""italic"">WiTTFind im zugehörigen Faksimile.</hi> Bachelor‘s thesis, LMU.
</bibl>
               <bibl>
                  <hi rend=""bold"">Lindinger, </hi>
                  <hi rend=""bold"">Matthias</hi> (2015): 
                        <hi rend=""italic"">Entwicklung eines WEB-basierten Faksimileviewers mit Highlighting von Suchmaschinen-Treffern und Anzeige der zugehörigen Texte in unterschiedlichen Editionsformaten</hi>. Master's thesis, LMU.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Pichler, Alois</hi> (2017): 
                        <hi rend=""italic"">Wittgenstein Archives at the University of Bergen (WAB): Open Access to Wittgenstein's Nachlass. XML based Interactive Dynamic Presentation (IDP) of WAB's Nachlass transcriptions.</hi> 16. Mai 2017. http://wab.uib.no/transform/wab.php?modus=opsjoner [letzter Zugriff 20.09.2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Hadersbeck, </hi>
                  <hi rend=""bold"">Maximilian / </hi>
                  <hi rend=""bold"">Pichler, </hi>
                  <hi rend=""bold"">Alois / </hi>
                  <hi rend=""bold"">Fink, </hi>
                  <hi rend=""bold"">Florian /</hi>
                  <hi rend=""bold""> Gjesdal, </hi>
                  <hi rend=""bold"">Øyvind L.</hi> (2014): „Wittgenstein's Nachlass: WiTTFind and Wittgenstein advanced search tools (WAST)“
                        <hi rend=""italic"">,</hi> in: 
                        <hi rend=""italic"">Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage</hi>, 91-96. ACM.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Hadersbeck , </hi>
                  <hi rend=""bold"">Maximilian /</hi>
                  <hi rend=""bold""> Pichler, </hi>
                  <hi rend=""bold""> Alois /</hi>
                  <hi rend=""bold""> Bruder, Daniel / Schweter, Stefan</hi> (2016): 
                        <hi rend=""italic"">New (re)search </hi>
                  <hi rend=""italic"">possibilities for Wittgenstein's Nachlass II: Advanced Search, Navigation and Feedback with the FinderApp WiTTFind</hi>. http://wab.uib.no/alois/Hadersbeck_Pichler%20Kirchberg2016.pdf [letzter Zugriff 20.09.2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Röhrer, </hi>
                  <hi rend=""bold"">Ines / </hi>
                  <hi rend=""bold"">Ullrich, </hi>
                  <hi rend=""bold"">Sabine / </hi>
                  <hi rend=""bold"">Hadersbeck, </hi>
                  <hi rend=""bold"">Maximilian </hi>(2019): 
                        <hi rend=""italic"">Weltkulturerbe international digital: Erweiterung der Wittgenstein Advanced Search Tools durch Semantisierung und neuronale maschinelle Übersetzung</hi>. multimedial multimodal. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 25. - 29.03.2019 an den Universitäten zu Mainz und Frankfurt.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Röhrer, </hi>
                  <hi rend=""bold"">Ines </hi>(2017): 
                        <hi rend=""italic"">Musik und Ludwig Wittgenstein: Semantische Suche in seinem Nachlass.</hi> Bachelor‘s thesis, LMU.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Schmidt, </hi>
                  <hi rend=""bold"">Alfred </hi>(2018): „Ludwig Wittgenstein’s Nachlass in the UNESCO Memory of the World register.“, in: 
                        <hi rend=""italic"">Nordic Wittgenstein Review </hi>7(2):209–213.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Ullrich, </hi>
                  <hi rend=""bold"">Sabine /</hi>
                  <hi rend=""bold""> Bruder, </hi>
                  <hi rend=""bold"">Daniel /</hi>
                  <hi rend=""bold""> Hadersbeck, </hi>
                  <hi rend=""bold"">Maximilian </hi>(2018): Aufdecken von „versteckten"" Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning. Kritik der digitalen Vernunft. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 26.02.-02.03. 2018 an der Universi
                        <anchor id=""id__GoBack""/>tät zu Köln, veranstaltet vom Cologne Center for eHumanities (CceH).
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Ullrich, Sabine </hi>(2019):
                        <hi rend=""italic""> Boosting Performance of a Similarity Detection System using State of the Art Clustering Algorithms</hi>. Master‘s thesis. LMU.
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,git projektmanagement;lexikalische arbeit;mongodb;nlp-ähnlichkeitssuche;ocr;suchmaschinen programmierung;tei-p5 xml edition und transformation;web-programmierung,German,bilderfassung;forschung;infrastruktur;programmierung;projektmanagement;webentwicklung
10969,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,Spielräume bei der retroperspektivischen Analyse der Wittgenstein-Edition und die Herausforderungen für das Semantic Clustering,,Maximilian Hadersbeck;Sabine Ullrich;Sebastian Still;Alois Pichler,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""DH-Heading"" type=""div1"">
            <head>Einleitung</head>
            <p>
  Seit 2010 kooperieren das Wittgenstein Archiv an der Universität Bergen (WAB, Alois Pichler) und das Centrum für Informations- und Sprachverarbeitung der Ludwig-Maximilians Universität München
  (CIS, Max Hadersbeck et. al.) in der Forschungsgruppe „Wittgenstein Advanced Search Tools” (WAST). Die WAST-Projektgruppe entwickelt die web-basierte 
  FinderApp WiTTFind (<ptr target=""http://wittfind.cis.lmu.de/""/>), die einen computerlinguistisch gestützten digitalen Zugang zu WABs Wittgenstein-Edition erlaubt. Nach einer kompletten Neuscannung des Nachlasses und intensiven Verhandlungen des WAB mit den Rechteinhabern, dürfen seit 2018 WABs Edition auf der WiTTFind-Webseite durchsucht und Faksimileextrakte dargestellt werden. Nun konnten wir uns einer zentralen Frage der Wittgensteinforscher widmen: Wo finden sich in seinem Nachlass semantisch ähnliche Bemerkungen und, retroperspektivisch betrachtet, wann fanden diese Änderungen statt? 
</p>
            <p>Wir entwickelten das Analysetool WiTTSim (Ullrich, 2018), das semantisch ähnliche Bemerkungen in der Edition aufspürt, zusammen mit einem vorgeschalteten semantischem Clusterverfahren (Ullrich, 2019), welches die Rechenzeit der Ähnlichkeitssuche um den Faktor 100 verkürzte. Zur retroperspektivischen Analyse der Edition entwickelten wir ein zeitorientiertes, textgenetisches Datenmodell, das die Spielräume der Interpretation der bisher dokumentorientierten Edition auf zugelassene Lesarten reduziert.</p>
            <p>In unserem Vortrag stellen wir die Verfahren unserer Ähnlichkeitssuche mit vorgeschaltetem semantischen Clustering und ein neues mehr textgenetisch- als dokumentorientiertes Modell einer Edition vor, das im Web-Frontend des OdysseeReaders (www.odysseereader.wittfind.cis.lmu.de) implementiert ist und auch die Frage beantwortet: „Wann gibt es semantisch ähnliche Bemerkungen“.</p>
         </div>
         <div rend=""DH-Heading"" type=""div1"">
            <head>Die Datenbasis: Dokument- und Zeitorientierte Modelle</head>
            <p>Die bei uns verwendete Datenbasis BNE 2015- und IDP 2016-, die am Wittgensteinarchiv an der Universität Bergen (Pichler, WAB) erstellt werden, enthalten Faksimile und Transkriptionen (auf der Basis von XML-TEI-P5) des Nachlasses von Ludwig Wittgenstein. Dieser Nachlass umfasst ca. 20.000 Seiten, welche vom WAB in Dokumente und diese wiederum in logische Textabschnitte unterteilt sind. Jeder der 54.930 Textabschnitte – eine sogenannte Bemerkung – wird mit einer eindeutigen Bezeichnung, dem sogenannten Siglum, versehen und wird in unserer Ähnlichkeitssuche als einzelnes Textobjekt definiert und semantisch analysiert. </p>
            <p>Betrachtet man die Annotationen der BNE unter dem Aspekt der Retroperspektive, taucht folgendes Problem auf: Die BNE liefert nur auf der Ebene der Bemerkungen Informationen über ihren Erstellungszeitpunkt bzw. -zeitrahmen. Die Änderungen auf Wort und Zeichenebene sind zwar akribisch annotiert, allerdings fehlt die zeitliche Information wann diese Änderungen vorgenommen wurden. Um textgenetische Metainformationen auf Wort- bzw. Zeichenebene in das “ordered hierarchy of content objects model data” (OHCO) einer XML-Edition, wie das der BNE zu integrieren, schlägt das TEI-P5 Konsortium Fragmentierungs-, Milestone oder Standoff-Markup Annotationen vor (Jörg Hornschemeyer, 2013), die am WAB bisher nicht durchgeführt wurden. Von Geisteswissenschaftlern, deren wissenschaftliches Kerngebiet im Allgemeinen weit entfernt von der XML-Programmierung liegt, würde großer programmtechnischer Editionsaufwand verlangt. Eine Folge ist, dass von „Nachverwertern“ der Edition zur Generierung der textlichen Varianten algorithmisches Ausmultiplizierten der annotierten Varianten implementiert wird, was z.B. in der Wittgenstein-Edition bei einzelnen Bemerkungen eine vierstellige Anzahl von Lesarten generiert. Betrachtet man die so automatisch generierten Lesarten, sind die meisten syntaktisch und semantisch falsch, was fatale Auswirkungen auf semantische Analysen der Textobjekte hat. Ohne zusätzliche, fein granulierte Metainformation in den annotierten Varianten sind die Spielräume der automatisierten Lesartengenerierung jedoch nicht einzugrenzen.</p>
            <p>Im Umfeld der Wittgensteinforschung gibt es eine Edition, die bis auf Zeichenebene zeitliche Informationen zur Textgenese liefert: Die Prototractatus-Tools (PTT 2016) von Martin Pilch (Pilch 2018). Sie dokumentieren den 
Nutzern Ludwig Wittgensteins Schreibprozess, beginnend mit einem leeren Notizbuch im Jahre 1915 und bis zum endgültigen Diktat des Ts-204 im Sommer 1918, das zu seiner einzigen philosophischen Veröffentlichung zu Lebzeiten, der „Logisch-Philosophischen Abhandlung“ führte. Leider konnten wir die Daten und Metainformationen der PTT-Edition in unserer FinderApp Infrastruktur nicht direkt analysieren, da unsere WiTTFind Infrastruktur zum einen auf das dokumentorientierte XML-TEI-P5 Datenformat aus Bergen zugeschnitten ist, und zum anderen die PTT-Edition im inkompatiblen Microsoft Word-97 Format vorliegt. Alle verfügbaren XML-TEI Importtools erfassten nur Bruchteile der Annotationen, sodass z.B. die Zeitinformationen der PTT überhaupt nicht erkannt und transformiert wurden. Um möglichst viel von der PTT-Textedition weiterzuverwenden, und damit der PTT-Hg. die Edition in seiner gewohnten Microsoft-Office Umgebung weiter optimieren kann, entwickelten wir eine mit Microsoft EXCEL leicht zu bedienende mehrdimensionale Tabellenstruktur. Die Editionsdaten und Metainformation der Word-97 Edition konnten wir größtenteils mit eigenen Programmen und Office-Macrotechniken transferieren. Zur Integration der Tabellen in die Infrastruktur unserer FinderApp verwendeten wir LibreOffice-Tools und selbst geschriebene Python Programme, die die Daten, sobald sie in das git-Repository des Projekts kopiert werden, mit Hilfe der continuous Integration automatisch transformieren und importieren. Zur Web-Präsentation werden sie an unsere neu entwickelte FinderApp, den 
OdysseeReader (siehe Abb. 1,
<ref target=""http://odysseereader.wittfind.cis.lmu.de/"">odysseereader.wittfind.cis.lmu.de</ref>), übergeben. Dieses Vorgehen trennt zwar das Daten- und Repräsentationsmodell, jedoch entwickelten wir ein positionsinvariantes Siglensystem, bestehend aus dem Tupel (Zeitstempel, Dokument, Seite, Zeile, Zeichenposition), das die beiden Modelle eineindeutig verknüpft. Diese bijektive Relation zwischen den beiden Modellen definiert dem Hg., wo er in seinem Datenmodell Änderungen vornehmen muss um sie an eine bestimmte Stelle, zu einem bestimmten Zeitpunkt im Repräsentationsmodell zu platzieren.
</p>
            <p>
               <figure>
                  <graphic url=""108_final-4552ec749bd02ac4ba9463fd464cc20d.png""/>
                  <head> Abbildung 1: Der OdysseeReader <ref target=""http://odysseereader.wittfind.cis.lmu.de/"">odysseereader.wittfind.cis.lmu.de</ref>
                  </head>
               </figure>
            </p>
         </div>
         <div rend=""DH-Heading"" type=""div1"">
            <head>Ähnlichkeitssuche mit vorgeschaltetem Semantic Clustering</head>
            <p>Die Ähnlichkeitssuche WiTTSim berechnet mit Hilfe
computerlinguistischer Methoden für jede Bemerkung einen
„charakteristischen” Vektor, oder, intuitiv gesprochen: Man bestimmt
einen “Fingerabdruck”. Dieser automatisierte Prozess wird unabhängig
im Voraus berechnet, was spätere Prozesse vereinfacht und
beschleunigt. Dieser „Fingerabdruck“ beinhaltet linguistische
Informationen, wie beispielsweise Wörter, deutsche und englische
Synonyme (aus Germanet und Wordnet), Wortarten (Treetagger) und
Lemmata <hi rend=""italic""> (WiTTLex,</hi> Röhrer 2019). Diese Informationen werden in binäre Vektoren übersetzt, welche insgesamt etwa 115.000 Features umfassen. Zusätzlich zur Datenbasis wurden 471 Bemerkungen bereits gruppiert, also mit Ground Truth Labels versehen. Die Gruppen bestehen dabei aus 2-15 Bemerkungen und das gelabelte das Korpus umfasst 1.670 Bemerkungen, was ca. 3% des gesamten Nachlasses entspricht.
</p>
            <p>Zur Semantischen Ähnlichkeitsberechnung ist allerdings eine Reduktion des Feature Raumes zwingend nötig, da die Vektoren mit so hoher Dimensionalität semantisch „weit voneinander entfernt“ sind und keine semantischen Gruppierungen auszumachen sind. Dieses Phänomen ist auch bekannt als 
<hi rend=""italic"">Curse of Dimensionality</hi>. Daher werden die Vektoren zunächst auf eine angemessene Anzahl von Features skaliert, um sie anschließend clustern zu können. Verwendete Reduktionstechniken umfassen Singular Vector Decomposition (SVD), Principal Component Analysis (PCA), Sparse Random Projection (SRP) und Uniform Manifold Approximation and Projection (UMAP). Auf unseren Daten zeigte eine SVD Reduktion zu 1.600 Dimensionen die besten Ergebnisse, zusammen mit UMAP, welches darüber hinaus die Daten im zweidimensionalen Raum klar gruppiert. Letzteres erlaubt nur eine Zieldimension von 2 bis 100 Dimensionen, weshalb zum Erhalt der Varianz die maximale Dimensionsanzahl von 100 gewählt wurde, um einen bestmöglichen Erhalt der gespeicherten Information zu gewährleisten.
</p>
            <p>Nach erfolgter Reduktion der Dimension können die Datenpunkte, also alle Bemerkungen, geclustert werden. Verwendete Clustering Techniken umfassen den klassischen K-Means Ansatz (Mac-Queen 1967, Ball and Hall 1956, Lloyd 1982, Steinhaus 1955), aber auch Dichte-basierte Ansätze wie Mean-Shift (Duda und Hart 1973) und DBSCAN (Ester et al. 1996), das statistische Gaussian Mixture Modell (Redner und Walker 1984) und das hierarchische Ward Clustering (Ward 1963). Beste Ergebnisse konnten mit einer Kombination von SVD und K-Means mit einer Anzahl an k=150 Clustern erzielt werden. Evaluiert wurde anhand der drei unüberwachten Metriken Silhouette Score, Davies Bouldin Index, und Calinski-Harabasz Index. Zusätzlich konnte durch die verfügbaren Ground Truth Labels auch der Recall berechnet werden, welcher in den Experimenten einen maximalen Wert von 1,0 erreicht. Dies zeigt, dass alle der gelabelten Daten richtig zugeordnet werden konnten. Wird eine Suchanfrage zum Auffinden ähnlicher Bemerkungen gestartet, muss nur der charakteristische Vektor der eingegebenen Bemerkung berechnet werden und das nächstliegende Cluster bestimmt werden. Letzteres erfolgt durch eine Bestimmung des am nächsten gelegenen Cluster Mittelpunkts (Zentroids). Anschließend werden die Abstände zu allen Bemerkungen des bestimmten Clusters gemessen, welche zuletzt dem Philologen zur genaueren Prüfung „gerankt“ vorgeschlagen werden.</p>
         </div>
         <div rend=""DH-Heading"" type=""div1"">
            <head>Zusammenfassung und Ausblick</head>
            <p>Unsere zeitgesteuerte textgenetische Edition kann von einem Wissenschaftler ohne XML Kenntnisse innerhalb einer Office Umgebung erstellt werden. Das continuous Integration System von git transferiert die Edition automatisch in unser WEB-basiertes Repräsentationssystem, den OdysseeReader. Über das von uns entwickelte eineindeutige Siglensystem verliert der Hg. niemals den klaren Zusammenhang zwischen Editions- und Präsentationsmodell.
</p>
            <p>Das von uns entwickelte Ähnlichkeitstool mit vorgeschaltetem Semantic Clustering könnte auch zur Ähnlichkeitsbestimmung zwischen zwei gegebenen Texten verwendet werden: Der Nutzer könnte einen Text eingeben, und es werden potentiell ähnliche Textpassagen in einer Sammlung von Texten gesucht, die dann „gerankt“ nach Ähnlichkeiten in einer Art Hitliste ausgegeben werden. Eine derartige Sortierung nach Textähnlichkeiten könnte es dem Philologen zum Beispiel besonders erleichtern, potentielle Zitate, Einflüsse und Verweise eines Autors innerhalb seines Werkes und im Bezug auf die Literatur seiner Zeit aufzuspüren.</p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Ball, </hi>
                  <hi rend=""bold"">Geoffrey H.</hi>
                  <hi rend=""bold""> / Hall </hi>
                  <hi rend=""bold"">David J.</hi> (1965): 
                        <hi rend=""italic"">Isodata, a novel method of data analysis and pattern classification</hi>. Technical report, Stanford research inst Menlo Park CA.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Duda, Richard</hi>
                  <hi rend=""bold""> O. / Hart, </hi>
                  <hi rend=""bold"">Peter E.</hi> (1973): ""Pattern analysis and scene classification."" 
                        <hi rend=""italic"">J. Wiley</hi> 1:73.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Ester, </hi>
                  <hi rend=""bold"">Martin</hi>
                  <hi rend=""bold""> / Kriegel </hi>
                  <hi rend=""bold"">Hans-Peter /</hi>
                  <hi rend=""bold""> Sander, </hi>
                  <hi rend=""bold"">Jörg</hi>
                  <hi rend=""bold""> / Xu, </hi>
                  <hi rend=""bold"">Xiaowei</hi>
                  <hi rend=""bold""> et al.</hi> (1996): „A density-based algorithm for discovering clusters in large spatial databases with noise.“, in 
                        <hi rend=""italic"">KDD</hi>, volume 96, pages 226–231.
                    </bibl>
               <bibl>Hadersbeck, Maximilian / Pichler, Alois / Fink, Florian / Gjesdal, Oyvind (2014): Wittgenstein’s Nachlass: WiTTFind and Wittgenstein Advanced Search Tools (WAST), DATeH, Madrid.</bibl>
               <bibl>
                  <hi rend=""bold"">Hadersbeck, </hi>
                  <hi rend=""bold"">Maximilian</hi>
                  <hi rend=""bold""> / Still, </hi>
                  <hi rend=""bold"">Sebastian</hi>
                  <hi rend=""bold""> (2018): </hi>
                  <hi rend=""italic"">Investigating Wittgenstein’s Nachlass: WiTTFind, WiTTReader, OdysseeReader and Wittgenstein Advanced Search Tools</hi>, im Katalog zur Ausstellung „DIE TRACTATUS ODYSSEE“ S.127-137, Wittgenstein Initiative, Wien.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Lloyd, </hi>
                  <hi rend=""bold"">Stuart P. </hi>(1982): „Least squares quantization in pcm“, in: 
                        <hi rend=""italic"">IEEE transactions on information theory</hi>, 28(2):129–137.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">MacQueen, </hi>
                  <hi rend=""bold"">J. B.</hi> (1967): „Some methods for classification and analysis of multivariate observations.“, in: 
                        <hi rend=""italic"">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</hi>, volume 1, pages 281–297. Oakland, CA, USA, 1967
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Pichler, Alois / Krüger, Heinz W. / Smith, D. / Bruvik, Tone / Lindebjerg, Anne / Olstad, Vemund </hi>(Hrsg.) (2009): Wittgenstein Source Bergen Facsimile (BTE). Wittgenstein Source Bergen.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Redner, </hi>
                  <hi rend=""bold"">Richard A. /</hi>
                  <hi rend=""bold""> Walker, </hi>
                  <hi rend=""bold"">Homer F. </hi>(1984): Mixture densities, maximum likelihood and the em algorithm. SIAM review, 26(2):195–239.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Röhrer, </hi>
                  <hi rend=""bold"">Ines / </hi>
                  <hi rend=""bold"">Ullrich, </hi>
                  <hi rend=""bold"">Sabine / </hi>
                  <hi rend=""bold"">Hadersbeck, </hi>
                  <hi rend=""bold"">Maximilian </hi>(2019): 
                        <hi rend=""italic"">Weltkulturerbe international digital: Erweiterung der Wittgenstein Advanced Search Tools durch Semantisierung und neuronale maschinelle Übersetzung</hi>. multimedial multimodal. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 25. - 29.03.2019 an den Universitäten zu Mainz und Frankfurt.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Steinhaus, </hi>
                  <hi rend=""bold"">Hans </hi>(1955): Quelques applications des principes topologiques à la géométrie des corps convexes. Fund. Math, 41:284–290.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Ullrich, Sabine /</hi>
                  <hi rend=""bold""> Bruder, Daniel /</hi>
                  <hi rend=""bold""> Hadersbeck, Maximilian </hi>(2018): “Aufdecken von “versteckten” Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning”, 5. Tagung Digital Humanities im deutschsprachigen Raum 26.2.-2.3. (Köln).
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Ullrich, Sabine </hi>(2019):
                        <hi rend=""italic""> Boosting Performance of a Similarity Detection System using State of the Art Clustering Algorithms</hi>. Master‘s thesis. LMU.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Pilch, Martin </hi>(2018): 
                        <hi rend=""italic"">Frontverläufe im Prototractatus – Zur gedanklichen Entwicklung von Krakau bis Sokal (1914/1915)</hi>, Wittgenstein-Studien 9 (S.101-154), Internationale Ludwig Wittgenstein Gesellschaft (ILWG).
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Still, Sebastian </hi>(2018): 
                        <hi rend=""italic"">Ludwig Wittgenstein: 100 Jahre Traktatus. Der Odyssee-Reader, ein web-basiertes Tool zur text-genetischen Suche im Traktatus</hi>, Masterthesis, Ludwig-Maximilians-Universität München.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Feldweg, Birgit /</hi>
                  <hi rend=""bold""> Feldweg, Helmut </hi>(1997): „GermaNet - a Lexical-Semantic Net for German."", in: 
                        <hi rend=""italic"">Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications</hi>. Madrid.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Henrich, Verena / Hinrichs, Erhard </hi>(2010): „GernEdiT - The GermaNet Editing Tool"", in: 
                        <hi rend=""italic"">Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010)</hi>. Valletta, Malta, pp. 2228-2235.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Hörnschemeyer, Jörg / Thaller, Manfred / Förtsch, Reinhard</hi> (2017): 
                        <hi rend=""italic"">Textgenetische Prozesse in Digitalen Editionen</hi>, Köln Universitäts- und Stadtbibliothek Köln 2017, https://www.worldcat.org/title/textgenetische-prozesse-in-digitalen-editionen/oclc/1002260195
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Schmidt, </hi>
                  <hi rend=""bold"">Alfred </hi>(2018): „Ludwig Wittgenstein’s Nachlass in the UNESCO Memory of the World register.“, in: 
                        <hi rend=""italic"">Nordic Wittgenstein Review </hi>7(2):209–213.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">UNESCO</hi> (2017): UNESCO-Weltdokumentenerbe - Zwei
  Neuaufnahmen. URL:
  <ref target=""https://www.unesco.at/presse/artikel/article/unesco-weltdokumentenerbe-zwei-neuaufnahmen/"">https://www.unesco.at/presse/artikel/article/unesco-weltdokumentenerbe-zwei-neuaufnahmen/</ref>
  [letzter Zugriff 19. Juni 2018].
</bibl>
               <bibl>
                  <hi rend=""bold"">Ward, John H. </hi>(1963): „Hierarchical grouping to optimize an objective function."" 
                        <hi rend=""italic"">Journal of the American statistical association</hi> 58.301: 236-244.
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,clustering;edition;nlp;semantik;textgenetik,German,annotieren;inhaltsanalyse;metadaten;transkription
11019,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,Volltexttransformation frühneuzeitlicher Drucke – Ergebnisse und Perspektiven des OCR-D-Projekts,,Matthias Boenig;Elisabeth Engl;Konstantin Baierer;Volker Hartmann;Clemens Neudecker,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Einleitung</head>
            <p style=""text-align:left; "">Das schriftliche Kulturgut des deutschsprachigen Raums aus dem 16.–18. Jahrhundert wird schon seit Jahrzehnten in den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke (VD) zusammengetragen. Ein signifikanter Anteil der verzeichneten Titel wurde der Forschung bereits durch die Bereitstellung von Volldigitalisaten oder einzelnen Schlüsselseiten leichter zugänglich gemacht. Die Verfügbarmachung von Volltexten ist dagegen noch ein Desiderat der Forschung. Das DFG-Projekt OCR-D nimmt sich seit Oktober 2015 im Rahmen der Koordinierten Förderinitiative zur Weiterentwicklung von Verfahren für die Optical Character Recognition (OCR) dieser Aufgabe an, indem es eine modular aufgebaute Open Source-Software entwickelt, deren Werkzeuge alle für die Texterkennung nötigen Schritte abdecken sollen. Der modulare Ansatz ermöglicht es, die technischen Abläufe und Parameter der Texterkennung stets nachzuvollziehen und maßgeschneiderte Workflows zu definieren, die jeweils optimale Ergebnisse für spezifische Titel aus dem Zeitraum des 16. bis 19. Jahrhunderts liefern. Zudem werden Antworten auf die damit verbundenen konzeptionellen, informationswissenschaftlichen und organisatorischen Fragen gefunden.</p>
            <p style=""text-align:left; "">Künftig sollen mithilfe der OCR-D-Software Volltexte generiert werden, die zum einen von Forschenden zur Recherche verwendet werden können. Zum anderen könnten diese zum Ausgangspunkt für Studien im Bereich der Digital Humanities (DH) werden, wobei auch auf diese Texte die textkritische Methode anzuwenden ist. Gerade bei einer automatisierten Weiterverarbeitung der erzeugten Volltexte ist es für Forschende unerlässlich, die Genese der von ihnen verwendeten Daten kritisch zu hinterfragen. Nur so können Eigenheiten der Daten, die Resultat von zuvor genutzten “Spielräumen” sind, von DH-Forschenden erkannt und in ihrem Umgang mit der Datengrundlage berücksichtigt werden. Nicht nur diese interpretatorischen Spielräume sind zu betrachten, sondern auch, welche konkreten Implementierungen den DH die gewünschten “Spielräume“ für die Erkenntnisgenerierung geben. Im Folgenden wird in vier Thesen eine notwendige Begrenzung der Spielräume vorgenommen. Diese Begrenzung ergibt sich aus dem Vergleich mit anderen Projekten und der heute gängigen Praxis. Ziel ist es, den Forderungen der DH nach qualitativ hochwertigen Volltexten gerecht zu werden. </p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Im Rückblick</head>
            <p style=""text-align:left; "">Das Projekt hat sich in den vergangenen vier Jahren mit verschiedenen Themen auf der DHd zur Diskussion gestellt (Boenig et al 2016; Boenig et al 2018; Baierer et al 2019). Zu Beginn standen methodische Fragen, wie die Textqualität erhöht werden kann. Dabei wurden statistische Methoden vorgestellt, die auf Basis eines Vergleichs von mindestens zwei erstellten Textfassungen entwickelt wurden. Im Rahmen des Themas “Kritik der digitalen Vernunft” wurden die DH befragt, wie in den Geisteswissenschaften Ergebnisse ohne Ground Truth und Referenzdaten gewonnen bzw. verifiziert werden. Diesem Desiderat begegnete das Projekt OCR-D mit dem Vorschlag von Transkriptionsrichtlinien für die Erfassung von Ground Truth-Daten<ref n=""1"" target=""ftn1""/> und in der Folge mit der Definition von spezifischen Metadaten. Bei dem 2019 veranstalteten Workshop konnten Wissenschaftler und Wissenschaftlerinnen sowie Interessierte Einblicke in den OCR-D-Workflow erhalten. An Beispielen konnten die Möglichkeiten der Software demonstriert und getestet werden. Die Diskussion, Hinweise und Fragen wurden soweit wie möglich in OCR-D umgesetzt. 
                </p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Thesen</head>
            <p style=""text-align:left; "">Das Ziel der prototypischen Implementierung des OCR-D-Workflows und damit der Generierung von Forschungsdaten, die sich durch eine erkennbare XML-Strukturierung sowie eine hohe Zeichen- und Textqualität auszeichnen, wird im ersten Quartal 2020 erreicht werden. Dies stellt jedoch nicht das Ende des Weges dar, sondern eher den Beginn der nun folgenden Volltexttransformation. Letztlich besteht die Aufgabe darin, ca. 1 Mio. frühneuzeitliche Titel mit ca. 250 Mio. Seiten, die zum Teil bereits als Bilddigitalisate vorliegen, zu Volltextdigitalisaten zu transformieren.</p>
            <p style=""text-align:left; "">1. Die Volltexttransformation der Bestände stellt eine Herausforderung für Bibliotheken und Archive dar. Die vorhandenen institutionellen und interinstitutionellen Vorgehensweisen und Konventionen sind möglichst zentral aufeinander abzustimmen, damit die Aufgabe in absehbarer Zeit gelöst wird.<ref n=""2"" target=""ftn2""/>
            </p>
            <p style=""text-align:left; "">Es gibt bereits einige Projekte, in denen (Teil-)Bestände und Sammlungen volltextdigitalisiert wurden.<ref n=""3"" target=""ftn3""/> Deren Nutzen für die DH wird jedoch v.a. durch zwei Faktoren begrenzt: Zum einen weisen die erstellten Volltexte aufgrund fehlender Standards bzw. Konventionen im Bereich von Text- und Strukturerkennung eine große Bandbreite in der Transkription der Texte und der Benennung von Textstrukturen auf, die deren automatisierte Auswertung und Bearbeitung durch die DH erschweren. Zum anderen gibt es bislang keine zentrale Anlaufstelle, die die Bereitstellung und auch die Erstellung von Volltexten steuert. Dadurch sind die existierenden Volltexte sowohl für die Forschung, als auch für die volltextdigitalisierenden Einrichtungen weniger sichtbar, was die Gefahr aufwändiger und teurer Doppelarbeiten erhöht.
                </p>
            <p style=""text-align:left; "">2. Die Volltexttransformation auf Basis von Erkennungssoftware, die neuronale Netze nutzt, setzt Trainingsdaten voraus. Diese fundamental wichtigen Daten sind systematisch aus vorhandenen Ressourcen zu gewinnen und aktiv zu erweitern.</p>
            <p style=""text-align:left; "">Mit ihren Förderinitiativen von 2010 und 2013 hat die DFG die Bedeutung der Forschungsdaten und des zugehörigen Managements erkannt.<ref n=""4"" target=""ftn4""/> Heute sollten Projekte von Beginn an mit entsprechenden Forschungsdatenmangementplänen aufgesetzt und die entstehenden Daten in den zuvor bereitgestellten Repositorien verwahrt werden.<ref n=""5"" target=""ftn5""/> Gerade bei der automatisierten Texterfassung im Rahmen von Editionsprojekten werden in der Regel aber nur die abschließend bearbeiteten und korrigierten Daten veröffentlicht. Eine Nachnutzung dieser Daten ist in vielfacher Hinsicht nur begrenzt möglich. Dabei spielt nicht nur das Format der Daten, sondern auch die Methodik der Datenerfassung eine entscheidende Rolle. Für die Nachnutzung ist eine Transformation dieser Daten nötig, die entweder von den Nutzenden zu leisten ist, oder von den bestandsverwaltenden Einrichtungen angeboten werden könnte. Um eine solche Transformation zu gewährleisten, sind sowohl Richtlinien als auch entsprechende Metadaten zu etablieren, damit vergleichbare und konsistente Daten bereitgestellt werden können.<ref n=""6"" target=""ftn6""/>
            </p>
            <p style=""text-align:left; "">3. Die Volltexttransformation wird für einen Teil der Dokumente ein Prozess sein, der sich über einen größeren Zeitraum wiederholt.</p>
            <p style=""text-align:left; "">Digitale Daten müssen beständig gepflegt und aktualisiert werden. Dies haben auch Bibliotheken als Herausforderung der digitalen Transformation ihrer Bestände erkannt (vgl. Kempf 2015: 277–278). Werden lernende Systeme für die Text- und Strukturerkennung genutzt, können diese in absehbaren Intervallen verbessert werden.<ref n=""7"" target=""ftn7""/> Denn die Verbesserung bestehender Algorithmen sowie die Nutzung zusätzlicher oder verbesserter Trainingsdaten führt auch zu besseren Ergebnissen in der Text- und Strukturerkennung, wie sich beispielsweise im GoogleBooks-Projekt<ref n=""8"" target=""ftn8""/> zeigt. Diese wiederkehrende Prozessierung muss konzeptionell berücksichtigt werden.
                </p>
            <p style=""text-align:left; "">4. Die Volltexttransformation muss in ihrer Qualität von den Nutzenden beurteilbar sein.</p>
            <p style=""text-align:left; "">Bibliotheken geben den Nutzenden mit ihrem Bestand und dessen Erschließung ein Qualitätsversprechen. Die Nutzenden können sich auf die vorhandenen Daten verlassen und sie z.B. in Bibliographien verwenden. Das Volltextangebot aus der automatischen Texterkennung kann dagegen häufig nur unpräzise als “schmutzige OCR”<ref n=""9"" target=""ftn9""/> bezeichnet werden. Diese pauschale Angabe ermöglicht den DH keine verlässliche Qualitätseinschätzung und führt dazu, dass Volltextbestände oft a priori als minderwertig eingeschätzt werden. Daher besteht die Gefahr, dass projektintern eine erneute Volltextdigitalisierung durchgeführt wird, die nicht immer sinnvoll ist, da die Erkennung teilweise nur durch eine Korrektur verbessert werden könnte. Oder es könnten im umgekehrten Fall auf Grund einer ungenauen bzw. zu groben Einschätzung aufwendige Korrekturen vorgenommen werden. In beiden Fällen werden finanzielle und personelle Ressourcen verschwendet.
                </p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Lösungen und Desiderate des OCR-D-Projekts</head>
            <p style=""text-align:left; "">
               <hi rend=""bold"">Zu 1:</hi> Die bisherigen umfassenden Bilddigitalisierungsarbeiten im VD17 wurden über einen Masterplan gesteuert, um die große Anzahl an Titeln effizient, in nachnutzbarer Form verarbeiten zu können und Doppelarbeiten zu vermeiden. Ein ähnliches Vorgehen, bei dem die zu prozessierenden Titel an interessierte Einrichtungen verteilt werden, dürfte auch für die Volltexttransformation der VD zielführend sein. Die Voraussetzungen und Rahmenbedingungen für diese Arbeiten wurden von dem OCR-D-Koordinierungsprojekt um die Jahreswende 2019/2020 durch eine Umfrage mit den VD-Bibliotheken zusammengetragen. OCR-D wird die mehrjährige Projekterfahrung im Austausch mit den verschiedenen Stakeholdern nutzen, um die Nachnutzbarkeit von Daten und Abläufen zu verbessern, sowohl mit technischer Dokumentation und Best Practices, als auch als Katalysator für einen ergebnisorientierten, inklusiven Diskurs zur Etablierung von Standards.</p>
            <p style=""text-align:left; "">
               <hi rend=""bold"">Zu 2:</hi> Für die Transkription von Texten gibt es unzählige Richtlinien, die von verschiedenen Fächern, Arbeitskreisen und Forschungsprojekten entsprechend ihrer jeweiligen Anforderungen aufgestellt und wiederum an die spezifischen Erfordernisse bestimmter Transkriptionsprojekte angepasst wurden. Bei diesen Gruppen ist zum einen ein Bewusstsein dafür zu schaffen, ihre Transkriptionen auch mit Blick auf deren Nachnutzbarkeit durch andere Projekte anzufertigen. Zum anderen sind interdisziplinär erarbeitete und gültige Transkriptionsrichtlinien ein großes Desiderat der Forschung. Erste Impulse hierfür könnten große Fördergeber wie bspw. die DFG geben, indem Praxisrichtlinien geschaffen werden, die von Antragstellern zu beachten sind. Das OCR-D-Projekt ist zudem darum bemüht, seine auf Grundlage des DTA-Basisformats erstellten Transkriptionsrichtlinien interdisziplinär zur Nutzung durch weitere Projekte zu kommunizieren.</p>
            <p style=""text-align:left; "">
               <hi rend=""bold"">Zu 3:</hi> Modelltraining mit tesstrain und okralact</p>
            <p style=""text-align:left; "">Das Projekt ocropy, die Python-Implementierung von Tom Breuels OCRopus-Projekt, brachte neben Werkzeugen für die Text- und Strukturerkennung auch Werkzeuge für das Erstellen von Ground Truth und das Trainieren neuer Modelle mit sich. Mit diesen Werkzeugen und einigen Anpassungen lassen sich auch die auf ocropy basierenden Weiterentwicklungen Calamari und Kraken trainieren. Insbesondere für tesseract, die mit Abstand am meisten genutzte Open Source OCR, gab es bis 2018 kaum Dokumentation oder Tooling für das Training. Daher wurde im Rahmen von OCR-D 
<hi rend=""italic"">ocrd-train</hi> entwickelt, eine Makefile-basierte Lösung zum Trainieren von Tesseracts LSTM-Engine, das inzwischen unter dem Namen 
<hi rend=""italic"">tesstrain</hi> vom Tesseract-Entwicklerteam gepflegt und weiterentwickelt wird.<ref n=""10"" target=""ftn10""/> Die Aufrufe zum Training von Texterkennungsmodellen und insbesondere das Inventar an freien Parametern sind allerdings in hohem Maße engine-spezifisch, keineswegs trivial und erfordern zur optimalen Feinadjustierung manuelle Intervention. Daher entwickelt OCR-D seit 2019 das Werkzeug okralact,<ref n=""11"" target=""ftn11""/> das über ein komfortables Webinterface und ein skalierbares Backend ein Training aller relevanter Open Source OCR-Engines mit einem einheitlichen Interface ermöglichen wird.
                </p>
            <p style=""text-align:left; "">
               <hi rend=""bold"">Zu 4:</hi> Nachkorrektur und Qualitätsanalyse</p>
            <p style=""text-align:left; "">Innerhalb des OCR-D-Projektes beschäftigen sich zwei Projekte mit der automatischen, bzw. semi-automatischen Nachkorrektur von OCR-Texten. Das Hauptproblem dabei ist es, historische Schreibweisen und Druckfehler von OCR-Fehlern zu unterscheiden. Für moderne Texte würde eine reine Rechtschreiberkennung genügen, wie sie in jedem Textverarbeitungsprogramm verfügbar ist. Die Projekte kooperieren und haben verschiedene Verfahren entwickelt, basierend auf einem Fehler-Profiler, neuronalen Netzen oder endlichen Automaten. Als trainierbare Algorithmen werden sie, analog zur Struktur- und Texterkennung, mit mehr und besseren Trainingsdaten bessere Ergebnisse liefern. Was ""besser"" bedeutet ist noch Gegenstand der Forschung. OCR-D bringt sich in die Entwicklung ein und unterstützt tatkräftig Projekte wie dinglehopper<ref n=""12"" target=""ftn12""/> (ein Werkzeug zur Fehlervisualisierung). Gerade im Bereich der Ground-Truth-freien Evaluation von Text und der Qualitätsanalyse von Strukturdaten gibt es noch große Lücken im Software-Portfolio, die zu schließen sich OCR-D auch weiterhin befleißigen wird.
                </p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Ausblick</head>
            <p style=""text-align:left; "">Ab der ersten Jahreshälfte 2020 werden die entwickelten Software-Komponenten im OCR-D-Workflow verankert sein. Damit tritt diese Software immer mehr aus dem Projektstadium heraus und wird in den produktiven Einsatz überführt. Um kontinuierlich gute Erkennungsergebnisse mit dem aus fast vier Jahrhunderten stammenden Material zu erhalten, sind Optimierungen notwendig. Dabei wird stets darauf abgezielt, Forschungsdaten aus den digitalen Beständen der Bibliotheken zu erzeugen und nicht unstrukturierte Textdaten. So wird die Volltexttransformation in einem umfassenden Maße Grundlagen für datenzentrierte Digital Humanities schaffen.</p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note n=""1"" rend=""footnote text"" id=""ftn1"">
      Im Kontext von OCR bezeichnet 
      <hi rend=""italic"">Ground Truth</hi> manuell korrigierte, fehlerfreie Transkriptionen. Diese werden zum einen für das Training von OCR-Engines, zum anderen für die Evaluation der OCR-Ergebnisse benötigt.
    </note>
            <note n=""2"" rend=""footnote text"" id=""ftn2"">
      Der Gedanke folgt der neunten Empfehlung (“Establish an ‘OCR Service Bureau’”) aus dem Report von Smith und Cordell (2018).
    </note>
            <note n=""3"" rend=""footnote text"" id=""ftn3"">
      Vgl. bspw. die folgenden Projekte, die sich auf unterschiedlich große (Teil-)Bestände beziehen: Helmstedter Drucke Online:
      <ref target=""http://www.hab.de/de/home/wissenschaft/forschungsprofil-und-projekte/helmstedter-drucke-online.html"">http://www.hab.de/de/home/wissenschaft/forschungsprofil-und-projekte/helmstedter-drucke-online.html</ref>;
      Über 14.000 preußische Drucke des 17. Jahrhunderts online verfügbar:
      <ref target=""https://blog.sbb.berlin/ueber-14-000-preussische-drucke-des-17-jahrhunderts-online-verfuegbar/"">https://blog.sbb.berlin/ueber-14-000-preussische-drucke-des-17-jahrhunderts-online-verfuegbar/</ref>;
      Projekt Digi20
      <ref target=""https://digi20.digitale-sammlungen.de/de/fs1/about/static.html"">https://digi20.digitale-sammlungen.de/de/fs1/about/static.html</ref>
            </note>
            <note n=""4"" rend=""footnote text"" id=""ftn4"">
      Nachdem im Jahr 2010 der Aufbau von Infrastrukturen für Forschungsdaten von der DFG ausgeschrieben worden war, wurde drei Jahre später das Förderprogramm „Informationsinfrastrukturen für Forschungsdaten“ eingerichtet. Vgl. DFG 2019: 7.
    </note>
            <note n=""5"" rend=""footnote text"" id=""ftn5"">
      Zur aktuellen Situation des Datenmanagements und der Rolle, die Bibliotheken in diesem Bereich einnehmen (könnten), vgl. Neuroth et al 2019.
    </note>
            <note n=""6"" rend=""footnote text"" id=""ftn6"">
      Die Notwendigkeit einheitlicher Richtlinien wird besonders an Projekten wie “Venice Time Machine” deutlich, dessen bereits vorhandenen 8 TB an Daten aufgrund fehlender einheitlicher Richtlinien und Vorgehensweisen bei der Erfassung der Metadaten für die Forschung vermutlich wertlos sind. Vgl. Castelvecchi 2019: 607.
    </note>
            <note n=""7"" rend=""footnote text"" id=""ftn7"">
      Kempf geht davon aus, dass mit OCR-Software nie völlig fehlerfreie Volltexte generiert werden können. Vgl. Kempf 2015: 274.
    </note>
            <note n=""8"" rend=""footnote text"" id=""ftn8"">
      Während die OCR-Ergebnisse im Rahmen des GoogleBooks-Projekts zunächst insgesamt unbefriedigend, für gebrochene Schriften vollkommen unbrauchbar waren, konnten ab dem Jahr 2008 einzelne Frakturtexte in ausreichender Qualität prozessiert werden. In den letzten Jahren konnte die Erkennungsrate noch deutlich gesteigert werden. Vgl. Wikisource: Google Book Search.
    </note>
            <note n=""9"" rend=""footnote text"" id=""ftn9"">
      Bspw. gibt Google die Fehlerquote im Google Books pauschal mit 1,37 % an (vgl. Kempf 2015: 272). Diese für die wissenschaftliche Nutzung hohe Fehlerrate unterscheidet sich, bedingt durch die Vielfalt an Typen und Layouts sowie den großen Publikationszeitraum der digitalisierten Bücher, von Text zu Text deutlich.
    </note>
            <note n=""10"" rend=""footnote text"" id=""ftn10"">
               <ref target=""https://github.com/tesseract-ocr/tesstrain"">https://github.com/tesseract-ocr/tesstrain</ref>
            </note>
            <note n=""11"" rend=""footnote text"" id=""ftn11"">
               <ref target=""https://github.com/OCR-D/okralact"">https://github.com/OCR-D/okralact</ref>
            </note>
            <note n=""12"" rend=""footnote text"" id=""ftn12"">
               <ref target=""https://github.com/qurator-spk/dinglehopper"">https://github.com/qurator-spk/dinglehopper</ref>
            </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Baierer, Konstantin / Boenig, Matthias / Hartmann, Volker / Hermann, Elisa / Neudecker, Clemens</hi> (2019): 
	<hi rend=""italic"">„Vom gedruckten Werk zu elektronischem Volltext als Forschungsgrundlage“</hi> (Workshop)
	(<ptr target=""https://zenodo.org/record/2596095/files/2019_DHd_BookOfAbstracts_web.pdf""/>, S. 58).
      </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Boenig, Matthias / Würzner, Kay-Michael / Binder, Arne / Springmann, Uwe </hi> (2016): 
	<hi rend=""italic"" space=""preserve"">„Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts.“ </hi>
	Vortrag auf der DHd 2016, 7.12.03.2016 in Leipzig (<ptr target=""http://dhd2016.de/boa.pdf#page=103""/>).
      </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Boenig, Matthias / Federbusch, Maria / Herrmann, Elisa / Neudecker, Clemens / Würzner, Kay-Michael </hi>(2018):
	<hi rend=""italic""> „Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?“</hi>.
	Vortrag auf der DHd2018, 28.02.2018 in Köln
	(<ptr target=""http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf#page=221""/>).
      </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Castelvecchi, Davide</hi> (2019): 
      <hi rend=""italic"">“Venice ‘Time Machine’ Project Suspended amid Data Row. Disagreements between International Partners Leave Plans to Digitize the Italian City’s History in Limbo”</hi> in: 
      <hi rend=""italic"">Nature</hi> 574: 607.
      </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">DFG </hi> (2019): 
	<hi rend=""italic"">„Weiterentwicklung des Förderprogramms ‚Informationsinfrastrukturen für Forschungsdaten‘""</hi>
                  <ptr target=""https://zenodo.org/record/2650866""/> [6.3.2019 / 26.9.2019].
      </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Kempf, Klaus</hi> (2015): 
      <hi rend=""italic"">„Data Curation oder (Retro-)Digitalisierung ist mehr als die Produktion von Daten“</hi> in: 
      <hi rend=""italic"">o-bib</hi> 4: 268–278.
      </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Neuroth, Heike / Rothfritz, Laura / Petras, Vivien / Kindling, Maxi</hi>  (2019): “Digitales Datenmanagement als neue Aufgabe für wissenschaftliche Bibliotheken” in: Bibliothek. Forschung und Praxis 43: 421–431.</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Smith, David A. / Cordell, Ryan </hi> (2018): 
	<hi rend=""italic"">“A Research Agenda for Historical and Multilingual Optical Character Recognition”</hi>
                  <ptr target=""http://hdl.handle.net/2047/D20297452""/> [9.12.2019].
      </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Wikipedia, Die freie Enzyklopädie</hi>  (2019): 
	<hi rend=""italic"">„Google Books“</hi>.
	<ptr target=""https://de.wikipedia.org/w/index.php?title=Google_Books&oldid=189583765""/> [16.6.2019 / 25.9.2019].
      </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Wikisource </hi> (2019): 
	<hi rend=""italic"">“Google Book Search”</hi>
                  <ptr target=""https://de.wikisource.org/wiki/Wikisource:Google_Book_Search""/> [22.8.2019 / 26.9.2019].
      </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,frühe neuzeit;ocr-d;volltexterkennung,German,bilderfassung;programmierung;software;transkription;umwandlung
11020,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,SubRosa – Multi-Feature-Ähnlichkeitsvergleiche von Untertiteln,,Jan Luhmann;Manuel Burghardt;Jochen Tiepmar,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Einleitung: Filmanalyse auf Basis von Untertiteln</head>
            <p>Mit der stetig wachsenden Verfügbarkeit von Filmen und Serien, die durch Streaming-Dienste wie Netflix und Amazon Prime in den letzten Jahren weiter befördert wurde, ergeben sich aus Perspektive der Filmanalyse ganz neue Möglichkeiten für quantitative Untersuchungen im Sinne des 
<hi rend=""italic"">distant viewing</hi> (Arnold & Tilton, 2019). Wenngleich Film zunächst vor allem ein visuelles Medium ist, so werden in zunehmendem Maße auch Metadaten und insbesondere die Dialoge (vgl. Kozloff, 2000) in Form von online verfügbaren Untertiteln, Drehbüchern und Fan-Transkripten Gegenstand quantitativer Untersuchungen (vgl. Bednarek, 2020; Burghardt et al., 2016, 2019; Schmidt, 2014). Insbesondere die freie Datenbank 
<hi rend=""italic"">OpenSubtitles</hi>
               <ref n=""1"" target=""ftn1""/> hat sich hier als ertragreiche Datenquelle bewährt. Während die Daten von 
<hi rend=""italic"">OpenSubtitles</hi> bislang vor allem im Bereich maschineller Übersetzung (vgl. Müller & Volk, 2013; Lison & Tiedemann, 2016; Tiedemann, 2016) Verwendung fanden, schlagen wir in diesem Artikel eine Nutzung im Sinne quantitativer Filmstilanalyse basierend auf Ähnlichkeitsvergleichen vor. Wir erweitern damit bestehende Arbeiten (Blackstock & Spitz, 2008; Nessel & Cimpa, 2011; Bougiatiotis & Giannakopolous, 2017), die sich ebenfalls mit Ähnlichkeitsvergleichen von Untertiteln beschäftigen, dabei aber jeweils mit relativ überschaubaren Korpora arbeiten oder sehr spezifische Ansätze der Ähnlichkeitsberechnung umsetzen. 
</p>
            <p>Wir präsentieren das experimentelle Analysetool 
<hi rend=""italic"">SubRosa</hi>, welches Ähnlichkeitsvergleiche für mehrere tausend Untertitel über eine grafische Benutzeroberfläche erlaubt. Wir setzen dabei eine ganze Reihe von Features für die Ähnlichkeitsberechnung zwischen Untertiteln um, die zudem jeweils individuell gewichtet werden können. 
<hi rend=""italic"">SubRosa</hi> versteht sich damit als exploratives Werkzeug, um die grundlegende Eignung unterschiedlicher Features bzw. Feature-Kombinationen für die computergestützte Ähnlichkeitsberechnung zwischen Untertiteln zu untersuchen, welche dann wiederum in einem nächsten Schritt für großangelegte Ähnlichkeitsvergleiche mithilfe statistischer Verfahren genutzt werden können.
</p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Korpus und Datenaufbereitung</head>
            <p>
               <hi rend=""italic"">SubRosa</hi> stellt Vergleiche zwischen insgesamt 5.896 englischen Untertiteln an, die über 
<hi rend=""italic"">OpenSubtitles</hi> bezogen wurden. 
<hi rend=""italic"">OpenSubtitles</hi> versteht sich als offene Plattform, bei der Nutzer*innen Untertitel in unterschiedlichen Sprachen für unterschiedliche Filme hochladen können. Das Format der Untertitel entspricht dem Exportformat des 
<hi rend=""italic"">SubRip</hi>-Tools, welches automatisiert über OCR Textzeilen aus Filmen mit bereits bestehenden Untertiteln extrahiert. Darüber hinaus werden aber auch viele von Nutzer*innen selbst transkribierte Untertitel hochgeladen. Im Ergebnis gibt es so für die meisten Filme mehrere Versionen von Untertiteln. Wir wählen jeweils die Version für unser Korpus aus, die einer automatischen Validierung in Hinblick auf Encoding- oder OCR-Fehler Stand hält. Weiterhin werden alle ausgewählten Untertitel grundlegend aufbereitet, d.h. es werden bspw. Metainformationen, Autoren-Tags, etc. entfernt, die definitiv nicht Teil des eigentlichen Filmdialogs sind. Als nächstes erfolgt eine Vorverarbeitung der Untertitel im Sinne des 
<hi rend=""italic"">natural language processing</hi> (NLP), welche die folgenden Einzelschritte enthält: Tokenisierung, Satzsegmentierung, Lemmatisierung, POS-Tagging und 
<hi rend=""italic"">named entity recognition</hi>. Zuletzt werden alle Untertitel mit Metadaten wie etwa „Titel“, „Jahr der Veröffentlichung“, „Genre“, etc. verknüpft, die über die IMDb-Datenbank<ref n=""2"" target=""ftn2""/> bezogen werden.
</p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Analyseverfahren</head>
            <p>Mit 
<hi rend=""italic"">SubRosa</hi> setzen wir einen parametrisierbaren Ähnlichkeitsvergleich zwischen Filmuntertiteln um, der auf ganz unterschiedlichen Features basiert. Die nachfolgenden Features sind allesamt über eine interaktiven Web-Applikation verfügbar, die eine Ähnlichkeitssuche für die eingangs erwähnten annähernd 6.000 englischsprachigen Film-Untertitel erlaubt.
</p>
            <list type=""unordered"">
               <item>
                  <hi rend=""bold"">SubRosa Code</hi>:
    <ref target=""http://github.com/bbrause/subrosa"">http://github.com/bbrause/subrosa</ref>
               </item>
               <item>
                  <hi rend=""bold"">SubRosa Live-Demo</hi>:
    <ref target=""http://ch01.informatik.uni-leipzig.de:5001/"">http://ch01.informatik.uni-leipzig.de:5001/</ref>
               </item>
            </list>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Features auf der inhaltlichen Ebene</head>
               <p>
                  <hi rend=""bold"" space=""preserve"">a) Bag of words / tf-idf </hi>(„Worüber sprechen die Figuren?“): Das 
    <hi rend=""italic"">bag of words</hi>-Modell ist ein einfacher Ansatz für die Repräsentation von Textdokumenten im NLP und Information Retrieval. In unserem Anwendungskontext entspricht ein Untertitel einem „Dokument“, für das die einzelnen lemmatisierten Tokens jeweils mit einer sublinearen tf-idf-Skalierung (Manning et al., 2008, S. 126-127) gewichtet werden. Durch diese Gewichtung können wir diejenigen Wörter identifizieren, die in einem bestimmten Dokument häufig vorkommen, aber insgesamt im Gesamtkorpus nur selten auftreten. Es kann davon ausgegangen werden, dass diese Begriffe für das jeweilige Dokument dann besonders aussagekräftig sind. Dementsprechend filtern wir alle Begriffe heraus, die in weniger als 2,5% und mehr als 95% aller Dokumente vorkommen. Darüber hinaus werden 
    <hi rend=""italic"">named entities</hi>, die Personen-, Orts- oder Institutionsnamen bezeichnen, entfernt, da diese die Ergebnisse stark verzerren können. Es verbleiben insgesamt 4.952 Wörter, die beim Ähnlichkeitsvergleich der Untertitel berücksichtigt werden.
  </p>
               <p>
                  <hi rend=""bold"" space=""preserve"">b) Sentiment Analyse </hi>(„Was fühlen die Figuren?“): Um Muster bzgl. der von den Figuren im Dialog zum Ausdruck gebrachten Gefühle und Emotionen automatisch zu detektieren, wurde das weitverbreitete 
    <hi rend=""italic"">open source</hi>-Tool 
    <hi rend=""italic"">VADER Sentiment</hi> (Hutto & Gilbert, 2014) verwendet. Dabei werden für beliebige Textabschnitte Sentiment-Bewertungen im Bereich -1 (maximal negativ) bis +1 (maximal positiv) berechnet. Da sich Emotionen im Laufe eines Films meist sehr divers entwickeln, ist es nicht sinnvoll, das Sentiment des gesamten Filmdialogs mit einem einzigen Wert wiederzugeben. Stattdessen berechnen wir für jede Sekunde eines Films einen spezifischen Sentiment-Wert für den dort gesprochenen Dialog, sodass sich für jeden Film eine Zeitreihe von Sentiment-Werten ergibt. Als Features dieser Zeitreihen extrahieren wir den Mittelwert und Quartilswerte, um die Verteilung der Sentiment-Werte zu erfassen. Weiterhin wird die Nulldurchgangsrate der Zeitreihenkurve sowie deren erste und zweite Ableitung ausgewertet, um Hinweise auf periodische Eigenschaften zu erlangen.
  </p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Features auf der stilistischen Ebene („Wie sprechen die Figuren?“)</head>
               <p>
                  <hi rend=""bold"">a) Stoppwort-Verteilung</hi>: Als weitere Features implementieren wir eine Analyse der Verteilung von Stoppwörtern, also von Wörtern, die in unserem Korpus am häufigsten auftreten und im Gegensatz zum vorherigen Ansatz nur geringe inhaltliche Aussagekraft für einen Film besitzen. Wir berücksichtigen insgesamt 87 Stoppwörter, die nach ihrer Termfrequenz gewichtet werden. 
</p>
               <p>
                  <hi rend=""bold"">b) POS-Trigramme</hi>: Darüber hinaus setzen wir einen Ansatz von Argamon et al. (2003) und Santini (2004) um, die im Kontext stilometrischer Genreklassifikation mit POS-Trigrammen arbeiten. Wir ignorieren dabei all die POS-Trigramme, die in weniger als 90% unserer Dokumente vorkommen, was zu insgesamt 417 verbleibenden POS-Trigrammen führt. Gewichtet werden diese ebenfalls nach ihrer Termfrequenz.
</p>
               <p>
                  <hi rend=""bold"">c) Statistische Maße</hi>: Wir berechnen außerdem verschiedene statistische Maße, die im Bereich der Stilometrie weit verbreitet sind und die als weitere Features bei unserer Ähnlichkeitsberechnung verwendet werden können. Zu diesen Maßen zählen die Durchschnittswerte einfacher Wort- und Satzlängen sowie auch die 
  <hi rend=""italic"">Entropie</hi> (Shannon, 1948) und die 
  <hi rend=""italic"">standardized type-token ratio</hi> (Johnson, 1944; Torruella & Capsada, 2013).
</p>
               <p>
                  <hi rend=""bold"">d) Dialogtempo</hi> („Wie schnell bzw. wie viel wird gesprochen?“): Als letztes Feature betrachten wir das „Dialogtempo“, das sich allerdings nicht auf die Sprechgeschwindigkeit einzelner Figuren bezieht, sondern vielmehr Dialoganteile pro Zeit misst. Analog zum Verfahren bei unserem Modell der Sentiment-Analyse messen wir hier pro Sekunde eines Films die Anzahl der gesprochenen Wörter, sodass sich je Film eine Zeitreihe ergibt. Als Features der Zeitreihen extrahieren wir ebenfalls Mittelwert und Quartilswerte zur Erfassung der Verteilung der Dialogtempo-Werte, sowie die Rate der Mittelwertdurchgänge jeder Zeitreihe und Nulldurchgangsraten der ersten und zweiten Ableitung zur Abschätzung von periodischen Eigenschaften.
</p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Ähnlichkeitsberechnung</head>
               <p>Für alle Untertitel werden anhand der oben genannten Feature-Modelle entsprechende Ergebnisvektoren berechnet (vgl. Abb. 1). Ähnlichkeiten bzw. Distanzen werden pro Modell separat berechnet. Für das 
  <hi rend=""italic"">bag of words</hi>-Modell verwenden wir die
  Cosinus-Ähnlichkeit als Metrik, für alle anderen Modelle die
  Cosinus-Delta-Metrik, die der Cosinus-Ähnlichkeit auf
  standardisierten Feature-Werten
  (<hi rend=""italic"">z-Scores</hi>) entspricht und auch häufig in der Stilometrie Verwendung findet. Ein Gesamtähnlichkeitswert zwischen zwei Filmen, wie er in 
  <hi rend=""italic"">SubRosa</hi> letztendlich ablesbar ist, wird
  berechnet als der gewichtete Mittelwert der Ähnlichkeitswerte aus
  den einzelnen Modellen.
  Darüber hinaus ist eine spezifische Gewichtung (jeweils 0 - 100%) der einzelnen Features über das Interface des Webtools
  <hi rend=""italic"">SubRosa</hi> möglich.
  </p>
               <figure>
                  <graphic height=""5.346422222222222cm"" n=""1001"" rend=""inline"" url=""181_final-e738308e6bfa4c1038fde72d6052470a.png"" width=""10.664636111111111cm""/>
                  <head>Abbildung 1: Anzahl der Dimensionen je Feature-Modell.</head>
               </figure>
            </div>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Ergebnisse in SubRosa</head>
            <p>Wie eingangs beschrieben versteht sich 
  <hi rend=""italic"">SubRosa</hi> als exploratives Tool um die Auswirkung unterschiedlicher Features auf die Ähnlichkeitsberechnungen zwischen Untertiteln zu untersuchen. Zur besseren Illustration der Möglichkeiten des Tools zeigt Abb. 2 die grafische Benutzeroberfläche von 
  <hi rend=""italic"">SubRosa</hi> mit einer Darstellung ähnlicher Filme zum Film „Alien (1979)“. Auf der linken Seite zu sehen sind die unterschiedlichen Feature-Modelle und deren Gewichtung, die sich jeweils auf die Ergebnisdarstellung auswirken. Die Ergebnisse der Ähnlichkeitsberechnungen zwischen den Filmen werden in einem Graphen visualisiert, in dem jeder Knoten einen Film darstellt und die Länge der Kante zwischen jeweils zwei Filmen näherungsweise proportional zum Quadrat der zwischen ihnen berechneten Distanz ist. 
  </p>
            <figure>
               <graphic height=""11.329458333333333cm"" n=""1002"" rend=""inline"" url=""181_final-0da7b3c93deaf393135516d583683cc7.png"" width=""16.002cm""/>
               <head>Abbildung 2: Ähnlichkeitsnetzwerk für Alien (1979) in <hi rend=""italic"">SubRosa</hi>.</head>
            </figure>
            <p>In Detailansichten (vgl. Abb. 3) für jedes Feature-Modell lassen sich darüber hinaus für jeden einzelnen Film seine extrahierten Feature-Daten analysieren und mit denen anderer Filme vergleichen.</p>
            <figure>
               <graphic height=""9.416561111111111cm"" n=""1003"" rend=""inline"" url=""181_final-768e472c13ef402fdd1cf6732e1c8f27.png"" width=""16.002cm""/>
               <head>Abbildung 3: Detailsicht der einzelnen Feature-Modelle für Alien (1979), hier für die beispielhaften Features „Sentiment Analyse“ und „Dialogtempo“.</head>
            </figure>
            <p>Um einen Überblick zu allgemeinen Ähnlichkeitsmustern im Sinne von Cluster-Bildung innerhalb unseres Korpus an Untertiteln zu erlangen, haben wir zudem den hochdimensionalen Vektorraum jedes Modells mithilfe einer SVD (singular value decomposition) reduziert und die Ergebnisse mittels t-SNE (t-distributed stochastic neighbor embedding) in einem zweidimensionalen Raum als Punkte visualisiert, die entsprechend der Filmgenres eingefärbt sind. Beispielhaft zeigen sich bei der Visualisierung einer gewichteten Kombination aller Modelle (50% Bag-of-Words-Modell, andere Modelle je 10%; siehe Abb. 4) interpretierbare Cluster von Filmen bestimmter Genres, am deutlichsten im Falle von Horror- und Comedy-Filmen. Bei näherer Betrachtung zeigen sich zudem Cluster von Filmen, die sich zwar im Genre stark unterscheiden, jedoch durch ein gemeinsames Setting oder Thema verbunden sind (wie z.B. Weltraum, Western, Schifffahrt, Sport, …).</p>
            <figure>
               <graphic height=""6.983236111111111cm"" n=""1004"" rend=""inline"" url=""181_final-1fb95b8fbf99fd0874c4a6d3eb517d3b.png"" width=""16.002cm""/>
               <head>Abbildung 4: Gewichtete Kombination aller Feature-Modelle und 2D-Projektion mittels SVD und t-SNE.</head>
            </figure>
            <p>Weiterhin lässt sich zeigen, dass die meisten Features nicht miteinander korrelieren, d.h. Filme die bspw. anhand des Features „Sentiment“ ähnlich sind, können sich erheblich unterscheiden was etwa das Dialogtempo angeht (vgl. Abb. 5).</p>
            <figure>
               <graphic height=""5.265616666666666cm"" n=""1005"" rend=""inline"" url=""181_final-7a836db1b478d9ece474a2609f9388ac.png"" width=""16.002cm""/>
               <head>Abbildung 5: Die 2D-Projektion der Untertitel mittels SVD und t-SNE anhand der Features “Sentiment Analyse” und “Dialogtempo” zeigt sehr unterschiedliche Cluster und lässt darauf schließen, dass diese beiden Merkmale nicht korrelieren. Dies gilt im Übrigen auch für die meisten der anderen Features; die entsprechenden Diagramme finden sich online über plot.ly<ref n=""3"" target=""ftn3""/>.</head>
            </figure>
            <p>Die Unterschiedlichkeit der verschiedenen Features lässt sich auch gut anhand beispielhafter Analysen illustrieren. So zeigt sich etwa, dass bei der Suche nach ähnlichen Filmen zu “The Room” (2003) für jedes einzelne Feature bei den Top 5 der als ähnlich identifizierten Ergebnisse jeweils ganz unterschiedliche Filme herauskommen (vgl. Abb. 6). Einzig “Ruby Sparks” (2012) findet sich sowohl bei “Syntax” als auch bei “Sentiment” wieder. Der Film “The Disaster Artist”, der dokumentationsartig die Entstehungsgeschichte des Klassikers “The Room” schildert (und damit einen unmittelbaren inhaltlichen Bezug hat), kommt interessanterweise nur bei der 
  <hi rend=""italic"">bag of words</hi>-Methode in den Top 5 der Ergebnismenge vor. Es zeigt sich also, dass ein multifaktorieller Vergleich von Filmen anhand unterschiedlicher, dialog-basierter Features, nicht zielführend ist, sondern vielmehr unterschiedliche Merkmale unterschiedliche Ähnlichkeitsaspekte kodieren. Im nächsten Schritt planen wir eine systematische Korrelationsanalyse der unterschiedlichen Features, um gemeinsam auftretende Phänomene und Muster für spezifische Filmgenres etc. identifizieren zu können.
  </p>
            <figure>
               <graphic height=""6.234197222222222cm"" n=""1006"" rend=""inline"" url=""181_final-c7e339f98fab8168db3eeebda61ce974.png"" width=""12.010330555555555cm""/>
               <head>Abbildung 6: Unterschiede in der Ergebnismenge verschiedener Feature-Konfigurationen für <hi rend=""italic"">„The Room“</hi> (2003).
    </head>
            </figure>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Fazit und Ausblick</head>
            <p>Im hier vorgestellten Projekt dokumentieren wir aktuelle Experimente zur Identifikation von Ähnlichkeitsbeziehungen zwischen Film-Untertiteln auf Basis ganz unterschiedlicher Features, die künftig für quantitative Stil- und Genreanalyse von Filmen herangezogen werden können. 
  <hi rend=""italic"">SubRosa</hi> versteht sich zunächst als experimentelle Plattform, die es erlaubt interaktiv unterschiedliche Feature-Kombinationen für unterschiedliche Filme bzw. Fragestellungen zu erproben. Als Verbesserung auf technischer Ebene planen wir die Integration eines größeren Korpus<ref n=""4"" target=""ftn4""/> (Lison & Tiedemann, 2016), welches systematischer validiert und korrigiert wurde als es bei unserem aktuellen Testkorpus der Fall ist. 
  </p>
            <p>Darüber hinaus soll über eine systematische Evaluation eine Feature-Selektion und optimale Gewichtung erfolgen. Geplant ist hierzu eine Evaluation gegen eine 
  <hi rend=""italic"">ground truth</hi> auf Basis bestehender Ähnlichkeitsverbindungen, bspw. über die Empfehlungen via 
  <hi rend=""italic"">collaborative filtering</hi> bei Amazon oder über den frei verfügbaren Datensatz 
  <hi rend=""italic"">MovieLens</hi>.<ref n=""5"" target=""ftn5""/> Offen ist dabei die Frage, ob Ähnlichkeitsbewertungen auf Basis audio-visueller Features grundsätzlich mit Ähnlichkeitsbewertungen auf Dialogebene korrelieren, oder die verschriftlichte Dialogebene ggf. als isolierte Ebene betrachtet werden muss. Wir planen deshalb weitere Fallstudien mithilfe von 
  <hi rend=""italic"">SubRosa</hi>, die zusammen mit Film- und Sprachwissenschaftlern durchgeführt werden sollen.
  </p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note n=""1"" rend=""footnote text"" id=""ftn1"">
      OpenSubtitles: <ptr target=""https://www.opensubtitles.org/de""/>
            </note>
            <note n=""2"" rend=""footnote text"" id=""ftn2"">
      IMDb: <ref target=""https://www.imdb.com/"">https://www.imdb.com/</ref>
            </note>
            <note n=""3"" rend=""footnote text"" id=""ftn3"">
               <hi style=""font-size:10pt"" space=""preserve"">Feature-Visualisierungen: </hi>
               <ref target=""https://chart-studio.plot.ly/~bbrause/#/"">https://chart-studio.plot.ly/~bbrause/#/</ref>
            </note>
            <note n=""4"" rend=""footnote text"" id=""ftn4"">
      OpenSubtitles 2018-Korpus: <ref target=""http://opus.nlpl.eu/OpenSubtitles2018.php"">http://opus.nlpl.eu/OpenSubtitles2018.php</ref>
            </note>
            <note n=""5"" rend=""footnote text"" id=""ftn5"">
      MovieLens Dataset: <ref target=""https://movielens.org/"">https://movielens.org/</ref>
            </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Aggarwal, C. C.</hi> (2001): On k-anonymity and the curse of dimensionality. In: Proc. 31st International Conference on Very Large Data Bases (VLDB), S. 901–909. ACM, 2005.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Argamon, S. / Shimoni,
  A. R. / Koppel, M. </hi> (2003): Automatically categorizing written texts by author gender. In: Literary and Linguistic Computing, Vol. 17, Nr. 4, S. 401– 412.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Bednarek, M. </hi> (to appear
  2020): The Sydney Corpus of Television Dialogue: Designing and
  building a corpus of dialogue from US TV series. Corpora 15/1.
  Pre-Print-Version hier verfügbar:
  <ref target=""https://www.monikabednarek.com/wp-content/uploads/2019/09/Designing-and-building-a-corpus-of-US-TV-dialogue_Academia.pdf"">https://www.monikabednarek.com/wp-content/uploads/2019/09/Designing-and-building-a-corpus-of-US-TV-dialogue_Academia.pdf</ref>
               </bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Blackstock, A. / Spitz,
  M. </hi> (2008): Classifying movie scripts by genre with a MEMM using NLP-based features. M.Sc. Kurs Natural Language Processing, stud. Projektbericht, Juni 2008. Stanford University.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Bougiatiotis, K.  / 
  Giannakopoulos, T.</hi> (2017): Multimodal content representation
  and similarity ranking of movies. Pre-Print-Version hier verfügbar:
  <ptr target=""https://arxiv.org/pdf/1702.04815.pdf""/>
               </bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Burghardt, M. / Kao, M. /
  Wolff, C. </hi> (2016): Beyond Shot Lengths – Using Language Data and Color Information as Additional Parameters for Quantitative Movie Analysis. In Book of Abstracts of the International Digital Humanities Conference (DH).
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Burghardt, M. / Meyer, S. /
  Schmidtbauer, S. / Molz, J. </hi> (2019): “The Bard meets the Doctor” – Computergestützte Identifikation intertextueller Shakespearebezüge in der Science Fiction-Serie Dr. Who. In Book of Abstracts, DHd 2019.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Schmidt, B.</hi> (15.9.2014):
  Screen time! Published on <ref target=""http://sappingattention.blogspot.com/2014/09/screen-time.html"">http://sappingattention.blogspot.com/2014/09/screen-time.html</ref>
                  <hi style=""font-size:10pt"" space=""preserve""> (letzter Zugrifff am 24.9.2019)</hi>
               </bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Hutto, C. J. / Gilbert,
E. </hi> (2014): VADER: A parsimonious rule-based model for sentiment analysis of social media text. In: International Conference on Weblogs and Social Media.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Johnson, W. </hi> (1944): Studies in language behavior: I. A program of research. In: Psychological Monographs, Vol. 56, S. 1-15.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Kozloff, S.</hi> (2000): Overhearing Film Dialogue. University of California Press.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Santini, M. </hi> (2004): A shallow approach to syntactic feature extraction for genre classification. In Proceedings of the 7th Annual Colloquium for the UK Special Interest Group for Computational Linguistics (CLUK 2004).
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Shannon, C. </hi> (1948): A mathematical theory of communication. In: The Bell System Technical Journal, Vol. 27, S. 379–423, 623–656, Juli und October 1948.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Taylor, A. / Tilton, L.</hi> (2019): Distant viewing: analyzing large visual corpora. In Digital Scholarship in the Humanities, 2019. Published by Oxford University Press on behalf of EADH.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Torruella, J. / Capsada, R.</hi> (2013): Lexical statistics and topological structures: A measure of lexical richness. In: 
  <hi rend=""italic"" style=""font-size:10pt"">Procedia - Social and Behavioral Sciences</hi>, Vol. 95, S. 447-454.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Manning, C. / Raghavan, P. /  Schütze, H.</hi> (2008): Introduction to Information Retrieval. Cambridge University Press.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Müller M. / Volk M. </hi> (2013): Statistical Machine Translation of Subtitles: From OpenSubtitles to TED. In: Gurevych I., Biemann C., Zesch T. (eds) Language Processing and Knowledge in the Web. Lecture Notes in Computer Science, vol 8105. Springer, Berlin, Heidelberg.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Nessel, J. / Cimpa, B.</hi> (2011): The MovieOracle-content based movie recommendations. In: Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology, S. 361-364.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Lison, P. / Tiedemann, J. </hi> (2016): OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles. Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC), p. 923-929, European Language Resources Association.
</bibl>
               <bibl>
                  <hi rend=""bold"" style=""font-size:10pt"">Tiedemann , J. </hi> (2016): Finding Alternative Translations in a Large Corpus of Movie Subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC), p. 3518–3522, European Language Resources Association.
</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,information retrieval;nlp;stylometry;subtitles;text similarity,German,inhaltsanalyse;programmierung;stilistische analyse;text;visualisierung;webentwicklung
11025,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,„The Vectorian“ – Eine parametrisierbare Suchmaschine für intertextuelle Referenzen,,Manuel Burghardt;Bernhard Liebl,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Einleitung: Shakespeare, Intertextualität und computergestützte Erkennung von Zitaten</head>
            <p>Shakespeare ist überall. Über alle zeitlichen und medialen Grenzen hinweg finden sich intertextuelle Bezüge auf die Werke von Shakespeare (vgl. Garber, 2005; Maxwell & Rumbold, 2018), der damit nicht nur der meistzitierte und meistgespielte Autor aller Zeiten, sondern auch der meistuntersuchte Autor der Welt ist (Taylor, 2016). Doch wenngleich in zahllosen Studien diverse Einzelaspekte von Shakespeares Werk aus Perspektive der Intertextualitätsforschung gründlich mittels 
                    <hi rend=""italic"">close reading</hi> untersucht wurden, so gibt es bis heute keinen Überblick, kein Gesamtbild, keine systematische Karte intertextueller Shakespeare-Referenzen für größere Textkorpora. Auffällig ist zudem, dass bislang kaum Verfahren der computergestützten Erfassung intertextueller Shakespeare-Referenzen im Sinne des 
                    <hi rend=""italic"">distant reading</hi> zum Einsatz kommen. Dies verwundert umso mehr, als dass sich im Bereich der Informatik und des 
                    <hi rend=""italic"">natural language processing</hi> vielfältige Methoden zur Ermittlung der Ähnlichkeit zwischen Texten finden (Bär et al., 2012; Bär et al. 2015) – und nichts anderes ist Intertextualität letzten Endes. Natürlich ist hier anzumerken, dass die volle Bandbreite intertextueller Phänomene mit bloßen Mitteln der Textähnlichkeitsbestimmung nicht abgedeckt werden kann. Für unser Verständnis von Intertextualität berufen wir uns daher auf die Definition von Genette (1993) – “la présence effective d’un text dans un autre” – wobei wir unter der “effektiven Präsenz” eines Texts in einem anderen tatsächlich eine mehr oder weniger objektiv erkennbare, explizite Referenz an der Textoberfläche verstehen. Die textuelle Umschreibung einer Balkonszene mit einem Mann und einer Frau würden wir demnach nicht automatisch “Romeo and Juliet” zuordnen, was vermutlich auch nicht in allen Fällen korrekt wäre. Die folgende Variante eines bekannten Zitats aus Macbeth (Shakespeares Ursprungsvariante steht jeweils in eckigen Klammern) wäre nach unserem Verständnis hingegen objektiv aus dem Text zu erkennen und eindeutig als intertextuelle Referenz einzuordnen:
</p>
            <p>By the 
<hi rend=""underline"">stinking</hi> [pricking] of my 
<hi rend=""underline"">nose</hi> [thumbs], something 
<hi rend=""underline"">evil</hi> [wicked] this way 
<hi rend=""underline"">goes</hi> [comes]. 
<hi rend=""italic"">(Terry Pratchett: „I Shall Wear Midnight“).</hi>
            </p>
            <p>Eine weitere methodische Einschränkung machen wir, indem wir Phänomene wie strukturelle Ähnlichkeit (Versmaß, Figurenkonstellation) und stilistische Ähnlichkeit<ref n=""1"" target=""ftn1""/>, wie sie bspw. in der 
<hi rend=""italic"">Parodie</hi> oder im 
<hi rend=""italic"">Pastiche</hi> üblich sind, zunächst außer Acht lassen. In Erweiterung einer ersten Pilotstudie zur Identifizierung von Shakespearezitaten in der Fernsehserie „Dr. Who“ (Burghardt et al., 2019) erproben wir in einem aktuellen Experiment das Potenzial von 
<hi rend=""italic"">word embeddings</hi> (Mikolov et al., 2013), um so zusätzlich semantisch ähnliche oder zumindest “funktional äquivalente” (Bubenhofer, 2019) Wörter und Phrasen zu identifizieren. Durch die Auswahl unterschiedlicher 
<hi rend=""italic"">embeddings</hi>-Modelle und weiterer, damit einhergehender Parameter (bspw. der Gewichtung anhand von Wortarten, dem Festlegen von Ähnlichkeitsschwellwerten, etc.) kann es mitunter zu sehr unterschiedlichen Ergebnissen kommen. Um hier systematisch Parameterkombinationen zu untersuchen, die möglichst optimierte Werte bzgl. 
<hi rend=""italic"">precision</hi> und 
<hi rend=""italic"">recall</hi> liefern, wurde im Sinne von Molnars (2019) Desiderat eines „interpretable machine learning“ eine parametrisierbare Suchmaschine zur Identifizierung von Shakespeare-Referenzen als Vorstufe für einen 
<hi rend=""italic"">embeddings</hi>-basierten Ansatz umgesetzt. 
</p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>The Vectorian</head>
            <p>Abb. 1 zeigt die Systemarchitektur der besagten Suchmaschine, die fortan als “The Vectorian”<ref n=""2"" target=""ftn2""/> bezeichnet wird. Im 
<hi rend=""italic"">Vectorian</hi> fungieren kurze Shakespeare-Passagen (bspw. „If you prick us, do we not bleed?“) als Queries; Texte, die diese Textteile (wortwörtlich oder als Variante) aufgreifen, stellen im Sinne des Information Retrieval dann die entsprechenden Ergebnisdokumente dar (für einen vergleichbaren Ansatz siehe Manjavacas et al., 2019).
</p>
            <figure>
               <graphic height=""10.86026388888889cm"" n=""1001"" rend=""inline"" url=""187_final-a9f800b143e4c0d74da4f4f9abb05929.png"" width=""16.002cm""/>
               <head>Abbildung 1: Systemarchitektur der Zitat-Suchmaschine <hi rend=""italic"">“The Vectorian""</hi>.  </head>
            </figure>
            <p>Kern des 
<hi rend=""italic"">Vectorian</hi> ist die Suche von optimalen semi-globalen 
<hi rend=""italic"">alignments</hi> zwischen Satzpaaren (wobei wir einen Satz als Sequenz von Worten verstehen) über eine Variante des Needleman-Wunsch-Algorithmus (Sellers, 1974) mit sog. 
<hi rend=""italic"">free shift alignment</hi>. Als Bewertungsfunktion nutzen wir eine über 
<hi rend=""italic"">word embeddings</hi> errechnete Distanz zwischen Worten. Diesen Ansatz kombinieren wir mit einer Reihe experimenteller Parameter (siehe die fünf Punkte im nachfolgenden Abschnitt). 
</p>
            <p>Abb. 2 zeigt das Frontend des 
<hi rend=""italic"">Vectorian</hi>. Zu sehen ist ein Eingabefeld für beliebige Suchanfragen, d.h. die Textstellen, die man als intertextuelle Referenzen in anderen Texten finden will. Die Parameter der Suche, die nachfolgend noch näher erläutert werden, können über entsprechende Auswahlmenüs konfiguriert werden. Schließlich gibt der 
<hi rend=""italic"">Vectorian</hi> eine Ergebnisliste zurück, deren Ranking dem jeweils höchsten Ähnlichkeitswert zwischen der Suchanfrage und einer entsprechenden Textstelle entspricht. Wortwörtliche Zitate haben demnach einen höheren Wert als stark abgeänderte Referenzen mit diversen Auslassungen und Substitutionen.
</p>
            <figure>
               <graphic height=""13.774208333333334cm"" n=""1002"" rend=""inline"" url=""187_final-c4cf92bbdf69758cd3458c2c5b486c42.png"" width=""16.002cm""/>
               <head> Abbildung 2: Frontend des <hi rend=""italic"">Vectorian</hi> mit allen möglichen Suchparametern und einer beispielhaften Ergebnisliste für die Suchanfrage “If you prick us, do we not bleed?”.
  </head>
            </figure>
            <p>Der 
<hi rend=""italic"">Vectorian</hi> durchsucht aktuell ein Korpus von 230 englischen Einzeltexten, darunter 50 Werke von Shakespeare (Dramen und Sonette) sowie diverse Romane aus unterschiedlichen Epochen und Transkripte von Filmen und Fernsehserien. Das Korpus enthält rund 19,5 Millionen Tokens mit POS-Annotationen (POS = 
<hi rend=""italic"">parts of speech</hi>), die sich auf rund 2,2 Millionen Sätze verteilen. Der 
<hi rend=""italic"">Vectorian</hi> bietet mit 
<hi rend=""italic"">fastText</hi> (Mikolov, 2017) und 
<hi rend=""italic"">wnet2vec</hi> (Saedi, 2018) momentan zwei 
<hi rend=""italic"">embedding</hi>-Varianten zur Auswahl. Wir nutzen für 
<hi rend=""italic"">fastText</hi> bestehende, vortrainierte Modelle
(<ref target=""https://fasttext.cc/"">https://fasttext.cc/</ref>), für 
<hi rend=""italic"">wnet2vec</hi> wurde ein eigenes 
<hi rend=""italic"">embedding</hi> auf Basis unseres Korpus mit Hilfe
einer leicht angepassten Referenzimplementierung von Saedi et al.
(<ref target=""https://github.com/nlx-group/WordNetEmbeddings"">https://github.com/nlx-group/WordNetEmbeddings</ref>) erstellt. Im 
<hi rend=""italic"">Vectorian</hi> kann entweder eines der beiden 
<hi rend=""italic"">embeddings</hi> ausgewählt werden oder eine gewichtete Kombination aus beiden, bspw. 25% 
<hi rend=""italic"">fastText</hi> und 75% 
<hi rend=""italic"">wnet2vec</hi>. Bei der Suche wird auf dem Suchtext zunächst ein POS-Tagging durchgeführt. So können syntaktische Strukturen, die über die reine Wortreihenfolge hinausgehen, in die Suche einfließen.
</p>
            <p>Neben den beiden 
<hi rend=""italic"">embedding</hi>-Modellen wurden zusätzlich weitere parametrisierbare Optionen umgesetzt, etwa die Berücksichtigung bzw. unterschiedliche Gewichtung von Wortarten, Einschüben sowie generell einer graduellen Anpassung des Ähnlichkeitswerts. Diese Parameter werden nachfolgend kurz erläutert.
</p>
            <list type=""ordered"">
               <item>“<hi rend=""bold"">Ignore Determiners</hi>” entfernt alle Worte, die vom POS-Tagging als DT (“the”, “this”, etc.) erkannt wurden, aus der Suchanfrage.
  </item>
               <item>“<hi rend=""bold"">Ensure POS Match</hi>” ermöglicht das Ignorieren von Worten in den Korpusdokumenten, deren POS-Tags nicht dem der alignierten Worte im Suchtext entsprechen. Die Auswirkung der Einstellung kann graduell abgeschwächt werden.
  </item>
               <item>“<hi rend=""bold"">POST STSS Weighting</hi>”: Nicht alle Wortarten besitzen gleiches semantisches Gewicht für die Bedeutung eines Satzes. Mittels “POST STSS Weighting” gewichten wir daher Wortähnlichkeiten bei der Suche mit einer an POST STSS („part-of-speech tag-supported short-text semantic similarity“, Batanović, 2015) angelehnten Gewichtungsmatrix<ref n=""3"" target=""ftn3""/>. Die Auswirkung dieser Einstellung kann ebenfalls graduell abgeschwächt werden.
  </item>
               <item>“<hi rend=""bold"">Mismatch Length Penalty</hi>” konfiguriert, ab welcher Länge eines einzelnen 
  <hi rend=""italic"">mismatch</hi> im Ergebnis eine Abschwächung der Bewertung um 50% geschehen soll<ref n=""4"" target=""ftn4""/>. Eine Streuung von Matches ohne lokale Nähe führt in einem Ergebnis somit zur mehr oder weniger starken Abwertung. Die gesamte Abwertung für ein Ergebnis errechnet sich als Summe der Abwertungen für alle 
  <hi rend=""italic"">mismatches</hi>. 
  </item>
               <item>“<hi rend=""bold"">Similarity Threshold</hi>” regelt den Schwellwert zur Ähnlichkeitsbewertung zwischen Wörtern. Ein niedriger Schwellwert erlaubt bspw. größere Abweichungen und kann dadurch auch zu einem größeren Rauschen durch mehr 
  <hi rend=""italic"">false positives</hi> führen.
  </item>
            </list>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Beispielabfragen</head>
            <p>Der 
<hi rend=""italic"">Vectorian</hi> wurde als parametrisierbare und interpretierbare Suchmaschine konzipiert, um einen explorativen Zugang zur Analyse unterschiedlicher Parameterkonfigurationen auf potenzielle Suchergebnisse, also in unserem Falle Shakespeare-Referenzen, zu ermöglichen. Nachfolgend illustrieren wir einige Auswirkungen unterschiedlicher Parametereinstellungen am Beispiel der kurzen Shakespeare-Phrase “under the greenwood tree” (aus Shakespeares „As you like it“).
</p>
            <p>Die am besten bewerteten Ergebnisse sind zunächst viele Varianten nach dem Schema „under the X tree”, bspw. „under the 
<hi rend=""underline"">chestnut</hi> tree”. Mit dem Parameter 
<hi rend=""italic"">mismatch length penalty</hi> kann man zusätzlich
steuern, wie viele Einfügungen in den Treffern erlaubt sind. Werden
Einfügungen nur in geringem Umfang erlaubt, dann erhält man vor allem
Sätze bei denen die Präposition variiert wird, bspw.
„<hi rend=""bold"">beneath</hi> the <hi rend=""underline"">beech</hi>
tree”.
Erlaubt man hingegen mehr Einfügungen, kommt es entsprechend auch zu Ergebnissen wie “under the 
<hi rend=""bold"">dear old</hi>
               <hi rend=""underline"">plane</hi> tree”.
</p>
            <p>Beim Parameter der 
<hi rend=""italic"">embeddings-</hi>Wahl sieht man sehr gut, wie 
<hi rend=""italic"">FastText</hi> und 
<hi rend=""italic"">WordNet</hi> ganz unterschiedliche Präferenzen bei
der Auswahl von alternativen „trees“ liefern
(<hi rend=""italic"">FastText</hi>: „<hi rend=""italic"">chestnut“</hi> > „<hi rend=""italic"">beech“</hi> vs. 
<hi rend=""italic"">WordNet</hi>: „<hi rend=""italic"">beech“</hi> > „<hi rend=""italic"">oak“</hi>). Das 
<hi rend=""italic"">mixed embedding</hi> (also eine Aktivierung beider 
<hi rend=""italic"">embeddings</hi> zu gleichen Teilen) scheint Vorteile beider 
<hi rend=""italic"">embeddings</hi> optimal zu kombinieren, indem z.B. „oak tree“ höher gewertet wird als „bodhi tree“, wobei es sich bei Letzterem um einen spezifischen Baum aus einem religiösen Kontext handelt.
                </p>
            <p>POST-STSS, ein Parameter der unterschiedliche POS unterschiedlich stark gewichtet, ist in Kombination mit dem 
                    <hi rend=""italic"">WordNet embedding</hi> am
		    aufschlussreichsten: Mit POST STSS werden im
		    Zweifel reine Baumphrasen bevorzugt (""the fir
		    tree"", ""the yew tree"").
		    Ohne POST-STSS werden auch Substantive hoch bewertet, die mit Bäumen zwar nichts zu tun haben, dafür aber eine hohe semantische Nähe zu anderen Wörtern aufweisen, z.B. „greenwood“ und „garden“.
                </p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Fazit und Ausblick</head>
            <p style=""text-align:left; "">Im aktuellen Stadium dient der 
                    <hi rend=""italic"">Vectorian</hi> wie eingangs geschildert zunächst als Experimentierplattform, mit deren Hilfe man explorativ die Auswirkungen unterschiedlicher Einstellungsparameter erproben kann. Im nächsten Schritt soll eine systematische Evaluierung der Suchmaschine erfolgen, indem gegen eine vorab definierte 
                    <hi rend=""italic"">ground truth</hi> an Shakespeare-Zitaten in einem Teilkorpus aus Fantasy-Romanen gesucht wird. Dabei werden alle möglichen Parameterkonfigurationen (insgesamt 72 Kombinationsmöglichkeiten) nacheinander durchgerechnet und die jeweiligen Bewertungen der einzelnen Sätze dokumentiert. Weiterhin soll berücksichtigt werden, wie viele 
                    <hi rend=""italic"">false positives</hi> sich unter die 
                    <hi rend=""italic"">true positives</hi> aus der 
                    <hi rend=""italic"">ground truth</hi> mischen. Ziel ist es, diejenige Konfiguration zu identifizieren, die für möglichst viele Sätze der 
                    <hi rend=""italic"">ground truth</hi> einen hohen 
                    <hi rend=""italic"">alignment score</hi> aufweist und dabei die Zahl der 
                    <hi rend=""italic"">false positives</hi> minimiert. Im nächsten Schritt sollen dann mit der bestbewerteten Konfiguration systematisch mehrere hundert Shakespeare-Zitate, die aus bestehenden Zitate-Datenbanken wie 
                    <hi rend=""italic"">WikiQuote</hi> (https://en.wikiquote.org/) extrahiert werden, in einem großen Korpus von Fantasy-Literatur und Transkripten von Filmen und TV-Serien gesucht werden
                    <ref n=""5"" target=""ftn5""/>. 
                </p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note n=""1"" rend=""footnote text"" id=""ftn1"">
      Für eine Systematisierung von text reuse Methoden anhand der Kategorien inhaltliche, strukturelle und stilistische Ähnlichkeit vgl. Bär et al. 2012.
    </note>
            <note n=""2"" rend=""footnote text"" id=""ftn2"">
     “The Vectorian” ist als Prototyp auf Anfrage verfügbar.
    </note>
            <note n=""3"" rend=""footnote text"" id=""ftn3"">
      Beispiel: Eine Ähnlichkeit auf einem Adjektiv (Tag JJ) wird mit dem Faktor 0.7 gewichtet, während ein Verb (Tag VB) mit 1.2 gewichtet wird.
    </note>
            <note n=""4"" rend=""footnote text"" id=""ftn4"">
      Die Abwertung über andere Längen erfolgt ausgehend vom gegebenen Basiswert exponentiell in der Länge des  <hi rend=""italic"">mismatches</hi>, was uns intuitiv und aufgrund der Beobachtungen in (Beeferman, 1997) sinnvoll erscheint. Der genaue Kurvenverlauf für gängige Längen wird im UI als Plot dargestellt.
    </note>
            <note n=""5"" rend=""footnote text"" id=""ftn5"">
      Die Dokumentbasis des 
      <hi rend=""italic"">Vectorian</hi> kann flexibel erweitert werden solange die Texte in einem grundlegend bereinigten 
      <hi rend=""italic"">plain text</hi>-Format vorliegen.
    </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Bär, D. / Zesch, T. / Gurevych, I. </hi> (2012): Text Reuse Detection using a Composition of Text Similarity Measures. Proceedings of COLING 2012, 167-184. </bibl>
               <bibl>
                  <hi rend=""bold"">Bär, D. / Zesch, T. / Gurevych, I. </hi>  (2015): Composing Measures for Computing Text Similarity. Technical Report TUD-CS-2015-0017, TU Darmstadt.</bibl>
               <bibl>
                  <hi rend=""bold"">Batanović, V. / Bojić, D. </hi>  (2015): “Using Part-of-Speech Tags as Deep Syntax Indicators in Determining Short Text Semantic Similarity"". In Computer Science and Information Systems, 12(1), S. 1–31.</bibl>
               <bibl>
                  <hi rend=""bold"">Beeferman, D. / Berger, A. / Lafferty, J. </hi>  (1997): A model of lexical attraction and repulsion. In Proceedings of the 8th Conference on European Chapter of the Association for Computational Linguistics, S. 373-380.</bibl>
               <bibl>
                  <hi rend=""bold"">Bubenhofer, N. </hi>  (2019): Word Embeddings: Funktionale Äquivalenz statt Synonymie. Publiziert auf Sprechtakel-Blog (2.3.2019), online verfügbar unter https://www.bubenhofer.com/sprechtakel/2019/03/02/.word-embeddings-funktionale-aequivalenz-statt-synonymie/</bibl>
               <bibl>
                  <hi rend=""bold"">Burghardt, M. / Meyer, S. / Schmidtbauer,  S.  / Molz, J. </hi>  (to appear in 2019): “The Bard meets the Doctor” – Computergestützte Identifikation intertextueller Shakespearebezüge in der Science Fiction-Serie Dr. Who. In Book of Abstracts, DHd 2019.</bibl>
               <bibl>
                  <hi rend=""bold"">Garber, M. </hi>  (2005): Shakespeare after All. New York: Anchor Books.</bibl>
               <bibl>
                  <hi rend=""bold"">Genette, G. </hi>  (1993): Palimpseste. Die Literatur auf zweiter Stufe. Frankfurt am Main: Suhrkamp. Translation of the revised second edition. [Genette, G. (1982). Palimpsestes. La littérature au second degré. Paris: Éditions de Seuil. Revised 2nd edition 1983.]</bibl>
               <bibl>
                  <hi rend=""bold"">Kusner, M. / Sun, Y. /  Kolkin, N. /  Weinberger, K. </hi>  (2015): “From Word Embeddings To Document Distances”. In Proceedings of the 32nd International Conference on Machine Learning. Lille, Frankreich.
      </bibl>
               <bibl>
                  <hi rend=""bold"">Manjavacas, E. /  Long, B. / Kestemont,
	M. </hi>  (2019): “On the Feasibility of Automated Detection
	of Allusive Text Reuse“. ArXiv: 1905.02973 [Cs], 8. Mai
	2019. <ptr target=""http://arxiv.org/abs/1905.02973""/>.
      </bibl>
               <bibl>
                  <hi rend=""bold"">Maxwell, J. / Rumbold, K. (eds.) </hi>  (2018): Shakespeare and Quotation. Cambridge: Cambridge University Press.</bibl>
               <bibl>
                  <hi rend=""bold"">Mikolov, T. / Chen, K. / Corrado, G. / Dean, J. </hi>  (2013): Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</bibl>
               <bibl>
                  <hi rend=""bold"">Mikolov, Tomas, et al. </hi> “Advances in Pre-Training Distributed Word Representations.” ArXiv:1712.09405 [Cs], Dec. 2017. arXiv.org, http://arxiv.org/abs/1712.09405.</bibl>
               <bibl>
                  <hi rend=""bold"">Molnar, C. </hi>  (2019). Interpretable Machine Learning. A Guide for Making Black Box Models Explainable. eBook online verfügbar unter 
	<ref target=""https://christophm.github.io/interpretable-ml-book/"">https://christophm.github.io/interpretable-ml-book/</ref>
               </bibl>
               <bibl>
                  <hi rend=""bold"">Saedi, C.  /  Branco, A. / Rodrigues, J. A. /  Silva, J. </hi>  (2018, July). Wordnet embeddings. In
      Proceedings of The Third Workshop on Representation Learning for
      NLP (pp. 122-131).
      </bibl>
               <bibl style=""text-align:left;"">
                  <hi rend=""bold""> Sellers, Peter H.</hi> (1974):  „On the Theory and Computation of Evolutionary Distances“. 
	<hi rend=""italic"">SIAM Journal on Applied Mathematics</hi> 26, Nr. 4 (Juni 1974): 787–93. 
	<ref target=""https://doi.org/10.1137/0126070"">https://doi.org/10.1137/0126070</ref>.
      </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,information retrieval;intertextuality;nlp;text reuse;word embeddings,German,annotieren;bewertung;entdeckung;inhaltsanalyse;programmierung;text
11719,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,"""Archiviz: ""A Tool for the Interactive, Visual Exploration of Digital Archives",,Brad Rittenhouse;Todd Michney;Ines Acosta,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p>
                In traditional, paper-based archives, finding aids afford basic means for discovery, but tend to return results shaped by researchers’ preexisting knowledge and queries. With the digitization of archives, new analysis and visualization methods allow researchers to textually map large corpora and not only pinpoint specific materials, but also open new ways of navigation.
                <note place=""end"" xml:id=""end1"" n=""1"">
                    <p rend=""endnote text"">
                        ENDNOTES
                    </p>
                    <p>
                        <hi style=""font-size:12pt"" xml:space=""preserve""> See Graham, S., Milligan, I., and Weingart, S. (2015), </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Exploring Big Historical Data: The Historian's Macroscope</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, Imperial College Press, London; Morrissey, R. (2015), “Archives of connection: ‘Whole network’ analysis and social history,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Historical Methods</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 48, no. 2, pp. 67–79; Duff, W., and Haskell, J. (2015), “New uses for old records: A rhizomatic approach to archival access,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">American Archivist</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 78, no. 1, pp. 38-58; Putnam, L. (2016), “The transnational and the text-searchable: Digitized sources and the shadows they cast,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">American Historical Review</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 121, no. 2, pp. 377–402; Edelstein, D., Findlen, P., Ceserani, G., Winterer, C., and Coleman, N. (2017), “Historical research in a digital age: Reflections from the Mapping the Republic of Letters project,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">American Historical Review</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 122, no. 2, pp. 400–424; and Hoekstra, R. and Koolen, M. (2018), “Data scopes for digital history research,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Historical Methods: A Journal of Quantitative and Interdisciplinary History</hi>
                        , vol. 52, no. 2, pp. 79–94.
                    </p>
                </note>
                However, effective use of computational tools including natural language processing (NLP), named entity recognition (NER), and knowledge graphing often requires considerable technical sophistication, forming an access barrier for many researchers.
                <note place=""end"" xml:id=""end2"" n=""2"">
                    <p>
                        <hi style=""font-size:12pt"" xml:space=""preserve""> Piotrowski, M. (2012), </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Natural Language Processing for Historical Texts</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, Synthesis Lectures on Human Language Technologies, vol. 17, Morgan & Claypool Publishers, San Rafael; Marrero, M., Urbano, J., Sánchez-Cuadrado, S., Morato, J., and Gómez-Berbís, J.M. (2013), “Named Entity Recognition: fallacies, challenges and opportunities,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Computer Standards and Interfaces</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 35, no. 5, pp. 482–489; Shahin, S. (2016), “When scale meets depth: Integrating Natural Language Processing and textual analysis for studying digital corpora,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Communication Methods and Measures</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 10, no. 1, pp. 28–50; Srinivasa-Desikan, B. (2018), </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Natural Language Processing and Computational Linguistics: A Practical Guide to Text Analysis with Python, Gensim, spaCy, and Keras</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, Packt Publishing, Birmingham and Mumbai; Ehrmann, M., Romanello, M., Flückiger, A. and Clematide, S. (2020), “Named Entity Recognition and linking on historical newspapers,” in Arampatzis, A. et al. (eds.), </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Experimental IR Meets Multilinguality, Multimodality, and Interaction</hi>
                        , Springer International, Cham, pp. 288–310.
                    </p>
                </note>
            </p>
            <p>
                <hi style=""font-size:12pt"" xml:space=""preserve"">To help bridge this technical gap, we have been developing a toolset, currently funded by an NEH-ODH Digital Humanities Advancement Grant, that integrates large-scale text processing and data visualization capabilities into the open-source </hi>
                <ref target=""https://omeka.org/"">
                    <hi rend=""color(4F81BD)"" style=""font-size:12pt"">Omeka</hi>
                </ref>
                <hi rend=""endnote_reference"">
                    <note place=""end"" xml:id=""end3"" n=""3"">
                        <p rend=""endnote text"">
                            We developed
                            <hi rend=""italic"" style=""font-size:12pt"">Archiviz</hi>
                            for the Omeka Classic version; technical and user support for our plugin will be available in February 2023 in the
                            <ref target=""https://omeka.org/classic/plugins/"">
                                Omeka plugin library
                            </ref>
                            . Additional availability is planned through github and a Docker container for ease of use.
                        </p>
                    </note>
                </hi>
                content management platform. With the tool, users can upload batches of documents, perform NER on them, and manipulate an interactive graph that displays extracted entities (people, places, organizations, etc.) from the collection and how they interconnect in its component documents. The toolset’s components
                <hi rend=""endnote_reference"">
                    <note place=""end"" xml:id=""end4"" n=""4"">
                        <p rend=""endnote text"">
                            The primary tools and plug-ins used to construct Archiviz include Harvard’s
                            <ref target=""https://omeka.org/classic/plugins/Elasticsearch/"">
                                Elasticsearch
                            </ref>
                            <hi style=""font-size:12pt"" xml:space=""preserve"">Omeka plugin for enhanced search capabilities, Google </hi>
                            <ref target=""https://github.com/tesseract-ocr/tesseract"">
                                Tesseract
                            </ref>
                            <hi style=""font-size:12pt"" xml:space=""preserve"">for OCR, </hi>
                            <ref target=""https://spacy.io/"">
                                spaCy
                            </ref>
                            ’s NLP functionality, particularly its NER functionality, with current development focusing on the integration of the Science Museum Group’s
                            <ref target=""https://github.com/TheScienceMuseum/heritage-connector/"">
                                Heritage Connector
                            </ref>
                            for better entity identification, linking, and disambiguation. The graphing of this information is primarily performed with the force-directed graphing of
                            <ref target=""https://d3js.org/"">
                                d3.js
                            </ref>
                            .
                        </p>
                    </note>
                </hi>
                were all specifically developed for (and tested by) non-technical researchers and community advocates, with all computational work taking place in a simple graphical user interface.
            </p>
            <p>
                Our 
                <hi rend=""italic"" style=""font-size:12pt"">Archiviz</hi>
                toolset produces social network-style visualizations that connect results on the basis of the important people, places, organizations, and other entities mentioned. By applying NER to a collection, users can see the full breadth of their research subject at a glance, and explore connections they may not have known exist.
                <hi rend=""endnote_reference"">
                    <note place=""end"" xml:id=""end5"" n=""5"">
                        <p rend=""endnote text"">
                            For a similar application, see Tumbe, C. (2019), ""Corpus linguistics, newspaper archives and historical research methods,"" 
                            <hi rend=""italic"" style=""font-size:12pt"">Journal of Management History</hi>
                            , vol. 25, no. 4, pp. 533–549.
                        </p>
                    </note>
                </hi>
                What our implementation offers that is new, is the efficient display of interrelationships between large numbers of nodes (named entities) combined with the possibility of rapid navigation to search results (documents). In addition, the interface moves beyond traditional query-based archival searching, “flattening” out a collection to show results beyond a researcher’s interests or knowledge.
            </p>
            <figure>
                <graphic n=""1001"" width=""15.91333888888889cm"" height=""8.228180555555555cm"" url=""Pictures/216d68a4a3e47da171317413488654ff.png"" rend=""inline""/>
            </figure>
            <p>
                We are continuing to refine
                <hi rend=""italic"" style=""font-size:12pt"">Archiviz</hi>
                 in ways intended to be intuitive, usable, and readily adoptable by users from a variety of different backgrounds. A primary design goal of the project has been to make it accessible not just to relatively tech-savvy academics, but more importantly, communities beyond the academy. It is our hope that the tool may particularly benefit disadvantaged communities—which in the United States have often faced pressure from gentrification and urban redevelopment, with consequent coercive displacement from historical neighborhoods. Residents in such areas often lack the infrastructure to collect, preserve, and interpret local history. As such, we have partnered with various community groups and activists throughout the process to ensure that the tool can be useful to them. Most ambitiously, in 2019 we hosted a “Community Researcher Workshop” for Atlanta-based librarians, archivists, community organizers, nonprofit staffers, and students to explore the
                <ref target=""https://ivanallen.iac.gatech.edu/mayoral-records/visual/elasticsearch"">
                    <hi rend=""color(4F81BD)"" style=""font-size:12pt"">Mayor Ivan Allen Digital Archive</hi>
                </ref>
                that has served as our first test corpus, using a prototype of our toolset.
                <hi rend=""endnote_reference"">
                    <note place=""end"" xml:id=""end6"" n=""6"">
                        <p>
                            <hi style=""font-size:12pt"" xml:space=""preserve""> On the issues here, see Flinn, A., Stevens, M., and Shepherd, E. (2009), “Whose memories, whose archives? Independent community archives, autonomy and the mainstream,” </hi>
                            <hi rend=""italic"" style=""font-size:12pt"">Archival Science</hi>
                            , vol. 9, no. 71, pp. 71–86.
                        </p>
                    </note>
                </hi>
            </p>
            <p>
                User testing of the interface during this workshop produced very positive feedback, with participants calling the platform “incredibly useful,” with the “potential to break down traditional barriers of why people are hesitant to use archives.” A GLAM (Gallery, Library, Archive, Museum) researcher noted that it “is something almost any archive or library could utilize to their advantage,” while a community advocate stated that she “wanted the information for myself, but also…to share with my fellow residents.” With an additional two years of grant-funded development since this workshop, we are excited to share our work with the DH community. While we have integrated much of the feedback from the workshop, future plans include capabilities to process a wider spectrum of digital, textualized media—documents, video and audio converted to text, and even images identified and categorized with computer vision making our tool relevant for a more diverse array of communities and collections.
                <hi rend=""endnote_reference"">
                    <note place=""end"" xml:id=""end7"" n=""7"">
                        <p rend=""endnote text"">
                            <hi style=""font-size:12pt"" xml:space=""preserve"">Wiriyathammabhum, P., Summers-Stay, D., Fermüller, C., and Aloimonos, Y. (2017), “Computer vision and Natural Language Processing: Recent approaches in multimedia and robotics,” </hi>
                            <hi rend=""italic"" style=""font-size:12pt"">ACM Computing Surveys</hi>
                            , vol. 49, no. 4, pp. 1–44.
                        </p>
                    </note>
                </hi>
            </p>
        </body>
        <back>
            <!-- <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi style=""font-size:12pt"" xml:space=""preserve""> See Graham, S., Milligan, I., and Weingart, S. (2015), </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Exploring Big Historical Data: The Historian's Macroscope</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, Imperial College Press, London; Morrissey, R. (2015), “Archives of connection: ‘Whole network’ analysis and social history,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Historical Methods</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 48, no. 2, pp. 67–79; Duff, W., and Haskell, J. (2015), “New uses for old records: A rhizomatic approach to archival access,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">American Archivist</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 78, no. 1, pp. 38-58; Putnam, L. (2016), “The transnational and the text-searchable: Digitized sources and the shadows they cast,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">American Historical Review</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 121, no. 2, pp. 377–402; Edelstein, D., Findlen, P., Ceserani, G., Winterer, C., and Coleman, N. (2017), “Historical research in a digital age: Reflections from the Mapping the Republic of Letters project,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">American Historical Review</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 122, no. 2, pp. 400–424; and Hoekstra, R. and Koolen, M. (2018), “Data scopes for digital history research,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Historical Methods: A Journal of Quantitative and Interdisciplinary History</hi>
                        , vol. 52, no. 2, pp. 79–94.
                    </bibl>
                    <bibl>
                        <hi style=""font-size:12pt"" xml:space=""preserve""> Piotrowski, M. (2012), </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Natural Language Processing for Historical Texts</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, Synthesis Lectures on Human Language Technologies, vol. 17, Morgan & Claypool Publishers, San Rafael; Marrero, M., Urbano, J., Sánchez-Cuadrado, S., Morato, J., and Gómez-Berbís, J.M. (2013), “Named Entity Recognition: fallacies, challenges and opportunities,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Computer Standards and Interfaces</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 35, no. 5, pp. 482–489; Shahin, S. (2016), “When scale meets depth: Integrating Natural Language Processing and textual analysis for studying digital corpora,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Communication Methods and Measures</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, vol. 10, no. 1, pp. 28–50; Srinivasa-Desikan, B. (2018), </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Natural Language Processing and Computational Linguistics: A Practical Guide to Text Analysis with Python, Gensim, spaCy, and Keras</hi>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">, Packt Publishing, Birmingham and Mumbai; Ehrmann, M., Romanello, M., Flückiger, A. and Clematide, S. (2020), “Named Entity Recognition and linking on historical newspapers,” in Arampatzis, A. et al. (eds.), </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Experimental IR Meets Multilinguality, Multimodality, and Interaction</hi>
                        , Springer International, Cham, pp. 288–310.
                    </bibl>
                    <bibl rend=""endnote text"">
                        We developed
                        <hi rend=""italic"" style=""font-size:12pt"">Archiviz</hi>
                        for the Omeka Classic version; technical and user support for our plugin will be available in February 2023 in the
                        <ref target=""https://omeka.org/classic/plugins/"">
                            Omeka plugin library
                        </ref>
                        . Additional availability is planned through github and a Docker container for ease of use.
                    </bibl>
                    <bibl rend=""endnote text"">
                        The primary tools and plug-ins used to construct Archiviz include Harvard’s
                        <ref target=""https://omeka.org/classic/plugins/Elasticsearch/"">
                            Elasticsearch
                        </ref>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">Omeka plugin for enhanced search capabilities, Google </hi>
                        <ref target=""https://github.com/tesseract-ocr/tesseract"">
                            Tesseract
                        </ref>
                        <hi style=""font-size:12pt"" xml:space=""preserve"">for OCR, </hi>
                        <ref target=""https://spacy.io/"">
                            spaCy
                        </ref>
                        ’s NLP functionality, particularly its NER functionality, with current development focusing on the integration of the Science Museum Group’s
                        <ref target=""https://github.com/TheScienceMuseum/heritage-connector/"">
                            Heritage Connector
                        </ref>
                        for better entity identification, linking, and disambiguation. The graphing of this information is primarily performed with the force-directed graphing of
                        <ref target=""https://d3js.org/"">
                            d3.js
                        </ref>
                        .
                    </bibl>
                    <bibl rend=""endnote text"">
                        For a similar application, see Tumbe, C. (2019), ""Corpus linguistics, newspaper archives and historical research methods,"" 
                        <hi rend=""italic"" style=""font-size:12pt"">Journal of Management History</hi>
                        , vol. 25, no. 4, pp. 533–549.
                    </bibl>
                    <bibl>
                        <hi style=""font-size:12pt"" xml:space=""preserve""> On the issues here, see Flinn, A., Stevens, M., and Shepherd, E. (2009), “Whose memories, whose archives? Independent community archives, autonomy and the mainstream,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">Archival Science</hi>
                        , vol. 9, no. 71, pp. 71–86.
                    </bibl>
                    <bibl rend=""endnote text"">
                        <hi style=""font-size:12pt"" xml:space=""preserve"">Wiriyathammabhum, P., Summers-Stay, D., Fermüller, C., and Aloimonos, Y. (2017), “Computer vision and Natural Language Processing: Recent approaches in multimedia and robotics,” </hi>
                        <hi rend=""italic"" style=""font-size:12pt"">ACM Computing Surveys</hi>
                        , vol. 49, no. 4, pp. 1–44.
                    </bibl>
                </listBibl>
            </div> -->
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,digital archives;digital tools;knowledge graphing;ner;nlp,English,"20th century;contemporary;design studies;digital archiving;english;history;interface design, development, and analysis;north america"
11743,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,Poetry as Error. A ‘Tool Misuse’ Experiment on the Processing of German Language Poetry,,Henny Sluyter-Gäthje;Peer Trilcke,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">
                <hi rend=""bold"">1. Research question</hi>
            </p>
            <p style=""text-align: left; "">In Computational Literary Studies texts are typically pre-processed with Natural Language Processing (NLP) tools. However, due to historical and/or aesthetic characteristics, literary texts sometimes deviate notably from the data the tools are trained on. Due to this difference in domain, the performance of the tools drops (Scheible et al., 2011; Rayson et al., 2007; Herrmann, 2018; Bamman, 2020). Instead of considering this to be a problem, the ‘erroneousness’ of the tools could provide a computational understanding of the ‘deviance of literary texts’; produced errors might reveal something about the characteristics of literature.</p>
            <p style=""text-align: left; "">In the following, we report on a 
                <hi rend=""italic"">Tool Misuse</hi> experiment on German lyric poetry – a genre that is usually associated with a high degree of deviance (Müller-Zettelmann, 2000: 100; Zymner, 2019: 29–30) – in which we develop a pipeline that provokes tokenization, lemmatization and POS tagging 'errors' of NLP tools and typologises these 'errors' in a rule-based way. 
            </p>
            <p style=""text-align: left; "">
            <hi rend=""bold"">2. Operationalization</hi>
         </p>
            <figure>
                <graphic n=""1001"" width=""15.920861111111112cm"" height=""8.528402777777778cm"" url=""Pictures/a2db260a3aba82fe4a88712f797527c3.png"" rend=""inline""/>
                <head>Pipeline for error typing of the corpora.</head>
            </figure>
            <p style=""text-align: left; "">Since gold standard annotations are not available for our scenario, we base our evaluation on the assumption that correctly produced lemmas can be found in dictionaries of German language. Based on the 
                <ref target=""https://textgridrep.org/"">TextGridRepository</ref>, we build a canon-based corpus of 'prototypical' German-language poetry comprising 5,144 poems. For comparison, we use a prose corpus of 100 German-language novels from the 19th century, compiled from the TextGridRepository and 
                <ref target=""https://www.projekt-gutenberg.org/"">Project Gutenberg</ref>. As dictionaries we use ‘GermaNet‘ (Hamp & Feldweg, 1997; Henrich & Hinrichs, 2010) and the ‘Digitales Wörterbuch der deutschen Sprache‘ (Klein & Geyken, 2010). To ensure that the resulting errors are not tagger specific, we use several NLP tools for tokenization, lemmatization and POS tagging of the corpora (fig. 01) and consider all content word types as potential errors that are lemmatized by at least two tools and for which none of the produced lemmas are found in the dictionaries (fig. 02, column ""pFail"").
            </p>
            <p style=""text-align: left; "">Our 'error pipeline' prefers recall over precision, thus it produces only circumstantial evidence of potential errors. A larger number of false positives is to be expected, because we process out-of-vocabulary words of the dictionaries.</p>
            <figure>
                <table rend=""rules"">
                    <row>
                        <cell style=""text-align: left;"" rend=""DH-Default""/>
                        <cell style=""text-align: left;"" rend=""DH-Default"">Poetry</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">Poetry</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">Prose</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">Prose</cell>
                    </row>
                    <row>
                        <cell style=""text-align: left;"" rend=""DH-Default""/>
                        <cell style=""text-align: left;"" rend=""DH-Default"">all</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">pFail</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">All</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">pFail</cell>
                    </row>
                    <row>
                        <cell style=""text-align: left;"" rend=""DH-Default"">Types</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">70,422</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">24,244</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">263,042</cell>
                        <cell style=""text-align: left;"" rend=""DH-Default"">115,785</cell>
                    </row>
                </table>
                <head>Number of word types (NOUN, VERB, ADJECTIVE) for the entire corpus (""all"") and for the sets with potential errors (""pFail"").</head>
            </figure>
            <p style=""text-align: left; "">
                <hi rend=""bold"">3. Analysis</hi>
            </p>
            <p style=""text-align: left; "">Based on manual inspections of the pFail set, we postulate 13 error types described in figure 03. For each type we formulate a rule
                <note place=""foot"" xml:id=""ftn1"" n=""1"">
                    <p rend=""footnote text""> For the rules see: 
                        <ref target=""https://gitup.uni-potsdam.de/sluytergaeth/poetry_as_error"">https://gitup.uni-potsdam.de/sluytergaeth/poetry_as_error</ref>
                    </p>
                </note> which is then applied to the pFail set following the order of the error types listed below. Multiple typings are not possible.
            </p>
            <figure>
                <graphic n=""1002"" width=""16.002cm"" height=""16.69873611111111cm"" url=""Pictures/0960c097bf9db429b9efb0b418c600c6.png"" rend=""inline""/>
                <head>Description of error types.</head>
            </figure>
            <p style=""text-align: left; "">
            <hi rend=""bold"">4. Results</hi>
         </p>
            <figure>
                <graphic n=""1003"" width=""12.272141666666666cm"" height=""12.483169444444444cm"" url=""Pictures/f3d5d7e4292c1cc925909a7a425d92ac.png"" rend=""inline""/>
                <head>Relative frequency for the types of potential errors for the two pFail sets.</head>
            </figure>
            <p style=""text-align: left; "">53.33 % of the word types in the pFail set for poetry and 59.88 % of the word types in the pFail set for prose are identified. PUNC and SHORT are predominantly sub-word level characters, mostly noise which appears to a comparable extent in poetry and prose. ORTH_SZ reflects the effect of Historical Orthography which a normalisation step could remedy.</p>
            <p style=""text-align: left; "">The ten remaining types can be combined into three groups:</p>
            <list type=""unordered"">
                <item>COMP_DASH, COMP, PART_ ADJECTIVE, PREFIXED gather 
                    <hi rend=""italic"">Creative Lexis</hi>, i.e. word formation mechanisms (composition, derivation); these are often out-of-vocabulary words and therefore pipeline errors, not tool errors. In poetry, 45.25 % of the ""pFail"" set can be assigned to this group, in prose 57.09 %. 
                </item>
                <item>As expected, the pipeline produces a higher error rate for poetry (0.62 %) than for prose (0.02 %) for ORTH_UPPER, which identifies a characteristic of 
                    <hi rend=""italic"">Lyric Typography</hi> (capitalizing first letters in lines). 
                </item>
                <item>The error rate of 
                    <hi rend=""italic"">Prosodic Deformation</hi> consisting of ELISION_APO, ELISION_SIMPLE, ELISION_END, EPITHESIS and CONTRACT is also higher for poetry than for prose (6.62 % compared to 1.93 %). We assume that the deformations are due to the addition or deletion of vowels for metric reasons.
                </item>
            </list>
            <p style=""text-align: left; "">5. Outlook</p>
            <p style=""text-align: left; "">Our pipeline identifies 
                <hi rend=""italic"">Prosodic Deformation</hi>, 
                <hi rend=""italic"">Lyric Typography</hi> and 
                <hi rend=""italic"">Creative Lexis</hi> as typical sources of error when processing poetry with NLP tools. However, our pipeline needs to be optimized: too many potential errors are, as in the case of 
                <hi rend=""italic"">Creative Lexis</hi>, in fact not tool errors but pipeline errors. Additionally, our rule-based typology is only able to describe 53.33 % of the pFail set. This reveals two areas for follow-up research: the pipeline could be improved on to decrease the number of pipeline errors and the rule-based typologisation procedure could be optimized against our baseline.
            </p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Bamman, D.</hi> (2020). LitBank: Born-Literary Natural Language Processing. [Preprint]. https://people.ischool.berkeley.edu/~dbamman/pubs/pdf/Bamman_DH_Debates_CompHum.pdf [Last accessed November 16, 2021].
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Braam, H.</hi> (2019). 
                        <hi rend=""italic"">Die berühmtesten deutschen Gedichte. Auf der Grundlage von 300 Gedichtsammlungen</hi>. Stuttgart: 2. Aufl., Kröner.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Hamp, B. and Feldweg, H.</hi> (1997). GermaNet - a Lexical-Semantic Net for German. In 
                        <hi rend=""italic"">Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications</hi>. Madrid, Spain, pp. 9–15. https://aclanthology.org/W97-0802.pdf [Last accessed November 16, 2021]. 
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Henrich, V. and Hinrichs, E.</hi> (2010). GernEdiT - The GermaNet Editing Tool. In 
                        <hi rend=""italic"">Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010)</hi>, Valletta, Malta, pp. 2228-35. http://www.lrec-conf.org/proceedings/lrec2010/pdf/264_Paper.pdf [Last accessed November 16, 2021].
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Herrmann, J. B</hi>. (2018). Praktische Tagger-Kritik. Zur Evaluation des PoS-Tagging des Deutschen Textarchivs. In 
                        <hi rend=""italic"" xml:space=""preserve"">DHd2018: Kritik der digitalen Vernunft. Book of Abstracts. </hi>Cologne, Germany, pp. 287-90. https://zenodo.org/record/3684897#.YO_x1W5CTOQ [Last accessed November 16, 2021].
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Honnibal, M. et al.</hi> (2020). spaCy: Industrial-strength Natural Language Processing in Python. Zenodo. https://doi.org/10.5281/zenodo.1212303 [Last accessed November 16, 2021]. 
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Klein, W. and Geyken, A.</hi> (2010). Das ‘Digitale Wörterbuch der Deutschen Sprache DWDS’, in: Lexicographica 26: 79–96.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Müller-Zettelmann, E.</hi> (2000). 
                        <hi rend=""italic"">Lyrik und Metalyrik. Theorie einer Gattung und ihrer Selbstbespiegelung anhand von Beispielen aus der englisch- und deutschsprachigen Dichtkunst</hi>. Heidelberg, Germany, Winter.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Qi, P. et al.</hi> (2018). Universal dependency parsing from scratch, in: 
                        <hi rend=""italic"">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</hi>, Brussels, Belgium, pp. 160-70.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Rayson, P. et al.</hi> (2007). Tagging the bard: Evaluating the accuracy of a modern POS tagger on Early Modern English corpora. In 
                        <hi rend=""italic"">Proceedings of Corpus Linguistics (CL2007)</hi>. https://eprints.lancs.ac.uk/id/eprint/13011/1/192_Paper.pdf [Last accessed November 16, 2021]. 
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Schmid, H.</hi> (1994). Probabilistic part-of speech Tagging using decision trees. In 
                        <hi rend=""italic"">Proceedings of International Conference on New Methods in Language Processing</hi>, Manchester, UK, pp. 154-62.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Schmid, H.</hi> (1995). Improvements in Part-of-Speech Tagging with an Application to German. In 
                        <hi rend=""italic"">Proceedings of the ACL SIGDAT-Workshop</hi>, Dublin, Ireland, 13-25. https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/tree-tagger2.pdf [Last accessed November 16, 2021]. 
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Schmid, H</hi>. (2019). Deep learning-based morphological taggers and lemmatizers for annotating historical texts. 
                        <hi rend=""italic"">In Proceedings of the 3rd international conference on digital access to textual cultural heritage</hi>, Brussels, Belgium, 133-37.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Scheible, S. et al.</hi> (2011). A gold standard corpus of Early Modern German. In: 
                        <hi rend=""italic"">Proceedings of the 5th Linguistic Annotation Workshop</hi>, pp. 124–28. https://dl.acm.org/doi/abs/10.5555/2018966.2018981 [Last accessed November 16, 2021]. 
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Zymner, R.</hi> (2019). Begriffe der Lyrikologie. In: Hildebrandt, Claudia et al. (eds.) Lyrisches Ich, Textsubjekt, Sprecher? (= Grundfragen der Lyrikologie, Bd. 1). Berlin, Germany: De Gruyter 25–50.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,error;literaricity;nlp;poetry,English,18th century;19th century;20th century;english;europe;linguistics;literary studies;natural language processing;text mining and analysis
11796,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,"Improving Named-Entity Recognition on Inscriptions on ""ukiyo-e"" prints: Towards a ‘Distant Viewing’ in Art History",,Ewa Machotka;Marita Chatzipanagiotou;John Pavlopoulos,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left;"">Japanese early modern woodblock prints, so-called <hi rend=""italic"">ukiyo-e</hi> or ‘pictures of the floating world’ produced between the seventeenth and mid-nineteenth century, are one of the most widely recognizable visual images today. Among them landscape prints remain the most popular as evidenced by the iconic “The Great Wave” designed by Katsushika Hokusai (1760-1849) and its global career (Guth 2016). However, the understanding of these images is still shaped by Western modern epistemologies that may not be well fitted for the analysis of pre-modern non-Western artefacts (Machotka 2020). The dominant Western modern concept of landscape indicates that landscape images function as representations of places (Andrews 1999) even if art is never a mirror for reality. However, this may not be the case in relation to Japanese early modern prints, which are built on poetic traditions and may have other than representational functions (Chino 2003; Machotka 2012; Shirane 2013). Therefore, to understand the relationships between images and places there is a need to look at Japanese early-modern prints afresh and artificial intelligence has a potential to aid realization of goals. </p>
            <p style=""text-align: left;"">The existing discourse on Japanese landscape prints has mainly targeted case studies e.g. selected themes or artists (Clark 2001; Forrer et al. 2011; Kobayashi 2020), the approach which does not allow broad explorations of the geographical distribution of the sites depicted within the prints, their changing frequency in relation to their production context (e.g. time, location, designer) etc. Therefore, we argue that the combination of ‘close reading’ of the artefacts through formal and contextual analysis with so-called ‘distant viewing’ or macroanalysis of visual materials (Taylor and Tilton 2019) based on the idea of ‘distant reading’ proposed by Franco Moretti (2000) for literary studies (Gold and Klein 2016) has the potential to develop a more nuanced understanding of Japanese <hi rend=""italic"">ukiyo-e</hi> prints.</p>
            <p style=""text-align: left;"">Hence, with this work we propose that distant viewing can be facilitated by Natural Language Processing technologies such as Named Entity Recognition (NER). NER can be used to extract named locations from any text, including titles and other printed inscriptions on prints. Extracted locations can then allow for a digital geospatial macroanalysis of the studied prints, which is currently impossible as the artefacts form an exceptionally large and highly divergent corpus. However, although NER has the potential to improve the study of prints, the current state of the art NER tools are not successful in the identification of artwork titles (Jain and Krestel 2019). This is mainly due to the training data scarcity. Even recent cross-domain datasets only focus on domains such as politics and natural sciences (Liu et al., 2021), leaving art history aside. This problem is especially relevant for the analysis of non-Western pre-modern artefacts such as Japanese prints as inscriptions are rendered in pre-modern scripts used before the standardization of the language in the late nineteenth and twentieth centuries (Frellesvig 2012). In premodern Japanese, the Sino-Japanese characters could be used alternately depending on their phonetic value and the same word could be written in different characters (Yada 2012). Another problem is the ambiguity inherent to the artwork inscriptions or the lack of data. Print inscriptions are not always standardized and metadata in different collections feature different information. These important issues challenge the proposed analysis. </p>
            <p style=""text-align: left;"">Lee et al. (2018) were the first to show that transfer learning can lead to state-of-the-art results in NER for English patient note de-identification, by transferring learning from a large labeled dataset to a much smaller one. Following their work, we transferred a generic pre-trained Japanese Convolutional Neural Network NER model (Honnibal and Montani 2017) to the domain of art history, using a very limited training set of 100 labeled data. By using 100 (unseen) labeled data for evaluation, in a prior study (Chatzipanagiotou et al. 2021), we showed that transfer learning can assist NER in the Japanese language and in the field of art history, for the task of place name recognition in inscriptions of landscape prints. We registered an improvement of 28% in Precision, increasing it from 62% to 90%, and more than doubled F1, increasing it from 15% to 36%. We argue that the improved NER already allows distant viewing of the data and we show that there is room for further improvement. The access to data was facilitated by the database hosted at the Art Research Centre at Ritsumeikan University, Kyoto one of the leading Digital Humanities hubs in Japan and a collaborative partner of this project (http://www.arc.ritsumei.ac.jp/en/index.html).</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style=""text-align: left;"">Andrews, M.(1999). 
                        <hi rend=""italic"">Landscape and Western Art</hi>. Oxford Univ. Press. 
                    </bibl>
                    <bibl style=""text-align: left;"">Chino, K.(2003). The Emergence and Development of Famous Place Painting as a Genre. <hi rend=""italic"">Review of Japanese Culture and Society</hi>, 15.</bibl>
                    <bibl style=""text-align: left;"">Clark, T..(2001) 
                        <hi rend=""italic"">100 Views of Mount Fuji.</hi> Trumbull, CT.: Weatherhill. 
                    </bibl>
                    <bibl style=""text-align: left;"">Forrer, M. and Suzuki, J. and Smith, H.(2011) 
                        <hi rend=""italic"">Hiroshige: Prints and Drawings</hi>. Munich: Prestel. 
                    </bibl>
                    <bibl style=""text-align: left;"">Frellesvig, B.(2012). 
                        <hi rend=""italic"">A History of the Japanese Language</hi>. Cambridge Univerity Press. 
                    </bibl>
                    <bibl style=""text-align: left;"">Honnibal, M. and Montani, I.(2017). SpaCy 2:  Natural Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing. To appear.</bibl>
                    <bibl style=""text-align: left;"">Gold, M., and Klein, L.(2019). 
                        <hi rend=""italic"">Debates in the Digital Humanities</hi>. University of Minnesota Press.
                    </bibl>
                    <bibl style=""text-align: left;"">Guth, C.(2016). 
                        <hi rend=""italic"">Hokusai's Great Wave: Biography of a Global Icon</hi>. Hawai’i University Press. 
                    </bibl>
                    <bibl style=""text-align: left;"">Jain, N. and Krestel, R.(2019). Who is mona l.? Identifying Mentions of Artworks in Historical Archives. <hi rend=""italic"">International Conference onTheory and Practice of Digital Libraries</hi>. Springer, pp.115–122.</bibl>
                    <bibl style=""text-align: left;"">Kobayashi, F.(2020). 
                        <hi rend=""Japanese"">文政期前後の風景画入狂歌本の出版とその改題・再印</hi>:―
                        <hi rend=""Japanese"">浮世絵風景画流行の前史として</hi>―. 
                        <hi rend=""italic"" style=""font-family:MS Gothic"">
                     <hi rend=""Japanese"">浮世絵芸術</hi>
                  </hi> , 179: 5-19.
                    </bibl>
                    <bibl style=""text-align: left;"">Lee, J. and Dernoncourt, F. and Szolovits, P.(2018). Transfer Learning for Named-Entity Recognition with Neural Networks. <hi rend=""italic"">The Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</hi>. pp. 4470-4473.</bibl>
                    <bibl style=""text-align: left;"">Liu, Z. and Yu, T. and Wenliang D. and Ji, Z. and Cahyawijaya, S. and Madotto, A. and Fung, P.(2021). 
                        <hi rend=""italic"">CrossNER: Evaluating Cross-Domain Named Entity Recognition."" The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)</hi>, pp. 13452-13460.
                    </bibl>
                    <bibl style=""text-align: left;"">Machotka, E.(2020). 
                        <hi rend=""Japanese"">美術史を超えて</hi>──
                        <hi rend=""Japanese"">ヴァナキュラー・マッピングとしての日本近世風景版画</hi>. 
                        <hi rend=""italic"" style=""font-family:MS Gothic"">
                     <hi rend=""Japanese"">造形のポエティカ</hi>
                  </hi>
                        <hi rend=""italic"">―</hi>
                        <hi rend=""italic"" style=""font-family:MS Gothic"">
                     <hi rend=""Japanese"">日本美術史を巡る新たな地平</hi>
                  </hi>, ed. Hiromi N. et al. Tokyo: Seikansha.
                    </bibl>
                    <bibl style=""text-align: left;"">Moretti, F.(2000). The Slaughterhouse of Literature. 
                        <hi rend=""italic"">Modern Language Quarterly,</hi> 61:1.
                    </bibl>
                    <bibl style=""text-align: left;"">Shirane, H.(2013) 
                        <hi rend=""italic"">Japan and the Culture of the four Seasons: Nature, Literature, and the Arts</hi>. Columbia University Press.
                    </bibl>
                    <bibl style=""text-align: left;"">Taylor, A. and Tilton, L.(2019). Distant Viewing: Analyzing Large Visual Corpora. 
                        <hi rend=""italic"" xml:space=""preserve"">Digital Scholarship in the Humanities, </hi>34:1: i3–i16. 
                    </bibl>
                    <bibl style=""text-align: left;"">Yada, T.(2012). 
                        <hi rend=""Japanese"">矢田勉</hi>. 
                        <hi rend=""Japanese"">国語文字・表記史の研究</hi>. Tokyo: Kyūko Shoin.
                    </bibl>
                    <bibl style=""text-align: left;"">Chatzipanagiotou, M. and Machotka, E. and Pavlopoulos, J.(2021). Automated Recognition of Geographical Named Entities in Titles of Ukiyo-e prints. <hi rend=""italic"">Digital Humanities Workshop (DHW 2021). Association for Computing Machinery</hi>, New York, USA, 70–77.</bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,art history;metadata;ner;nlp;ukiyo-e,English,18th century;19th century;art history;artificial intelligence and machine learning;asia;computer science;english;europe;natural language processing
11839,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,Reconstruction of cultural memory through digital storytelling: a case study of Shanghai Memory project,,Yaming Fu;Simon Mahony;Wei Liu,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p>
                <anchor xml:id=""OLE_LINK20""/>
                <anchor xml:id=""OLE_LINK21""/>
                <anchor xml:id=""OLE_LINK18""/>
                <anchor xml:id=""OLE_LINK19""/>
                <anchor xml:id=""Hlk100482290""/>
                <anchor xml:id=""Hlk89610860""/>The theory and practice of digital storytelling has been developing since the 1990s; firstly in media research, with a focus on audio-visual story creation using digital media 
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Lambert"",""given"":""Joe"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""edition"":""5th ed."",""id"":""ITEM-1"",""issued"":{""date-parts"":[[""2018""]]},""publisher"":""Routledge"",""title"":""Digital Storytelling: Capturing Lives, Creating Community"",""type"":""book""},""uris"":[""http://www.mendeley.com/documents/?uuid=70e8172b-f3c0-4421-8199-14b8762090f2""]}],""mendeley"":{""formattedCitation"":""(Lambert, 2018)"",""plainTextFormattedCitation"":""(Lambert, 2018)"",""previouslyFormattedCitation"":""(Lambert, 2018)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Lambert, 2018), and then extending into multiple fields such as public history 
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Burgess"",""given"":""Jean"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Klaebe"",""given"":""Helen"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""chapter-number"":""10"",""container-title"":""Story Circle: Digital Storytelling around the World"",""editor"":[{""dropping-particle"":"""",""family"":""Hartley"",""given"":""John"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""McWilliam"",""given"":""Kelly"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""id"":""ITEM-1"",""issued"":{""date-parts"":[[""2009""]]},""page"":""155-166"",""title"":""Digital Storytelling as Participatory Public History in Australia"",""type"":""chapter""},""uris"":[""http://www.mendeley.com/documents/?uuid=0fcd1105-602e-45da-8e42-97ff586c8481""]}],""mendeley"":{""formattedCitation"":""(Burgess & Klaebe, 2009)"",""plainTextFormattedCitation"":""(Burgess & Klaebe, 2009)"",""previouslyFormattedCitation"":""(Burgess & Klaebe, 2009)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Burgess & Klaebe, 2009) and education 
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Robin"",""given"":""B. R."",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""container-title"":""Theory into Practice"",""id"":""ITEM-1"",""issue"":""3"",""issued"":{""date-parts"":[[""2008""]]},""page"":""220-228"",""title"":""Digital storytelling: A powerful technology tool for the 21st century classroom"",""type"":""article-journal"",""volume"":""47""},""uris"":[""http://www.mendeley.com/documents/?uuid=402ab76f-b385-4d6c-a7d3-1cc243f73e52""]}],""mendeley"":{""formattedCitation"":""(Robin, 2008)"",""plainTextFormattedCitation"":""(Robin, 2008)"",""previouslyFormattedCitation"":""(Robin, 2008)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Robin, 2008), where there is a close relationship with human narrative. These fields discussed the possibilities for digital storytelling as they encountered the so-called 
                ""digital turn
                "" which prompted the move from traditional storytelling research into the digital sphere and brought about epistemological as well as methodological shifts 
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Noiret"",""given"":""Serge"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""chapter-number"":""Seven"",""container-title"":""A Companion to Public History"",""edition"":""1st ed."",""editor"":[{""dropping-particle"":"""",""family"":""Dean"",""given"":""David"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""id"":""ITEM-1"",""issued"":{""date-parts"":[[""2018""]]},""page"":""111-124"",""title"":""Digital Public History"",""type"":""chapter""},""uris"":[""http://www.mendeley.com/documents/?uuid=eaf2559b-8e39-438e-a1f4-390324522222""]}],""mendeley"":{""formattedCitation"":""(Noiret, 2018)"",""plainTextFormattedCitation"":""(Noiret, 2018)"",""previouslyFormattedCitation"":""(Noiret, 2018)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Noiret, 2018).
            </p>
            <p>Digital storytelling, understood here as a movement or method for creating, expressing, interpreting, and sharing stories and personal experiences using digital tools, has been viewed as a 
                <anchor xml:id=""OLE_LINK7""/>
                <anchor xml:id=""OLE_LINK8""/>
                ""democratization of culture.
                ""
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Clarke"",""given"":""Robert"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Adam"",""given"":""Andrea"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""container-title"":""Arts and Humanities in Higher Education"",""id"":""ITEM-1"",""issue"":""1-2"",""issued"":{""date-parts"":[[""2011""]]},""page"":""157-176"",""title"":""Digital storytelling in Australia: academic perspectives and reflections"",""type"":""article-journal"",""volume"":""11""},""uris"":[""http://www.mendeley.com/documents/?uuid=d0e73f41-e94d-49e6-85c7-6cd2cab4e6fb""]}],""mendeley"":{""formattedCitation"":""(Clarke & Adam, 2011)"",""plainTextFormattedCitation"":""(Clarke & Adam, 2011)"",""previouslyFormattedCitation"":""(Clarke & Adam, 2011)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Clarke & Adam, 2011) It draws attention from the mainstream to the marginalized, the minority, the overlooked and forgotten. Despite the discourse and practice of digital storytelling in education, history, and media research, its theory construction in DH and the practice in GLAMs is still at an exploratory stage. 
            </p>
            <p>Digital storytelling provides new opportunities for DH as both fields seek to encourage dialogue, make the world comprehensible, and discover new ways of interaction with the support of digital tools 
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""abstract"":""At first thought, combining storytelling, digital tools, and humanities seems improbable. For example, digital storytelling is characterized by interactivity, nonlinearity, flexible outcomes, user participation, even co-creation. Such affordances may be disruptive to traditional humanities scholars accustomed to working alone, with physical objects, and following established theoretical guidelines. However, they may be quite appealing to those seeking new opportunities for cross-disciplinary, iterative approaches to practice-based humanities scholarship and pedagogy. This essay defines digital storytelling as a combination of storytelling techniques, digital affordances, and humanities foci, describes several forms of digital storytelling, outlines frameworks and outcomes associated with their use, and promotes digital storytelling as providing new opportunities for humanities scholarship and teaching, especially with regard to critical thinking, communication, digital literacy, and civic engagement."",""author"":[{""dropping-particle"":"""",""family"":""Barber"",""given"":""John F."",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""container-title"":""Cogent Arts and Humanities"",""id"":""ITEM-1"",""issue"":""1"",""issued"":{""date-parts"":[[""2016""]]},""publisher"":""Cogent"",""title"":""Digital storytelling: New opportunities for humanities scholarship and pedagogy"",""type"":""article-journal"",""volume"":""3""},""uris"":[""http://www.mendeley.com/documents/?uuid=ceccbb16-d133-44d0-a065-41e561a9435e""]}],""mendeley"":{""formattedCitation"":""(Barber, 2016)"",""plainTextFormattedCitation"":""(Barber, 2016)"",""previouslyFormattedCitation"":""(Barber, 2016)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Barber, 2016). Digital storytelling also serves as a bridge between cultural heritage and DH with 
                ""space and time as shared concepts,
                ""
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Münster"",""given"":""S."",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Apollonio"",""given"":""F. I."",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Bell"",""given"":""P."",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Kuroczynski"",""given"":""P."",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Lenardo"",""given"":""I."",""non-dropping-particle"":""Di"",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Rinaudo"",""given"":""F."",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Tamborrino"",""given"":""R."",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""container-title"":""The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences"",""id"":""ITEM-1"",""issued"":{""date-parts"":[[""2019""]]},""page"":""813-820"",""title"":""Digital Cultural Heritage meets Digital Humanities"",""type"":""paper-conference""},""uris"":[""http://www.mendeley.com/documents/?uuid=fb807543-6385-46a2-b2b6-ef425b7287e4""]}],""mendeley"":{""formattedCitation"":""(Münster et al., 2019)"",""plainTextFormattedCitation"":""(Münster et al., 2019)"",""previouslyFormattedCitation"":""(Münster et al., 2019)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Münster et al., 2019) essential dimensions in storytelling and other forms of narratives. A great potential exists for DH practitioners to employ GLAM collections to reconstruct knowledge and cultural heritage, discover hidden knowledge, and support knowledge creation through the lens of digital storytelling. 
            </p>
            <p>
                <anchor xml:id=""OLE_LINK15""/>
                <anchor xml:id=""OLE_LINK14""/>This presentation, drawing on extensive published literature and in-depth reflection, examines how digital storytelling is applied to encourage and facilitate cultural memory reconstruction as part of the Shanghai Memory project.
                <note place=""foot"" xml:id=""ftn0"" n=""1"">
                    <p rend=""footnote text""> Shanghai Memory: http://memory.library.sh.cn</p>
                </note> Relevant aspects focus on democratizing DH practice and the theory of cultural memory construction. 
                <hi rend=""italic"">A Journey from Wukang Road</hi>
                <note place=""foot"" xml:id=""ftn1"" n=""2"">
                    <p rend=""footnote text""> A Journey from Wukang Road: http://wkl.library.sh.cn</p>
                </note>, a centrepiece of this project, associates the three dimensions of memory (the past), culture, and community as proposed by Assmann and Czaplicka (1995) to organize and construct the diverse collections pertaining to Wukang Road. Borrowing thinking from postcolonial studies around critical 
                ""re-reading
                "" and 
                ""re-writing
                "" of the colonial past, along with the continuing effect of memory 
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Ashcroft"",""given"":""Bill"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Griffiths"",""given"":""Gareth"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Tiffin"",""given"":""Helen"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""edition"":""2nd ed."",""id"":""ITEM-1"",""issued"":{""date-parts"":[[""2002""]]},""publisher"":""Routledge"",""publisher-place"":""London, New York"",""title"":""The Empire Writes Back: Theory and Practice in Post-Colonial Literatures"",""type"":""book""},""locator"":""221"",""uris"":[""http://www.mendeley.com/documents/?uuid=a8df0d8a-93f2-4d3b-8be4-47b7d737ec03""]}],""mendeley"":{""formattedCitation"":""(Ashcroft et al., 2002, p. 221)"",""plainTextFormattedCitation"":""(Ashcroft et al., 2002, p. 221)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Ashcroft et al., 2002, p. 221), it recognizes and tells the holistic story of the past. Memory is achieved through knowledge organization and representation methods, including ontology design for people, places, time, events, architectures, etc.; resource description framework (RDF) to describe resources in a universal way and linked data to connect the entities. Culture is presented using both historical records from the library and contemporary reflections from the public. The community aspect engages citizens by having them upload photos and personal accounts of their memories and experiences of the road, adding to the underrepresented art forms housed in library collections (magazines, music recordings, photos, maps, and old movies), the places and people that constitute the history of Wukang Road. 
            </p>
            <p>Wukang Road is famous as the home of many celebrities and historic buildings going back to the colonial era, all having their own stories. Here with digital storytelling, sharing methodologies with oral and public history, we capture the voice of the common people so that the history and culture of Shanghai is democratized in the modern postcolonial era. While the buildings themselves are monuments to the formal history, the 
                ""road is the smallest unit of urban geography [and] another focus of urban memory is the space-time structure,
                ""
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Xia"",""given"":""Cuijuan"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Wang"",""given"":""Lihua"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Liu"",""given"":""Wei"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""container-title"":""Digital Scholarship in the Humanities"",""id"":""ITEM-1"",""issue"":""4"",""issued"":{""date-parts"":[[""2021""]]},""page"":""841-857"",""title"":""Shanghai memory as a digital humanities platform to rebuild the history of the city"",""type"":""article-journal"",""volume"":""36""},""locator"":""849"",""uris"":[""http://www.mendeley.com/documents/?uuid=a6c443fa-c26c-4c0b-9342-76c572c522e6""]}],""mendeley"":{""formattedCitation"":""(Xia et al., 2021, p. 849)"",""plainTextFormattedCitation"":""(Xia et al., 2021, p. 849)"",""previouslyFormattedCitation"":""(Xia et al., 2021, p. 849)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Xia et al., 2021, p. 849) which is why these stories fill the gaps over time and give voice to those usually unheard.
            </p>
            <p>The project uses crowdsourcing method for the public to create digital stories, shifting them from the private to the public sphere, from 
                ""private forms of communication and translating them into contexts where they can potentially contribute to public culture.
                ""
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Burgess"",""given"":""Jean"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Klaebe"",""given"":""Helen"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""chapter-number"":""10"",""container-title"":""Story Circle: Digital Storytelling around the World"",""editor"":[{""dropping-particle"":"""",""family"":""Hartley"",""given"":""John"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""McWilliam"",""given"":""Kelly"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""id"":""ITEM-1"",""issued"":{""date-parts"":[[""2009""]]},""page"":""155-166"",""title"":""Digital Storytelling as Participatory Public History in Australia"",""type"":""chapter""},""locator"":""155"",""uris"":[""http://www.mendeley.com/documents/?uuid=0fcd1105-602e-45da-8e42-97ff586c8481""]}],""mendeley"":{""formattedCitation"":""(Burgess & Klaebe, 2009, p. 155)"",""plainTextFormattedCitation"":""(Burgess & Klaebe, 2009, p. 155)"",""previouslyFormattedCitation"":""(Burgess & Klaebe, 2009, p. 155)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Burgess & Klaebe, 2009, p. 155) Using digital storytelling in this way makes it an additional tool for researchers in public history and importantly 
                ""the recording of oral histories.
                ""
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Earley-Spadoni"",""given"":""Tiffany"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""container-title"":""Journal of Archaeological Science"",""id"":""ITEM-1"",""issued"":{""date-parts"":[[""2017""]]},""page"":""95-102"",""publisher"":""Elsevier Ltd"",""title"":""Spatial History, deep mapping and digital storytelling: archaeology's future imagined through an engagement with the Digital Humanities"",""type"":""article-journal"",""volume"":""84""},""locator"":""97"",""uris"":[""http://www.mendeley.com/documents/?uuid=42e308b7-5fa7-486d-b24a-603b1d7852fe""]}],""mendeley"":{""formattedCitation"":""(Earley-Spadoni, 2017, p. 97)"",""plainTextFormattedCitation"":""(Earley-Spadoni, 2017, p. 97)"",""previouslyFormattedCitation"":""(Earley-Spadoni, 2017, p. 97)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Earley-Spadoni, 2017, p. 97) The wider project identifies the material culture embedded in heritage objects and linked with sources makes 
                ""literature the historical witness for the material cultural heritage objects themselves.
                ""
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Xia"",""given"":""Cuijuan"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Wang"",""given"":""Lihua"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Liu"",""given"":""Wei"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""container-title"":""Digital Scholarship in the Humanities"",""id"":""ITEM-1"",""issue"":""4"",""issued"":{""date-parts"":[[""2021""]]},""page"":""841-857"",""title"":""Shanghai memory as a digital humanities platform to rebuild the history of the city"",""type"":""article-journal"",""volume"":""36""},""locator"":""844"",""uris"":[""http://www.mendeley.com/documents/?uuid=a6c443fa-c26c-4c0b-9342-76c572c522e6""]}],""mendeley"":{""formattedCitation"":""(Xia et al., 2021, p. 844)"",""plainTextFormattedCitation"":""(Xia et al., 2021, p. 844)"",""previouslyFormattedCitation"":""(Xia et al., 2021, p. 844)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Xia et al., 2021, p. 844) The personalized experience of citizens serve as an effective supplement to the formal literary accounts.
            </p>
            <p>The Shanghai Memory project brings together many aspects of memory construction as part of a comprehensive programme of heritage management 
                <hi rend=""background(none)"" xml:space=""preserve"">to more accurately reconstruct the history </hi>of the city 
                <?biblio ADDIN CSL_CITATION {""citationItems"":[{""id"":""ITEM-1"",""itemData"":{""author"":[{""dropping-particle"":"""",""family"":""Xia"",""given"":""Cuijuan"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Wang"",""given"":""Lihua"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""},{""dropping-particle"":"""",""family"":""Liu"",""given"":""Wei"",""non-dropping-particle"":"""",""parse-names"":false,""suffix"":""""}],""container-title"":""Digital Scholarship in the Humanities"",""id"":""ITEM-1"",""issue"":""4"",""issued"":{""date-parts"":[[""2021""]]},""page"":""841-857"",""title"":""Shanghai memory as a digital humanities platform to rebuild the history of the city"",""type"":""article-journal"",""volume"":""36""},""uris"":[""http://www.mendeley.com/documents/?uuid=a6c443fa-c26c-4c0b-9342-76c572c522e6""]}],""mendeley"":{""formattedCitation"":""(Xia et al., 2021)"",""plainTextFormattedCitation"":""(Xia et al., 2021)"",""previouslyFormattedCitation"":""(Xia et al., 2021)""},""properties"":{""noteIndex"":0},""schema"":""https://github.com/citation-style-language/schema/raw/master/csl-citation.json""}?>(Xia et al., 2021). This latest initiative to incorporate digital storytelling is the next phase to further democratize the practice and represent the unrepresented by presenting, creating, and sharing stories in relation to the past, current, and even the future of Shanghai city. This extension to an already established DH project adds significant value to the reconstruction of cultural memory and acts as a model for other memory projects in East Asia and beyond. 
            </p>
        </body>
        <back>
            <div type=""bibliogr"">
            <head>Bibliography</head>
                <listBibl>
               <bibl>
                <?biblio ADDIN Mendeley Bibliography CSL_BIBLIOGRAPHY?>Ashcroft, B., Griffiths, G., & Tiffin, H. (2002). 
                <hi rend=""italic"">The Empire Writes Back: Theory and Practice in Post-Colonial Literatures</hi> (2nd ed.). Routledge.
            </bibl>
               <bibl>Assmann, J., & Czaplicka, J. (1995). Collective Memory and Cultural Identity. 
                <hi rend=""italic"">New German Critique</hi>, 
                <hi rend=""italic"">65</hi>, 125–133.
            </bibl>
               <bibl>Barber, J. F. (2016). Digital storytelling: New opportunities for humanities scholarship and pedagogy. 
                <hi rend=""italic"">Cogent Arts and Humanities</hi>, 
                <hi rend=""italic"">3</hi>(1).
            </bibl>
               <bibl>Burgess, J., & Klaebe, H. (2009). Digital Storytelling as Participatory Public History in Australia. In J. Hartley & K. McWilliam (Eds.), 
                <hi rend=""italic"">Story Circle: Digital Storytelling around the World</hi> (pp. 155–166).
            </bibl>
               <bibl>Clarke, R., & Adam, A. (2011). Digital storytelling in Australia: academic perspectives and reflections. 
                <hi rend=""italic"">Arts and Humanities in Higher Education</hi>, 
                <hi rend=""italic"">11</hi>(1–2), 157–176.
            </bibl>
               <bibl>Earley-Spadoni, T. (2017). Spatial History, deep mapping and digital storytelling: archaeology’s future imagined through an engagement with the Digital Humanities. 
                <hi rend=""italic"">Journal of Archaeological Science</hi>, 
                <hi rend=""italic"">84</hi>, 95–102.
            </bibl>
               <bibl>Lambert, J. (2018). 
                <hi rend=""italic"">Digital Storytelling: Capturing Lives, Creating Community</hi> (5th ed.). Routledge.
            </bibl>
               <bibl>Münster, S., Apollonio, F. I., Bell, P., Kuroczynski, P., Di Lenardo, I., Rinaudo, F., & Tamborrino, R. (2019). Digital Cultural Heritage meets Digital Humanities. 
                <hi rend=""italic"">The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</hi>, 813–820.
            </bibl>
               <bibl>Noiret, S. (2018). Digital Public History. In D. Dean (Ed.), 
                <hi rend=""italic"">A Companion to Public History</hi> (1st ed., pp. 111–124).
            </bibl>
               <bibl>Robin, B. R. (2008). Digital storytelling: A powerful technology tool for the 21st century classroom. 
                <hi rend=""italic"">Theory into Practice</hi>, 
                <hi rend=""italic"">47</hi>(3), 220–228.
            </bibl>
               <bibl>Xia, C., Wang, L., & Liu, W. (2021). Shanghai memory as a digital humanities platform to rebuild the history of the city. 
                <hi rend=""italic"">Digital Scholarship in the Humanities</hi>, 
                <hi rend=""italic"">36</hi>(4), 841–857.
            </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,cultural memory reconstruction;democratization of culture;digital storytelling;heritage management;postcolonial digital humanities,English,asia;asian studies;contemporary;crowdsourcing;cultural studies;english;global;public humanities collaborations and methods
11841,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,A code for Murakami’s Tokyo: spatial diversity analyzed by digital means,,Simone Abbiati,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">From Shibuya ward with its intense nightlife in 
                <hi rend=""italic"" xml:space=""preserve"">After Dark </hi>and the commercial area of Kichijōji in 
                <hi rend=""italic"">Sputnik Sweetheart</hi>, to Setagaya’s highway in 
                <hi rend=""italic"">1Q84</hi>, Bunkyo's Rikugien Garden in 
                <hi rend=""italic"">Norwegian Wood</hi>, Aoyama Cemetery in 
                <hi rend=""italic"">South of the Border, West of the Sun</hi>, and the Pacific Hotel near Shinagawa station in 
                <hi rend=""italic"" xml:space=""preserve"">The Wind-Up Bird Chronicle – </hi>Haruki Murakami, arguably the most famous Japanese contemporary writer, has left his mark on the capital of Japan both within and outside of fiction. In his fourteen novels, Tokyo’s thousand faces appear in great detail, even though they are filtered through a wide variety of characters’ perspectives and reconceptualized by magical realism. Regardless of the literary space abstraction, Seymour Chatman’s theory affirms that literary characters are cognitively experienced in the same way a reader would get to know a person in real life; and Akhil Gupta’s research argues the existence of a link between spatial perception and cultural identity. This would suggest that the spatial conceptualization performed by literary characters entail a 
                <hi rend=""italic"" xml:space=""preserve"">forma vivendi </hi>ascribable to a certain cultural environment. But is it really that simple? Does an identifiable Tokyo's 
                <hi rend=""italic"">genius loci</hi> emerge from Murakami’s novels? In the wake of Computational Criticism, we will use code-writing to help us get a glimpse of the characters’ cognitive mapping processes and thus delve into a literary-cultural analysis, bearing in mind that imaginative literature is the result of layers of mediation and re-presentation causing straightforward cultural seeping-through to be questioned. In order to do this, we present a Python script able to extract spatial data concerning the cognitive mapping of Murakami’s fictional figures. 
            </p>
            <p style=""text-align: left; ""> In fact, in the last few years, literary spatiality has emerged both from a theoretical and a digital perspective. In light of this, the results given by the software application will be discussed in order to reflect on Tokyo as an example of Japanese urbanisation, and to identify pros and cons of digitally-assisted interpretive acts regarding spatiality. Lastly, in treating Murakami’s fictional spatiality with digital tools, particular attention will be given to recent criticism of distant reading and corpus selection. </p>
            <p style=""text-align: left; ""> From a technical perspective, the presented script will use Natural Language Processing to tokenize, tag, and parse a selected corpus from of Murakami's novels to eventually perform Named Entity Recognition. The script will identify when movement verbs present fictional characters as subjects, leading to the extraction of the starting points and destinations of some of their itineraries. Thus, it will be possible to identify meaningful landmarks and itineraries of some characters, leading to the schematization of their cognitive maps. </p>
            <p style=""text-align: left; ""> The spatial structures extracted by digital means will be interpreted in view of cultural differences that Eastern and Western societies present as far as living in metropolitan areas is concerned. Spurious results (counterfactual spatial indications, and phraseological expressions, among others) will also be considered from a technical perspective. Lastly, with respect to the multilingual DH approach, the analysis will be conducted onto the English corpus but a few observations the original text in Japanese language will be presented, thus suggesting new possible research paths.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Algee-Hewitt, M.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2017). Canon/Archive: Studies in Quantitative Formalism from the Stanford Literary Lab, N+1 Foundation</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Bode K.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2018). A World of Fiction: Digital Collections and the Future of Literary History, University of Michigan Press.</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Bode, K.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2020). </hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">«Why You Can</hi>
                        <hi rend=""italic"" style=""font-family:Arial Unicode MS;font-size:9pt"">’</hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">t Model Away Bias»</hi>
                        , Modern Language Quarterly, 81, 95-124
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Bushell, S.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2020). Reading and Mapping Fiction: Spatialising the Literary Text, Cambridge University Press</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Chatman, S.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2019). Story and Discourse: Narrative Structure in Fiction and Film, Cornell University Press</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Cooper, D. and Gregory, I.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2011) «</hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">Mapping the English Lake District: A Literary GIS»</hi>
                        , Transactions of the Institute of British Geographers 36, no. 1, 89–108
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Cooper, D., Donaldson, C., and Murrieta-Flores, P.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2016). Literary Mapping in the Digital Age, Burlington</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Gitelman L.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2013). Raw Data Is an Oxymoron, MIT Press</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Gupta, A. and Ferguson, J.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (1992). «</hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">Space, Identity, and the Politics of Difference»</hi>
                        , in Cultural Anthropology, Vol. 7, No. 1, pp. 6-23
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Herman, D.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2002). Story logic. Problems and possibilities of narrative, University of Nebraska Press.</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Heuser, R., Moretti, M., and Steiner, E.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2016). </hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">«The Emotions of London»</hi>
                        , Pamphlets of the Stan- ford Literary Lab
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve"">Hillman, J. </hi>
                        (1992). The Thought of the Heart and the Soul of the World, Spring
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Loper, E., Klein, E., Bird, S</hi>
                        . (2009). Natural Language Processing with Python, O’Reilly Media
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Marie-Laure Ryan, et al.</hi>
                        (2016). Narrating Space/spatializing Narrative: Where Narrative Theory and Geography Meet, Ohio State University Press
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Moretti F.</hi>
                        (1997). Atlas of the European Novel, Einaudi
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Moretti, F.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2013). Distant Reading, Verso</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Nan Z, D.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2019).</hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">«The Computational Case against Computational Literary Studies»</hi>
                        , in Critical Inquiry, 45
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve"">Schmid, C., Karaman, O., Hanakata, N. C., Kallenberger, P., Kockelkorn, A., Sawyer, L., Streule, M., & Wong, K. P. </hi>
                        (2018). Towards a new vocabulary of urbanisation processes: A comparative approach, Urban Studies, 55(1), 19–52
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Sorensen, A.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (1999). </hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">«Land Readjustment, Urban Planning and Urban Sprawl in the Tokyo Metropolitan Area»</hi>
                        , Urban Studies, 36(13), 2333–2360
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Strecher, M. C.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (1999). </hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">«Magical Realism and the Search for Identity in the Fiction of Murakami Haruki»</hi>
                        , The Journal of Japanese Studies , Vol. 25, No. 2, pp. 263-298
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Strecher, M. C.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2021). Dances with Sheep: The Quest for Identity in the Fiction of Murakami Haruki, University of Michigan Press</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Suter, R.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2008). The Japanization of modernity: Murakami Haruki between Japan and the United States, Harvard University Asia Center</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Tally, R.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2013). Spatiality, Routledge</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Underwood, T.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2019). Distant Horizons: Digital Evidence and Literary Change, University of Chicago Press</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Vasiliev, Y</hi>
                        . (2020). Natural Language Processing with Python and SpaCy: A Practical Intro- duction, No Starch Pres
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Westphal B.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2009). Geocriticism: Real and Fictional Spaces, Armando editore</hi>
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"" style=""font-family:Helvetica Neue;font-size:9pt"">Wilkens, M.</hi>
                        <hi style=""font-family:Helvetica Neue;font-size:9pt"" xml:space=""preserve""> (2021). «</hi>
                        <hi rend=""italic"" style=""font-family:Helvetica Neue;font-size:9pt"">“Too isolated, too insular”: American Literature and the World»</hi>
                        , The journal of Cultural analytics
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,geocriticism;murakami;named entity recognition;nlp;text mining,English,"asia;contemporary;english;europe;geography and geo-humanities;literary studies;north america;spatial & spatio-temporal analysis, modeling and visualization;text mining and analysis"
11895,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,HTR2CritEd: A Semi-Automatic Pipeline to Produce a Critical Digital Edition of Literary Texts with Multiple Witnesses out of Text Created through Handwritten Text Recognition,,Daniel Stoekl Ben Ezra;Hayim Lapin;Bronson Brown-DeVost;Pawel Jablonski,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p>Database structures and export formats of Handwritten Text Recognition tools (e.g. Transkribus, Tesseract, eScriptorium) are usually based on a document layout hierarchy with regions/zones and lines. Interlinear or marginal additions to the main text are in separate zones and lines (Page XML, Alto XML) (Kahle et al. 2017, Stokes et al. 2021).</p>
            <p>While this is less problematic for documentary texts (Chagué et al. 2021), it poses a problem for those working on critical editions of literary texts with multiple textual witnesses because any such edition presupposes a running text hierarchy (books, chapters, verses), where the interlinear and marginal additions need to be inserted at the right spots. This is a precondition to using text-alignment tools such as CollatEx (Dekkers et al. 2011).</p>
            <p>We present a pipeline that permits to overcome this problem for Medieval Hebrew manuscripts in a semi-automatized fashion beginning with the discovery of insertion marks in the HTR process and leading to a critical edition in TEI: </p>
            <list rend=""numbered"">
                <item>We include a series of different insertion marks in the recognition training data for the HTR. Different insertion marks distinguish between interlinear and marginal additions (Stökl Ben Ezra et al. 2021). </item>
                <item>Optimal matches of insertion marks with a) interlinear lines and b) marginal additions are calculated with the “Hungarian Algorithm” (Kuhn 1955). The results can be visualized via eScriptorium’s API for image annotation (see fig. 1).</item>
                <item>A. If there is already an e-text of a printed edition with an accepted text hierarchy, we use the Dicta synopsis-algorithm via an API (Brill et al. 2020) or alternatively a combination of global and local alignments of the Smith-Waterman (1981) and Needleman Wunsch (1970) algorithms to align the main text of the HTRed manuscript with the standard edition to calculate the places for the textual hierarchy markers. This needs to be manually verified subsequentially, especially if some of the markers for the text hierarchy should be in the interlinear or marginal additions.
                    <lb/>B. If there is no printed edition, the text hierarchy markers need to be inserted manually. This is usually necessary only for one manuscript (if there is a manuscript that represents the complete text).
                </item>
                <item>Based on the combination of 2 and 3, the first manuscript transcription in the HTR tool can now be converted from document hierarchy to text hierarchy TEI. If there was no printed edition (3B), the text hierarchy markers of this manuscript can be used in step 3A to automatically insert them. </item>
                <item>The resulting data is submitted via json to an optimized Needleman-Wunsch algorithm, Collatex or another alignment tool (Brill et al. 2020) to automatically produce an alignment between the different witnesses. For error correction, Microsoft Excel can be used or the tool in step 7. </item>
                <item>Text comparison in the alignment can serve to resolve most of the abbreviations.</item>
                <item>The final result is fed into TEI-Publisher (Turska and Meier 2021). We hope to be able to integrate a tabular tool that allows to manually but ergonomically correct any misalignments of the automatic alignment process to produce the critical edition: 
                    <ref target=""https://editions.erabbinica.org/"">https://editions.erabbinica.org/</ref>
                </item>
                <item>The TEI-Publisher publication includes accessibility via DTS (Distributed Text Service).</item>
            </list>
            <p style=""text-align: left;"">Fig. 1: eScriptorium with 3 panels turned on: On the left, the image annotation panel with the triangles representing the links between marginal (blue) or interlinear (red) insertion spots and the first and last word of the insertion, the segmentation panel. In the center, the image annotation panel with the main text and the marginal and interlinear text lines. On the right, the manually corrected automatic transcription.</p>
            <figure>
                <graphic n=""1001"" width=""16.002cm"" height=""9.001125cm"" url=""Pictures/9df966d83123c990dafdab87b641f41e.png"" rend=""inline""/>
            </figure>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Almas, B., Clérice, T., Cayless, H., Jolivet, V., Liuzzo, P., Romanello, M., Robie, J., and Scott, I.</hi> (2021). Distributed Text Services (DTS): a Community-built API to Publish and Consume Text Collections as Linked Data (
                        <ref target=""https://hal.archives-ouvertes.fr/hal-03183886"">https://hal.archives-ouvertes.fr/hal-03183886</ref>) 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Brill, O., Koppel, M., and Shmidman, A.</hi> (2020). FAST: Fast and Accurate Synoptic Texts. 
                        <hi rend=""italic"">Digital Scholarship Humanities</hi> 35(2): 254-264.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Chagué, A. and Scheithauer, H.</hi>
                        <hi rend=""HTML_Code""> (2021). page2tei - LECTAUREP [Computer software]</hi>: 
                        <ref target=""https://github.com/lectaurep/page2tei"">https://github.com/lectaurep/page2tei</ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Dekker, R. H. and Middell, G.</hi> (2011). Computer-Supported Collation with CollateX: Managing Textual Variance in an Environment with Varying Requirements. 
                        <hi rend=""italic"">Supporting Digital Humanities</hi> University of Copenhagen, Denmark. 17-18 November 2011.
                    </bibl>
                    <bibl>
                        <hi rend=""HTML_Cite"">
                            <hi rend=""bold"" xml:space=""preserve"">Kahle, P., Colutto, S., Hackl, G., </hi>
                            <hi rend=""bold"" xml:space=""preserve"">and </hi>
                        </hi>
                        <hi rend=""this-person"">
                            <hi rend=""bold"">Mühlberger</hi>
                        </hi>
                        <hi rend=""HTML_Cite"">
                            <hi rend=""bold"">,</hi>
                            <hi rend=""bold"" xml:space=""preserve""> G.</hi> (2017). 
                        </hi>
                        <hi rend=""Titre1"">Transkribus - A Service Platform for Transcription, Recognition and Retrieval of Historical Documents,”</hi>
                        <hi rend=""HTML_Cite"" xml:space=""preserve""> OST@ICDAR 2017: 19-24</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""reference-text"">
                            <hi rend=""bold"">Kuhn, H.</hi> (1955). ""The Hungarian Method for the assignment problem,"" 
                            <hi rend=""italic"">Naval Research Logistics Quarterly</hi>, 2: 83–97. 
                        </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""HTML_Cite"">
                            <hi rend=""bold"" xml:space=""preserve"">Needleman, </hi>
                            <hi rend=""bold"" xml:space=""preserve"">S. and </hi>
                            <hi rend=""bold"">Wunsch, C.</hi> (1970). ""A general method applicable to the search for similarities in the amino acid sequence of two proteins,"" Journal of Molecular Biology 48 (3): 443–53.
                        </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""HTML_Cite"">
                            <hi rend=""bold"">Smith, T</hi>
                            <hi rend=""bold"">. and</hi>
                            <hi rend=""bold"" xml:space=""preserve""> Waterman, M.</hi> (1981). ""Identification of Common Molecular Subsequences."" Journal of Molecular Biology 147 (1): 195–197.
                        </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Stokes, P., Stökl Ben Ezra, D., Kiessling, B., Tissot, R. and Gargem, E.</hi> (2021). “
                        <ref target=""https://classics-at.chs.harvard.edu/classics18-stokes-kiessling-stokl-ben-ezra-tissot-gargem/"">The eScriptorium VRE for Manuscript Cultures</ref>” in Claire 
                        <hi rend=""markgeiryylgw"">Clivaz</hi> and Garrick V. Allen (eds), 
                        <hi rend=""italic"">Ancient Manuscripts and Virtual Research Environments</hi>, special issue, 
                        <hi rend=""italic"">Classics@</hi> 18 n.p. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Stökl Ben Ezra, D., Brown-DeVost, B., and Jablonski, P.</hi> (2021). “Exploiting Insertion Symbols for Marginal Additions in the Recognition Process to Establish Reading Order” 
                        <hi rend=""italic"">IWCP@ICDAR</hi> 2021.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Turska, M., Meier, W.</hi> (2021). 
                        <hi rend=""italic"">TEI Publisher</hi>
                        <ref target=""http://teipublisher.com"">http://teipublisher.com</ref>. (version 7, 2021)
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,critical digital edition;hebrew;htr;manuscripts;nlp,English,"15th-17th century;5th-14th century;analysis;asia;bce-4th century;digital libraries creation, management, and analysis;english;europe;humanities computing;philology;scholarly editing and editions development, analysis, and methods"
11896,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,"Development of a ""Devanāgarī"" Optical Character Recognition (OCR) System",,Takahiro Kato;Yūki Tomonari;Chikamitsu Taniguchi;Tomejiro Osawa;Satoshi Fujimaki;Takashi Okada;Emi Hashimoto,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p>This poster outlines the objectives of the research project titled “Development of a 
                <hi rend=""italic"">Devanāgarī</hi> Optical Character Recognition (OCR) System.” 
            </p>
            <p>
                <hi rend=""italic"">Devanāgarī</hi> is an abugida script, which has been adopted as the writing system of several languages such as Hindī, Marāṭhī, Nepālī, and Sanskrit. Recently, digitizing Sanskrit texts written in 
                <hi rend=""italic"">Devanāgarī</hi> has been one of the most pressing and important tasks in the field of Sanskrit philology. Prosopographical Database for Indic Texts (PANDiT), Sanskrit Knowledge-System Project, and Göttingen Register of Electronic Text in Indian Languages (GRETIL) are some of the leading research projects.
            </p>
            <p> However, owing to the costs associated with time and human labor, building a database based on manually input text data is challenging. We have seen this in some preceding projects led by scholars in Germany, India, and Japan. We expect that the existing 
                <hi rend=""italic"">Devanāgarī</hi> OCR systems, developed often based on the contemporary Indic languages such as Hindī ([1][2]), may not accurately recognize the more complicated Sanskrit consonant clusters.
            </p>
            <p> In light of this situation, a team comprising Sanskrit language experts from the University of Tokyo and AI-OCR developers from Toppan Inc. have undertaken a cooperative research project. This project aims to develop a 
                <hi rend=""italic"">Devanāgarī</hi> OCR system and establish a Sanskrit text database automatically digitized by the OCR.
            </p>
            <p>In this poster presentation, we will</p>
            <list rend=""bulleted"">
                <item>Review the writing system of 
                    <hi rend=""italic"">Devanāgārī</hi> and describe how we correlate each combining letter with the Unicode encoding scheme. We took each letter as a composite of several elements. In this case, our experience of the Chinese character—a character consisting of multiple and irregularly ordered elements—served effectively. In this regard, we set a unit of letter called the “character shape.”
                </item>
                <item>Introduce and evaluate preceding and on-going OCR software such as the “Sanskrit OCR” run by ind.senz and “Google Document OCR.” According to our detailed analysis of these software programs, there are some specific cases where these OCRs frequently fail to recognize the letters. For example: some combined letters with the vowel sign 
                    “<hi rend=""italic"">i</hi> (<hi rend=""Deva"">ि</hi>),” where the sequence of letter elements (right to left, e.g. 
                    <hi rend=""italic"">k+i</hi>) goes against the stroke order (left to right, e. g. 
                    <hi rend=""Deva"">ि</hi>+<hi rend=""Deva"">क</hi>, 
                    <hi rend=""italic"">i+k</hi>); some irregularly typeset dots, which indicate the nasal sound 
                    (<hi rend=""italic"">anusvāra</hi>), and some lengthy consonant clusters such as 
                    (<hi rend=""Deva"">र्त्स्न्य</hi>, 
                    <hi rend=""italic"">rtsny-a</hi>). Focusing on these inadequacies that were insufficiently handled by the preceding studies, we show our design of an AI-OCR model, highlighting the uniqueness of this project.
                </item>
                <item>Expound the process of designing the “training data” through which an AI-OCR is generated. We obtained the training data from books included in Ānanda Āśrama Sanskrit Series, most of which were printed in metal typesetting. The strategy on how we define the “character shape” in the typesetting shall be explained in detail. An AI-OCR was generated through machine learning using the datasets prepared through the above process. Following is a brief overview of the outcomes obtained from the generated AI-OCR model.</item>
            </list>
            <p>Outcomes of Single Character Recognition (as of February 15, 2022)</p>
            <p> Out of the 2,434 sample letters:</p>
            <p> a. 2,340 letters exactly recognized* (Accuracy rate 96.14 %)</p>
            <p> b. 2,397 letters correctly listed** (Accuracy rate 98.48 %)</p>
            <p> * Only when each letter is listed as the first choice.</p>
            <p> ** Including cases where the correct letter is listed as a candidate.</p>
            <p>Based on the comparison, ""Google Document OCR"" and ""Sanskrit OCR"" showed an accuracy rate of 95.00 % and 89.92 %, respectively, on average for the common samples in this presentation. Once the factors affecting the accuracy are understood, we will outline the adjustments needed to improve the accuracy rate of the AI-OCR.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>Bansal, V and Sinha, M. (2001). A Complete OCR for Printed Hindi Text in Devanagari Script. 
                        <hi rend=""italic"">Proceedings of Sixth International Conference on Document Analysis and Recognition</hi>, pp. 800-804.
                    </bibl>
                    <bibl>Suryaprakash Kompalli, Sankalp Nayak, Srirangaraj Setlur and Venu Govindaraju (2005). Challenges in OCR of Devanagari documents. 
                        <hi rend=""italic"">Eighth International Conference on Document Analysis and Recognition (ICDAR'05),</hi> pp. 327-331.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,ai-ocr;devanāgarī;optical character recognition;sanskrit,English,asia;asian studies;contemporary;digital archiving;english;image processing and analysis
11900,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,Do we have to limit our research question by the tool to be used? The iLCM as an example of freely extensible research software for text-based research tasks in the humanities:,,Andreas Niekler;Christian Kahmann,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p>The barriers to our creativity are often the tools we have at our fingertips. This is particularly visible in text-oriented research tasks in the humanities, where a wide range of libraries and stand-alone software are available. The constraints of each tool and the researcher's skills in using the tools both shape and co-construct the research process. In our view, such influence should be minimized in the research process. The interactive Leipzig Corpus Miner (iLCM) [https://ilcm.informatik.uni-leipzig.de/] (Niekler et. al., 2018) represents a ready-to-use and GUI-supported software solution for the use of text mining in the humanities, cultural studies, and social sciences. The software is completely based on R [https://www.r-project.org/] and RShiny [https://shiny.rstudio.com/]. In parallel to the iLCM interface, an RStudio server [https://www.rstudio.com/] instance is provided as an IDE that ensures access to the available data and results. Among other things, the tool offers the pre-processing of multilingual documents, the retrieval and management of document collections, the deduplication of content, the analysis of word frequency, the analysis of word co-occurrence, time series analysis, topic models, the automatic coding and annotation of categories, supervised text classification (e.g. sentiment analysis) and more. In our poster, we demonstrate that its built-in ability to produce custom scripts, export results and script-based adaptations of the available analyses circumvents some restrictions of other tools used in humanities research. </p>
            <p>For researchers using text-oriented analysis methodologies, implementing their own analysis programs is often not a viable option. While more and more researchers in the humanities, cultural studies, and social sciences are acquiring programming skills, developing complex research software remains a complicated process that typically requires trained software developers. Instead, applied research relies on existing research software. There are many single, specific (i.e. for one purpose) ready-to-use tools, e.g. for topic modeling [e.g. https://dariah-de.github.io/TopicsExplorer/], concordance tools [e.g. https://voyant-tools.org/] or word scaling [e.g. http://www.wordfish.org]. This restricts researchers to narrow study designs that stay within the confines of the specific software which leads to a significant reduction in the method portfolio of a research project. If scholars want to map more complex (NLP) workflows it gets more difficult, because researchers often have to use full-blown frameworks or APIs [https://www.nltk.org/, https://spacy.io/, https://stanfordnlp.github.io/CoreNLP/, https://opennlp.apache.org/, https://quanteda.io/] which is a deterrent for many humanists. Digital humanities tools must therefore not only implement general best practices from the field of usability, but also need to address the special characteristics of users in the humanities (Burghardt and Wolff, 2014). There are isolated approaches of integrating the whole NLP-pipeline [https://weblicht.sfs.uni-tuebingen.de/, https://hudesktop.hucompute.org/, https://textgrid.de/]. Nevertheless, the limitations are that you can recombine the provided processes in the tools but not fundamentally extend them.</p>
            <p>We exemplify the benefits that result from the extensibility of the iLCM as follows:</p>
            <p>
                <hi rend=""bold"">Adaptability: </hi>Predefined analysis functions often require research-specific adaptations, both in preprocessing and in the analysis of text data. In the iLCM, a high degree of adaptability is provided by the ability to extensively parameterize each analysis step. Since internally each analysis method is implemented as a script written in the R programming language, it is possible to adapt the predefined methods directly. The edited processes can be reintegrated into the tool and are also available to the users of the graphical interface. In this way, iLCM can also be used to distribute tasks in interdisciplinary teams. A developer for the R components can adapt analyses for the humanities researchers and they use the processes via an easily accessible graphical interface.
            </p>
            <p>
                <hi rend=""bold"">Extensibility: </hi>If a needed function or method is not available in the iLCM, it should be possible to add these functions. In iLCM, new scripts can be created within the iLCM script editor to add new analysis functions or replace existing ones. Furthermore, it is possible to implement additional analyses based on intermediate results in an associated RStudio IDE.
            </p>
            <p>
                <hi rend=""bold"">Data export: </hi>If it is not possible or desired to fully implement a research design within the framework provided by the iLCM, it may still be possible to represent at least partial-processes using the tool. The result of these can then be exported and used in other software environments.
            </p>
            <p>With the iLCM we reflect the requirements of humanists for interactive, visual interfaces and standard tools. In response to frequently necessary adjustments in the creative processing of research tasks, we have also implemented the requirements of agile development in research processes. (Heyer, Kahmann and Kantner, 2019). With this Feature we contribute to the fact that the tool used does not limit the researchers and that the openest possible processing of text-oriented research tasks is possible.</p>
            <p>
                <figure>
                    <graphic url=""Pictures/59c42f9e6357db64c7bbfd9d2278e844.png""/>
                </figure>
            </p>
            <p>Figure 1: The view shows the interface of the iLCM in which an editing interface for the scripts has been integrated. The example of a co-occurrence analysis shows that the underlying standard process, which can also be used via the graphical user interface, is customizable. The edited process can then be integrated again with a custom process name and is also usable for all users of the graphical interface.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Burghardt, Manuel, & Wolff, Christian.</hi> (2014). Humanist-Computer Interaction: Herausforderungen für die Digital Humanities aus Perspektive der Medieninformatik. Universität Regensburg. 
                        <ref target=""https://doi.org/10.5283/EPUB.35716"">doi: 10.5283/EPUB.35716</ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Heyer, G., Kahmann, C. and Kantner, C.</hi> (2019) ‘Generic tools and individual research needs in the Digital Humanities - Can agile development help?’,
                        <hi rend=""italic"">INFORMATIK 2019: 50 Jahre Gesellschaft für Informatik - Informatik für Gesellschaft (Workshop-Beiträge)</hi>. Gesellschaft für Informatik e.V., pp. 175–180. doi: 10.18420/inf2019_ws19.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Niekler, A., Bleier, A., Kahmann, C., Posch, L., Wiedemann, G., Erdogan, K., Heyer, G., & Strohmaier, M.</hi> (2018). ILCM – A Virtual Research Infrastructure for Large-Scale Qualitative Data. 
                        <hi rend=""italic"">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).</hi> European Language Resources Association (ELRA).
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,nlp;research software;text;text mining;tool,English,20th century;computer science;contemporary;digital research infrastructures development and analysis;english;global;humanities computing;text mining and analysis
11916,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,Analysis of the Gutenberg 42-line Bible types aided by type-image recognition,,Mari Agata;Teru Agata,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <list rend=""bulleted"">
                <item>Background and purpose</item>
            </list>
            <p>While the traditional view of the type casting method used at the earliest stage of European printing asserts that identically shaped types were produced by metal punch, matrix, and hand mould, this has been under renewed debated over the past two decades. Reported in 2000, Paul Needham and Blaise Agüera y Arcas’s clustering analysis of images of the lowercase “i” of the Donatus Kalender type (DK type), that is, Johann Gutenberg’s first type, discovered hundreds of clusters of “i,” leading to the conclusion that “[e]ither many matrices were used in parallel, or equivalently, the matrix was temporary and needed to be re-formed between castings, – or both” (
                <hi rend=""fontstyle01"">Agüera y Arcas, 2003</hi>). Despite the considerable attention their research attracted, there have been relatively a few substantial follow-up studies. One of them is a clustering analysis of small samples – ten “i”s and 21 “a”s on a single page of the Gutenberg 42-line Bible (the B42) printed around 1455–, which corresponds to the conclusion of the DK type analysis (Alabert and Rangel, 2011
                <hi rend=""fontstyle01"">)</hi>.
            </p>
            <p> The authors have tried to contribute to this argument by analyzing the B42 types. The typeface is Gothic Textura and very similar to the DK type, but smaller in size. Nearly 300 types have been identified by earlier scholars, due to many variations of each letter. This paper is an interim report of the ongoing type analyses of the B42.</p>
            <list rend=""bulleted"">
                <item>Method</item>
            </list>
            <p>The digital images of the Keio University Library copy of the B42 (vol. 1 only) were used for analyses. Type-image recognition reinforced by machine learning was executed with an open-source OCR engine, Tessaract-OCR, followed by manual corrections. Each piece of type-image data had information about X and Y coordinates, pixel width and height, and transcribed character. Type-image recognition have been completed, 46 pages of which containing around 120,000 letters have been manually corrected and used for analyses.</p>
            <p> The first analysis is to calculate the vertical distance between a suspension stroke to show suspension of nasal “n,” “m,” and other letters and companion letter. The authors previously made a statistical analysis of a single letter “ū” that appeared on selected 42-line pages. The results showed that the variation was too wide for letters cast from a single matrix, thereby suggesting that they were made from multiple matrices (Agata and Agata, 2021). Several other types that appeared on 42-line pages as well as on 40-line pages, each with a suspension stroke, are newly analyzed. The 40-line pages had been printed at the very earliest stage of the print run, before the lines per page were increased to 42, thus enabling a chronological analysis.</p>
            <p>The second method is to analyze the horizontal position of suspension strokes in relation to a companion letter. The widths of suspension strokes and companion letters, and the distance of their horizontal centers are calculated based on the brightness of the pixels.</p>
            <p>The method of identifying inverted letters is also explored. The letters “n” and “u” are printed upside down in some places, thus an inverted “n” looks like “u”, and 
                <hi rend=""italic"">vice versa</hi>. The similarity of “u”s are measured by contour extraction, then the resultant similarity matrix is used for clustering analysis.
            </p>
            <list rend=""bulleted"">
                <item>Results</item>
            </list>
            <p> The results of vertical distance analysis show that the distribution of the distance between a suspension stroke and companion letter of the types used in 40-line pages is limited to a narrower range than that of 42-line pages. This may suggest that matrices were added during the print run.</p>
            <p>The horizontal position analysis indicates that the scatter diagram shows no correlation between the relative position of strokes and companion letters and that the strokes and letters are more likely to have been punched separately rather than together with whole-letter punches.</p>
            <p>The clustering analysis of “u”s shows the potential usefulness of the method together with several challenges.</p>
            <p>It should be noted that a relatively small number of types have been analyzed, but the analyses so far are promising. The present methods are not limited to the analysis of the B42, but are applicable to any early printed books. A culmination of examples will contribute to further discussions on type casting methods in European printing.</p>
            <list rend=""bulleted"">
                <item>
                    <anchor xml:id=""Hlk101472571""/>Acknowledgements
                </item>
            </list>
            <p>The B42 images taken by the HUMI Project were provided for research and reproduced by courtesy of the Keio University Library. This study was supported by JSPS KAKENHI Grant Number 18H03496.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Agata, M. Agata, T.</hi> (2021). Statistical analysis of the Gutenberg 42-line Bible types: special focus on letters with a suspension stroke. 
                        <hi rend=""italic"">The Papers of the Bibliographical Society of America</hi>, 115 (2): 167-83, 
                        <ref target=""https://doi.org/10.1086/713981"">
                            <hi rend=""color(337AB7)"">10.1086/713981</hi>
                        </ref>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Agüera y Arcas, B.</hi> (2003). Temporary matrices and elemental punches in Gutenberg’s DK type. In Jensen, K. (ed), 
                        <hi rend=""italic"">Incunabula and Their Readers: Printing, Selling and Using Books in the Fifteenth Century</hi>. London: British Library, pp. 1–12.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" xml:space=""preserve"">Alabert, A. and Rangel, L. M. (2011). </hi>Classifying the Typefaces of the Gutenberg 42-line Bible. International Journal on Document Analysis and Recognition, 14: 303–17, 
                        <ref target=""https://doi.org/10.1007/s10032-010-0140-6"">10.1007/s10032-010-0140-6</ref>.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,image recognition;ocr;printing history;the gutenberg bible,English,15th-17th century;book and print history;english;europe;image processing and analysis;optical character recognition and handwriting recognition
11932,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,"Hands-on Introduction to eScriptorium, an Open-Source Platform for HTR",,Peter Anthony Stokes;Daniel Stökl Ben Ezra,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p>
                The goal of this tutorial is to introduce participants to the principles and hands-on practice of the eScriptorium platform for the automatic and/or manual segmentation and transcription of manuscripts and printed books in a very wide range of languages, writing-systems and complex page-layouts.
            </p>
            <p>
                The tutorial will begin with a very brief overview of Handwritten Text Recognition (HTR) and its potential and remaining challenges, particularly when applied to so-called “rare” or historical scripts. HTR has long been a goal – indeed dream – of many in the Digital Humanities and beyond, and this is now becoming realised and increasingly accessible. Relatively general models for automatic transcription can now often achieve character error rates (CER) of less than 1% even for manuscript material, while a CER of 10% or even 20% is sufficient for some types of analysis such as text identification or text-image alignment. Very large quantities of manuscript material are also becoming available, particularly with the increasingly widespread use of IIIF. Machine learning is also relatively accessible, including computers with GPUs that are sufficiently powerful to treat thousands or even tens of thousands of images of manuscript pages within a reasonable timeframe. This combination is enabling new possibilities that were not feasible only a few years ago. However, most of the existing systems were designed at least initially in European (primarily Anglophone) contexts, and the differences between these and other writing contexts may seem subtle but very often make software unusable in practice.
            </p>
            <p>
                eScriptorium is Free Open-Source Software
                <note place=""foot"" xml:id=""ftn1"" n=""1"">
                    <p rend=""footnote text""> The source code is released under an MIT licence at 
                        <ref target=""https://gitlab.com/scripta/escriptorium/"">
                            <hi rend=""color(1155CC)"">https://gitlab.com/scripta/escriptorium/</hi>
                        </ref>.
                    </p>
                </note>
                <hi style=""font-size:11pt"" xml:space=""preserve""> designed to facilitate transcription of manuscripts in a wide variety of languages, scripts and complex layouts. The software began as part of the Scripta-PSL project which incorporated experts in dozens of scripts and languages, including Sumerian, Ugaritic, Syriac, Arabic, Hebrew, Classical and Pre-Imperial Chinese, Old and Medieval Japanese, Tibetan, Devanagari, Old Khmer, Pali, Tamil, and many more (see further </hi>
                <ref target=""https://scripta.psl.eu/langues/"">
                    <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://scripta.psl.eu/langues/</hi>
                </ref>
                ). It is designed to interact with kraken, another Free Open-Source Software developed by Benjamin Kiessling (of the same institution as the eScriptorium team).
                <note place=""foot"" xml:id=""ftn2"" n=""2"">
                    <p rend=""footnote text""> The source code is released under an Apache 2.0 licence at 
                        <ref target=""http://github.com/mittagessen/kraken"">
                            <hi rend=""color(1155CC)"">http://github.com/mittagessen/kraken</hi>
                        </ref>.
                    </p>
                </note>
                <hi style=""font-size:11pt"" xml:space=""preserve""> Kraken provides a flexible, modular HTR engine that can be run on its own or as the engine behind eScriptorium.</hi>
            </p>
            <p> </p>
            <figure>
                <graphic n=""1001"" width=""16.002cm"" height=""9.241013888888888cm"" url=""Pictures/d05da62527573fb05a90fbc943762137.jpg"" rend=""inline""/>
            </figure>
            <p style=""text-align: center;"">
                <hi rend=""italic"" style=""font-size:11pt"">Transcription (transliteration) of Old Javanese palm-leaf books with eScriptorium (courtesy of Marine Schoettel, École Française d’Extrême-Orient)</hi>
            </p>
            <figure>
                <graphic n=""1002"" width=""16.002cm"" height=""9.001125cm"" url=""Pictures/c9c40c6681cf44dad1445b042343079f.png"" rend=""inline""/>
            </figure>
            <p style=""text-align: center;"">
                <hi rend=""italic"" style=""font-size:11pt"">The interface showing a complex page layout: page image with lines and regions (left panel); transcription aligned to the page layout (centre panel); transcription in ‘regular’ lines (right panel).</hi>
            </p>
            <p>
                Participants will be introduced to eScriptorium and kraken, focussing particularly on the key design decisions that distinguish them from other HTR systems. One is that the system should be as independent as possible of any assumptions about language or script. For instance, writing can be left-to-right, right-to-left, top-to-bottom then right to left (as in Japanese), or top to bottom then left to right (as in Mongolian). The writing can be on a baseline (as in English), from a top-line (as in Hebrew) or in a column (as in Chinese), and the lines can be oriented at any angle, including upside-down and therefore ‘reversed’ from the normal direction relative to the page image (so upside-down Arabic reads from left to right relative to the page orientation, and so on). Participants are therefore strongly encouraged to bring their own materials in different scripts and languages, and will learn the possibilities and limits of eScriptorium and kraken for these different cases.
            </p>
            <p>
                After an introductory session on the principles of HTR and of eScriptorium and kraken in particular, we will go through the different stages of the workflow, always with attention to the challenges of non-European scripts. This involves first, segmentation of pages into regions and lines; second, potentially modifying the order of lines on the page if it has not been correctly identified by the system; and, third, performing the transcription. The first and third of these are currently based on machine learning (Kiessling 2020), and so they both normally involve a process of creating ground truth data for a sample of content, training a model on the basis of this data (most likely on top of an existing one already trained on similar content), then applying this model to new images. This process in turn raises methodological questions, such as which categories of region and line types to use, which standards of transcription to use, how best to share corpora for ground truth, how to manage large character sets or abbreviations, and so on. Participants will be directed to existing work and initiatives on this topic such as SegmOnto, HTR for All, and OCR-D.
            </p>
            <figure>
                <graphic n=""1003"" width=""16.002cm"" height=""10.449277777777779cm"" url=""Pictures/171322a13e9e2a456d6a71e8e572df97.png"" rend=""inline""/>
            </figure>
            <p style=""text-align: center;"">
                <hi rend=""italic"" style=""font-size:11pt"">Uncorrected results of automatic line detection in a previously unseen scroll (writing in Chinese from top to bottom then right to left).</hi>
            </p>
            <figure>
                <graphic n=""1004"" width=""11.712222222222222cm"" height=""8.113888888888889cm"" url=""Pictures/d5c4e77d6bd63ee2a2025501b10aedf1.jpg"" rend=""inline""/>
            </figure>
            <p style=""text-align: center;"">
                <hi rend=""italic"" style=""font-size:11pt"">Manually transcribing Chinese (top to bottom then right to left)</hi>
            </p>
            <figure>
                <graphic n=""1005"" width=""16.002cm"" height=""9.371541666666667cm"" url=""Pictures/c495a2f5259506f57b52691838359754.png"" rend=""inline""/>
            </figure>
            <p style=""text-align: center;"">
                <hi rend=""italic"" style=""font-size:11pt"">The eScriptorium interface showing lines showing mixed Hebrew (right to left) and Chinese (top to bottom or right to left). Numbers indicate the order of reading (detected automatically but correctable manually).</hi>
            </p>
            <p>
                eScriptorium and kraken are also designed to be as open as possible, including that data should be interchangeable and that users should not be locked into any one instance of the framework. Participants will therefore learn how to import and export images, page layout data and transcriptions in a variety of standard formats (plain text, ALTO, PAGE XML; import from IIIF manifests; transformation of export to TEI). We will also briefly discuss the strengths and limitations of these formats, particularly for the wide range of different scripts.
            </p>
            <p>
                As well as importing and exporting data, users of eScriptorium and kraken are also able (and actively encouraged) to import and export trained models for layout analysis and transcription, including not only sharing with other users in the same instance of eScriptorium but also publishing models on external repositories. A Zenodo community has been established to help share models, and publishing to and from Zenodo is already implemented as part of kraken and will be shortly in eScriptorium. In this way, models can be reused across different instances of the software, and no user is locked into any single instance. This has clear advantages in reducing the human effort in repeating training and ground-truth generation, and also provides a small but not insignificant step towards reducing the energy and environmental impact of machine learning, in that it reduces the need to pointlessly retrain identical models with the same ground truth. This is in contrast to many other systems of machine learning, where some or all of the software may be Open Source, and where the ground truth can sometimes be exported, but the models themselves are locked into a given instance of the system and cannot be exported, and/or the software is not sufficiently open that an entirely new instance can be set up independently.
            </p>
            <figure>
                <graphic n=""1006"" width=""16.002cm"" height=""5.886097222222222cm"" url=""Pictures/b523cdb2875cc0d0f3731286b83a2237.png"" rend=""inline""/>
            </figure>
            <p style=""text-align: center;"">
                <hi rend=""italic"" style=""font-size:11pt"">Model management in eScriptorium, including export, import, sharing and basic version control. Models shown are for manuscripts written in Hebrew</hi>
            </p>
            <p>
                eScriptorium also provides a web API to allow for automated execution of project-specific processes, and so a brief introduction to this will also be provided, depending on time and the interest of participants.
            </p>
            <p>
                Finally, eScriptorium and kraken are both freely available for anyone to download and install on their own servers, and a number of groups in different countries have already done so to our knowledge. In practice, eScriptorium can run even on a home computer for most tasks, but training on a large corpus requires relatively powerful computers and servers, and so the last part of the tutorial will be spent discussing practical questions about how to get an instance up and running, how to plan a future project with the software, and any other issues that come up during the session.
            </p>
            <p>Acknowledgements</p>
            <p>
                This work was supported by grants from the European Union (RESILIENCE RI and Vietnamica), Université Paris Sciences et Lettres (Scripta-PSL), the Mellon Foundation (OpenITI), the French Ministry of Higher Education and Research, the French Ministry of Culture (LectauRep) and the Domaine d'intérêt majeur STCN (ManuscriptologIA).
            </p>
            <p>
                For a full list of contributors to eScriptorium, see the GitLab repository and wiki (links below).
            </p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Chagué, A.</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve""> [no date]. Prendre en main eScriptorium. </hi>
                        <ref target=""https://lectaurep.hypotheses.org/documentation/prendre-en-main-escriptorium"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://lectaurep.hypotheses.org/documentation/prendre-en-main-escriptorium</hi>
                        </ref>
                        . English version (transl. Jonathan Allen),
                        <ref target=""https://lectaurep.hypotheses.org/documentation/escriptorium-tutorial-en"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://lectaurep.hypotheses.org/documentation/escriptorium-tutorial-en</hi>
                        </ref>
                         
                    </bibl>
                    <bibl>
                        <hi rend=""italic"" style=""font-size:11pt"">eScriptorium</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">[source code]: </hi>
                        <ref target=""https://gitlab.com/scripta/escriptorium/"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://gitlab.com/scripta/escriptorium/</hi>
                        </ref>
                         
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Gabay</hi>
                        <hi rend=""bold color(333333)"" style=""font-size:11pt"">, S., Camps, J.-B., Pinche, A., and Jahan C.</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve""> (2021). SegmOnto: common vocabulary and practices for analysing the layout of manuscripts (and more). </hi>
                        <hi rend=""italic color(333333)"" style=""font-size:11pt"">16th International Conference on Document Analysis and Recognition (ICDAR 2021).</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""italic"" style=""font-size:11pt"">HTR United</hi>
                        <ref target=""https://github.com/HTR-United"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://github.com/HTR-United</hi>
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Kiessling</hi>
                        <hi rend=""bold color(333333)"" style=""font-size:11pt"" xml:space=""preserve"">, B. </hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve"">(2020). A Modular Region and Text Line Layout Analysis System. </hi>
                        <hi rend=""italic color(333333)"" style=""font-size:11pt"">17th International Conference on Frontiers in Handwriting Recognition (ICFHR)</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"">.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Kiessling, B.</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve""> (2022). </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Kraken</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve""> [source code] </hi>
                        <ref target=""http://github.com/mittagessen/kraken"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">http://github.com/mittagessen/kraken</hi>
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Kiessling, B.</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve""> (2022). </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Kraken</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve""> [project website]. </hi>
                        <ref target=""http://kraken.re/"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">http://kraken.re/</hi>
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Kiessling</hi>
                        <hi rend=""bold color(333333)"" style=""font-size:11pt"">, B., Stökl Ben Ezra, D., Miller M.</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve""> (2019)</hi>
                        <ref target=""https://arxiv.org/abs/1907.04041"" xml:space=""preserve""> </ref>
                        BADAM: A Public Dataset for Baseline Detection in Arabic-script Manuscript
                        <ref target=""https://arxiv.org/abs/1907.04041"">
                            <hi rend=""color(333333)"" style=""font-size:11pt"">s</hi>
                        </ref>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve"">, </hi>
                        <hi rend=""italic color(333333)"" style=""font-size:11pt"">HIP@ICDAR</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve""> 2019. </hi>
                        <ref target=""https://arxiv.org/abs/1907.04041"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://arxiv.org/abs/1907.04041</hi>
                        </ref>
                        <hi rend=""color(333333)"" style=""font-size:11pt""> </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Kiessling</hi>
                        <hi rend=""bold color(333333)"" style=""font-size:11pt"">, B.; Tissot, R., Stökl Ben Ezra, D., Stokes P.</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve""> (2019) eScriptorium: An Open Source Platform for Historical Document Analysis, </hi>
                        <hi rend=""italic color(333333)"" style=""font-size:11pt"">OST@ICDAR 2019</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve"">. doi: </hi>
                        <ref target=""https://dx.doi.org/10.1109/ICDARW.2019.10032"">
                            10.1109/ICDARW.2019.10032
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Kiessling</hi>
                        <hi rend=""bold color(333333)"" style=""font-size:11pt"">, B.</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve""> (2019) Kraken – A Universal Text Recognizer for the Humanities. </hi>
                        <hi rend=""italic color(333333)"" style=""font-size:11pt"">Digital Humanities Book of Abstracts</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"">. doi:</hi>
                        <ref target=""https://doi.org/10.34894/Z9G2EX"">
                            10.34894/Z9G2EX
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""italic"" style=""font-size:11pt"">OCR-D: DFG-Funded Initiative for Optical Character Recognition Development</hi>
                        .
                        <ref target=""https://ocr-d.de/"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://ocr-d.de</hi>
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""italic"" style=""font-size:11pt"">OCR/HTR Model Repository</hi>
                        .
                        <ref target=""https://www.zenodo.org/communities/ocr_models"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://www.zenodo.org/communities/ocr_models</hi>
                        </ref>
                         
                    </bibl>
                    <bibl>
                        <hi rend=""italic"" style=""font-size:11pt"">SegmOnto</hi>
                        .
                        <ref target=""https://github.com/SegmOnto"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://github.com/SegmOnto</hi>
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Stokes, P.A.</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve""> (2020). RESILIENCE tool: eScriptorium. </hi>
                        <ref target=""https://www.resilience-ri.eu/blog/resilience-tool-escriptorium/"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://www.resilience-ri.eu/blog/resilience-tool-escriptorium/</hi>
                        </ref>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Version française : eScriptorium : un outil pour la transcription automatique des documents. </hi>
                        <ref target=""https://ephenum.hypotheses.org/1412"">
                            https://ephenum.hypotheses.org/1412
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Stokes</hi>
                        <hi rend=""bold color(333333)"" style=""font-size:11pt"">, P., Kiessling, B., Tissot, R., Stökl Ben Ezra, D.</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve""> (2019) EScripta: A New Digital Platform for the Study of Historical Texts and Writing, </hi>
                        <hi rend=""italic color(333333)"" style=""font-size:11pt"">Digital Humanities 2019 Book of Abstracts</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"">. doi:</hi>
                        <ref target=""https://doi.org/10.34894/BIXSWX"">
                            10.34894/BIXSWX
                        </ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-size:11pt"">Stokes</hi>
                        <hi rend=""bold color(333333)"" style=""font-size:11pt"">, P.A., Kiessling, B., Stökl Ben Ezra, D., Tissot, R. and Gargem, H.</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve""> (2021). The eScriptorium VRE for Manuscript Cultures. </hi>
                        <hi rend=""italic color(333333)"" style=""font-size:11pt"">Ancient Manuscripts and Virtual Research Environments</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve"">, ed. Claire Clivaz and Garrick V. Allen. Special issue of </hi>
                        <hi rend=""italic color(333333)"" style=""font-size:11pt"">Classics@</hi>
                        <hi rend=""color(333333)"" style=""font-size:11pt"" xml:space=""preserve""> 18. </hi>
                        <ref target=""https://classics-at.chs.harvard.edu/the-escriptorium-vre-for-manuscript-cultures/"">
                            <hi rend=""color(1155CC)"" style=""font-size:11pt"">https://classics-at.chs.harvard.edu/the-escriptorium-vre-for-manuscript-cultures/</hi>
                        </ref>
                    </bibl>
                    <bibl>
                        All URLs last verified 21 April 2022.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,htr;ocr;open data;tools,English,15th-17th century;18th century;5th-14th century;artificial intelligence and machine learning;book and print history;english;global;history;optical character recognition and handwriting recognition
11983,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,Building an OCR Pipeline for a Republican Chinese Entertainment Newspaper:,,Konstantin Henke;Matthias Arnold,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p>
                <anchor xml:id=""id_docs-internal-guid-e0290427-7fff-e493-e32c-95350b2da4ba""/>
                In recent years, the digitisation of newspapers has made a lot of progress, and large national and international initiatives like Trove<note xml:id=""ftn0"" place=""foot"" n=""1"">
                        <anchor xml:id=""id_docs-internal-guid-3acdbfd7-7fff-0323-4436-a7b63f044029""/> https://trove.nla.gov.au/newspaper/
                    </note>, Chronicling America<note xml:id=""ftn1"" place=""foot"" n=""2"">
                        <anchor xml:id=""id_docs-internal-guid-644e1fd9-7fff-40cb-23b5-1a19ae1dc6ed""/> https://chroniclingamerica.loc.gov/newspapers/
                    </note>, Europeana Newspapers<note xml:id=""ftn2"" place=""foot"" n=""3"">
                        <anchor xml:id=""id_docs-internal-guid-f599eda0-7fff-8309-ee28-ea7a5c1e8ccb""/> http://www.europeana-newspapers.eu/
                    </note>, Impresso<note xml:id=""ftn3"" place=""foot"" n=""4"">
                        <anchor xml:id=""id_docs-internal-guid-b4ba9880-7fff-d09f-70d2-79c66eac3f2f""/> https://impresso-project.ch/
                    </note>, NewsEye<note xml:id=""ftn4"" place=""foot"" n=""5"">
                        <anchor xml:id=""id_docs-internal-guid-ff07e3a6-7fff-0276-5464-28d401e5a4e2""/> https://www.newseye.eu/
                    </note>, Oceanic Exchanges<note xml:id=""ftn5"" place=""foot"" n=""6"">
                        <anchor xml:id=""id_docs-internal-guid-08248c4e-7fff-30ef-091e-296b471b6cfa""/> https://oceanicexchanges.org/
                    </note>, OCR-D<note xml:id=""ftn6"" place=""foot"" n=""7"">
                        <anchor xml:id=""id_docs-internal-guid-779bcbf0-7fff-751e-ad1c-2b68b4baf9a1""/> https://ocr-d.de/en/
                    </note>, Deutsches Zeitungsportal<note xml:id=""ftn7"" place=""foot"" n=""8"">
                        <anchor xml:id=""id_docs-internal-guid-cb953ccd-7fff-af71-0c81-e6358a1a67ac""/> https://www.deutsche-digitale-bibliothek.de/newspaper
                    </note>, 
                and Living with Machines<note xml:id=""ftn8"" place=""foot"" n=""9"">
                        <anchor xml:id=""id_docs-internal-guid-fafb278d-7fff-ea09-b19a-6ce4928e7f59""/> https://livingwithmachines.ac.uk/
                    </note> emerged that are building up on and going beyond sheer digitisation, venturing into various areas of content analysis (Oberbichler et al, 2021). Also, the outcomes of these initiatives are usually provided online with open access, and publications increasingly follow the FAIR principles (Wilkinson et al, 2016). However, most of the textual content covered is printed in Latin script languages, and to a large degree the analytical systems rely on linguistic features like word boundaries, digital lexica, or tagged corpora.
            </p>
            <p>
                Responding to this from an Asian perspective, i.e. looking at materials from regions where non-Latin scripts prevail, the situation is different. In our case we are working with newspapers from Republican China. Although there are some projects working on historical Chinese newspapers (Stewart et al, 2020), results have so far rarely been published. Other initiatives provide their final results as commercial products. In general, a certain reluctance can be observed when it comes to publishing research methodologies, not to mention the open access sharing of ground truth, test corpora, or trained models (Arnold et al, forthcoming).
            </p>
            <p>
                In our project we collected periodicals from the Republican era as image scans (Sung et al, 2014
                ) 
                and started OCR experiments to transform them into machine readable full text.
            </p>
            <p>
                <figure>
                    <graphic url=""Pictures/ee25818bb70b244c1d7cc83576a4cd28.jpg""/>
                </figure>
            </p>
            <p>
                <hi rend=""color(#000000) italic"">Fig. 1: One of the 9385 fold scans of </hi>
                Jing bao
            </p>
            <p>
                Looking closer at 
                <hi rend=""color(#000000) italic"">Jing bao </hi>
                <hi rend=""Chinese"">晶報</hi>
                 
                <hi rend=""color(#000000) italic"">(The Crystal)</hi>
                 (cf. Fig. 1), an entertainment newspaper that ran from 1919 to 1940, we soon learned that the key issue of OCR’ing the material actually lies in the page-level segmentation. We therefore started creating ground truth (GT) for geometrical data featuring semantically grouped bounding boxes with labels (article, image, advertisement, marginalia). We then used the resulting dataset to train dhSegment
                 and have the network detect content areas on the folds (Arnold, forthcoming) (Fig. 2).
            </p>
            <p>
                <figure>
                    <graphic url=""Pictures/b50eec755eb52b2afe34ae74f1604d29.png""/>
                </figure>
            </p>
            <p>Fig. 2: Automatic page segmentation results. Blocks with text content are shown in yellow.</p>
            <p>
                Additionally, we created a text GT that not only covers all text in a machine readable local XML format, but also contains information about reading sequence running direction of the text. Based on this GT we were able to process a first set of manual crops, introducing a character segmentation method for grid-based printing layouts which produces over 90,000 labeled images of single characters (Henke, 2021). In this work, a GoogLeNet is trained as an OCR classifier on said character images after extensive pre-training on synthetical character image data created from font files. Additional error correction using language models yields an accuracy of 97.44%.
            </p>
            <p>
                In our presentation we introduce our work on developing a document image processing pipeline currently focusing on Republican Chinese newspapers with complex layouts like the 
                <hi rend=""color(#000000) italic"">Jing bao</hi>. We will present the following concrete contributions:
            </p>
            <list type=""ordered"">
                <item>A page-level segmentation approach (as seen in Fig. 2) yielding single text blocks.</item>
                <item>An OCR pipeline taking single text blocks as input.</item>
            </list>
            <p>While Arnold (forthcoming) presented first promising experiments regarding (1), in this presentation we will concentrate on (2). Our evaluation metric for OCR output is the character error rate (CER) with regard to the ground truth annotation of every text block crop, which, based on the Levenshtein distance, is computed by:</p>
            <p>
                <figure rend=""center"">
                    <graphic url=""Pictures/59fbbe7bc21d630585cad5745f6aefaa.png"" width=""1.5in""/>
                </figure>
            </p>
            <p>(<hi rend=""color(#000000) italic"">S</hi>, 
                <hi rend=""color(#000000) italic"">D</hi>, 
                <hi rend=""color(#000000) italic"">I</hi>
                 
                = number of substitutions, deletions, insertions; L = length of the reference sequence, i.e. corresponding GT text).
            </p>
            <p>
                The character segmentation approach presented in Henke (2021) can however only process text blocks where characters are printed in a grid-like layout, which accounts for a very small portion of the 
                <hi rend=""color(#000000) italic"">Jing bao</hi>. Hence, there is a particular need for efficient character detection in less stable layout situations within text blocks, before passing single character images on to the actual OCR engine. As a baseline, we leverage the publicly available state-of-the-art OCR tool Tesseract (Smith, 2007) which provides out-of-the-box segmentation+recognition models even for vertically printed traditional Chinese. Tesseract however seems to struggle with the low input image resolution (~ 25x25 px per character) and overall inconsistent scan quality, leading to a very high CER of 
                <hi rend=""color(#000000) bold"">47.85%</hi>
                 on the test set from Henke (2021).
            </p>
            <p>
                To solve this issue, we use the readily-trained HRCenterNet from 
                Tang et al. (2020) 
                for character detection, and crop the bounding boxes to feed them into the GoogLeNet trained in Henke (2021).
                 However, while our crops have a great variety of aspect ratios, the HRCenterNet expects at least nearly-squared rectangles. Hence, we cut the original images into 250x250 px tiles with a 50 px overlap (both horizontally and vertically, Fig. 3c). Bounding boxes (Fig. 3d) found in the overlapping sections are filtered during the non-maximum suppression (NMS) operation already included in the HRCenterNet pipeline (Fig. 3e).
            </p>
            <table rend=""frame"" xml:id=""Table1"">
            <note type=""direction"">
                    <width unit=""pt"">12</width>
                    <width unit=""pt"">231</width>
               <!-- 3.375in - 12pt -->
                </note>
            <row>
                    <cell rend=""end color(#000000)"">a </cell>
                    <cell rend=""color(#000000)"">
                        <figure>
                            <graphic url=""Pictures/f431abbd0582765c87d36f5ee79ab461.png"" height=""1in""/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend=""end color(#000000)"">b </cell>
                    <cell rend=""color(#000000)"">
                        <figure>
                            <graphic url=""Pictures/2bcd7873638f561912eb93d4a35356ff.png"" height=""1in""/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend=""end color(#000000)"">c </cell>
                    <cell rend=""color(#000000)"">
                        <figure>
                            <graphic url=""Pictures/c86e0fdabcae3b2dad97ed5369366be4.png"" height=""1in""/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend=""end color(#000000)"">d </cell>
                    <cell rend=""color(#000000)"">
                        <figure>
                            <graphic url=""Pictures/f088e4707a71f7c12f0a0c177e3c9d67.png"" height=""1in""/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend=""end color(#000000)"">e </cell>
                    <cell rend=""color(#000000)"">
                        <figure>
                            <graphic url=""Pictures/0a5d82f98d6b62cd7f84566ac555f366.png"" height=""1in""/>
                        </figure>
                    </cell>
                </row>
            </table>
            <p>Fig 3: a) original image, b) image after contrast enhancing, c) tiling with overlap, d) bounding boxes found by HRCenterNet before NMS, e) final result after reconnection of tiles and NMS</p>
            <p>In addition, Fig. 4 shows how the HRCenterNet largely profits from contrast-enhancement (Fig 3b) during image pre-processing, especially for low-contrast input images.</p>
            <table rend=""frame"" xml:id=""Table2"">
                <row>
                    <cell rend=""center"">
                        <figure>
                            <graphic url=""Pictures/f51c737e55e4fad308332f9bd8181587.png"" width=""1.5in""/>
                        </figure>
                    </cell>
                    <cell rend=""center"">
                        <figure>
                            <graphic url=""Pictures/7e200243353b770fe0cd8dd0d1131e65.png"" width=""1.5in""/>
                        </figure>
                    </cell>
                </row>
            </table>
            <p>
                <hi rend=""color(#000000) italic"">Fig 4: Effects of contrast enhancement on character detection using HRCenterNet</hi>
            </p>
            <p>
                Using the above method, the CER on the test set of Henke (2021) is reduced to 
                <hi rend=""color(#000000) bold"">5.64%</hi>.
            </p>
            <p>
                In the presentation we will show how the results can be confirmed on a non-grid-based section of the corpus, for which we currently create GT annotations. We are confident that the additional pre-processing of crops and individual character images will help to further reduce the CER, and in combination with (1), yield a powerful document-level OCR pipeline for the 
                <hi rend=""color(#000000) italic"">Jing bao</hi>
                 and similar Republican newspapers. This will not only open the door to further processing with the tools of Digital Humanities, but also further contribute to FAIR-based work in the diverse Asian sphere.
            </p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Arnold, M. </hi>(2021). Multilingual Research Projects: Challenges for Making Use of Standards, Authority Files, and Character Recognition. Digital Studies/Le Champ Numérique, forthcoming. DOI: 10.11588/heidok.00030918 (preprint).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Arnold, M., Paterson, D. and Xie, J. </hi>(forthcoming). Procedural Challenges: Machine Learning Tasks for OCR of Historical CJK Newspapers. International Journal of Digital Humanities, Special issue on Digital Humanities and East Asian Studies. (manuscript accepted by special issue editors, currently under review by journal).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Henke, K.</hi> (2021). Building and Improving an OCR Classifier for Republican Chinese Newspaper Text. B.A. thesis, Heidelberg University. DOI: 10.11588/heidok.00030845
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Oberbichler, S., Boros, E., Doucet, A., Marjanen, J., Pfanzelter, E., Rautiainen, J., Toivonen, H. and Tolonen, M.</hi> (2021). Integrated Interdisciplinary Workflows for Research on Historical Newspapers: Perspectives from Humanities Scholars, Computer Scientists, and Librarians. Journal of the Association for Information Science and Technology. DOI: 10.1002/asi.24565
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Smith, Ray</hi> (2007). An Overview of the Tesseract OCR Engine. Ninth International Conference on Document Analysis and Recognition (ICDAR 2007), pp. 629-633. DOI: 10.1109/ICDAR.2007.4376991
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Stewart, S., <hi rend=""Chinese"">朱吟清</hi> Zhu, Y., <hi rend=""Chinese"">吴佩臻</hi> Wu, P., <hi rend=""Chinese"">赵薇</hi> Zhao, W., Gladstone, C., Long, H., Detwyler, A. and So, R. J. </hi>(2020). <hi rend=""Chinese"">比较文学研究与数字基础设施建设：以“民国时期期刊语料库</hi>(1918-1949),<hi rend=""Chinese"">基于</hi>PhiloLogic4<hi rend=""Chinese"">”为例的探索</hi> (Comparative Literature Research and Digital Infrastructure: Taking the ‘Republican China Periodical Corpus (1918-1949), Based on PhiloLogic 4’ as an Example). <hi rend=""Chinese"">数字人文</hi> Digital Humanities, no. 1: 175–82. online version
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sung, D., Sun, L. and Arnold, M.</hi> (2014). The Birth of a Database of Historical Periodicals: Chinese Women’s Magazines in the Late Qing and Early Republican Period. TSWL 33, no. 2: 227–37. URL: https://www.jstor.org/stable/43653333
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Tang, C., Liu, C. and Chiu, P. </hi>(2020). HRCenterNet: An Anchorless Approach to Chinese Character Segmentation in Historical Documents. IEEE International Conference on Big Data (Big Data). DOI: 10.1109/BigData50022.2020.9378051
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Wilkinson, M. D., Dumontier, M., Aalbersberg, IJ. J., Appleton, G., Axton, M., Baak, A., Blomberg, N. et al.</hi> (2016). The FAIR Guiding Principles for Scientific Data Management and Stewardship. no. 1. Scientific Data 3, no. 1: 160018. DOI: 10.1038/sdata.2016.18
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,chinese character recognition;chinese character segmentation;historical chinese newspapers;ocr;republican period,English,20th century;artificial intelligence and machine learning;asia;asian studies;english;optical character recognition and handwriting recognition
12123,2022 - Potsdam,Potsdam,Kulturen des digitalen Gedächtnisses,2022,DHd,DHd,Fachhochschule Potsdam (FHP / University of Applied Sciences Potsdam);University of Potsdam,Potsdam,,Germany,https://www.dhd2022.de/,Volltexterkennung für historische Sammlungen mit OCR4all-libraries iterativ und partizipativ gestalten,,Jan Sebastian Klaes;Kristof Korwisi;Katharina Krüger;Christian Reul;Nadine Towara,paper,,xml,,,massendigitalisierung;ocr;ocr4all;sammlung;volltexterkennung,German,daten;sammlung;software;text;umwandlung
12138,2022 - Potsdam,Potsdam,Kulturen des digitalen Gedächtnisses,2022,DHd,DHd,Fachhochschule Potsdam (FHP / University of Applied Sciences Potsdam);University of Potsdam,Potsdam,,Germany,https://www.dhd2022.de/,"Dokument, Transkription, Forschungsdatum. Technische und kulturelle Überlegungen für interdisziplinäre Transkriptionspraxis",,Konstantin Baierer;Matthias Boenig;Elisabeth Engl;Mareen Geestmann;Lena Hinrichsen;Clemens Neudecker;Paul Pestov;Michelle Weidling,paper,,xml,,,forschungsdaten;ground truth;handschrifterkennung;htr;künstliche neuronale netze;metadaten;neuronale netze;ocr;ocr4all;ocr-d;page-xml;standards;tei-xml;texterkennung;transkribus;transkription;unicode;utf-8,German,benannte entitäten (named entities);kollaboration;standards;teilen;text;transkription
12167,2022 - Potsdam,Potsdam,Kulturen des digitalen Gedächtnisses,2022,DHd,DHd,Fachhochschule Potsdam (FHP / University of Applied Sciences Potsdam);University of Potsdam,Potsdam,,Germany,https://www.dhd2022.de/,Poesie als Fehler. Ein ‘Tool Misuse’-Experiment zur Prozessierung von Lyrik,,Henny Sluyter-Gäthje;Peer Trilcke,paper,,xml,,,fehler;lyrik;nlp,German,annotieren;bereinigung;literatur;programmierung;stilistische analyse;text
12172,2022 - Potsdam,Potsdam,Kulturen des digitalen Gedächtnisses,2022,DHd,DHd,Fachhochschule Potsdam (FHP / University of Applied Sciences Potsdam);University of Potsdam,Potsdam,,Germany,https://www.dhd2022.de/,Adnominale Possession in einem Bibel-Parallelkorpus,,Florian Fleischmann,paper,,xml,,,adnominale possession;ocr;parallelkorpus,German,
12174,2022 - Potsdam,Potsdam,Kulturen des digitalen Gedächtnisses,2022,DHd,DHd,Fachhochschule Potsdam (FHP / University of Applied Sciences Potsdam);University of Potsdam,Potsdam,,Germany,https://www.dhd2022.de/,Handwritten Text Recognition und Word Mover’s Distance als Grundlagen der digitalen Edition “Die Kindheit Jesu Konrads von Fußesbrunnen”,,Stefan Tomasek;Christian Reul;Maximilian Wehner,paper,,xml,,,digitale edition;htr;ocr,German,datenerkennung;stilistische analyse;strukturanalyse;transkription;visualisierung
12187,2022 - Potsdam,Potsdam,Kulturen des digitalen Gedächtnisses,2022,DHd,DHd,Fachhochschule Potsdam (FHP / University of Applied Sciences Potsdam);University of Potsdam,Potsdam,,Germany,https://www.dhd2022.de/,Building and Improving an OCR Classifier for Republican Chinese Newspaper Text,,Matthias Arnold;Konstantin Henke,paper,,xml,,,character segmentation;chinese text ocr;ocr error correction;synthetic training data,English,datenerkennung;forschung;methoden;programmierung;projekte;text
12200,2022 - Potsdam,Potsdam,Kulturen des digitalen Gedächtnisses,2022,DHd,DHd,Fachhochschule Potsdam (FHP / University of Applied Sciences Potsdam);University of Potsdam,Potsdam,,Germany,https://www.dhd2022.de/,Flexibles Arbeiten mit OCR4all. Massenvolltextdigitalisierung von Drucken mithilfe von OCR-D und hochqualitative Transkription von Handschriften,,Florian Langhanki;Maximilian Wehner;Konstantin Baierer;Lena Hinrichsen;Christian Reul,paper,,xml,,,digitalisierung;ocr/htr;transkription,German,annotieren;literatur;manuskript;metadaten;schreiben;transkription
12206,2022 - Potsdam,Potsdam,Kulturen des digitalen Gedächtnisses,2022,DHd,DHd,Fachhochschule Potsdam (FHP / University of Applied Sciences Potsdam);University of Potsdam,Potsdam,,Germany,https://www.dhd2022.de/,"Grenzüberschreitendes Textmining von Historischen Zeitungen. Das impresso-Projekt zwischen Text- und Bildverarbeitung, Design und Geschichtswissenschaft",,Maud Ehrmann;Estelle Bunout;Simon Clematide;Marten Düring;Andreas Fickers;Daniele Guido;Roman Kalyakin;Frederic Kaplan;Matteo Romanello;Paul Schroeder;Philip Ströbel;Thijs van Beek;Martin Volk;Lars Wieneke,paper,,xml,,,historische zeitungen;interface design;nlp;visualisierung,German,benannte entitäten (named entities);forschungsprozess;software;visualisierung;webentwicklung;werkzeuge
12356,2023 - Graz,Graz,Collaboration as Opportunity,2023,ADHO,ADHO,Karl-Franzens Universität Graz (University of Graz),Graz,,Austria,https://dh2023.adho.org/,OCR4all - Open-Source OCR and HTR Across the Centuries,,Florian Langhanki;Maximilian Wehner;Torsten Roeder;Christian Reul,workshop / tutorial,,txt,,,htr;layout analysis;ocr;transcription,English,artificial intelligence and machine learning;book and print history;computer science;humanities computing;image processing and analysis;information retrieval and querying algorithms and methods;optical character recognition and handwriting recognition;philology
12358,2023 - Graz,Graz,Collaboration as Opportunity,2023,ADHO,ADHO,Karl-Franzens Universität Graz (University of Graz),Graz,,Austria,https://dh2023.adho.org/,Digital Humanities Applications of spaCy’s Span Categorizer,,Edward Schmul;Ákos Kádár;Andrew Janco;David Lassner;Nick Budak;Toma Tasovac;Natalia Ermolaev;Jajwalya Karajgikar,workshop / tutorial,,txt,,,digital editions;ner;nlp;spacy;text annotation,English,"artificial intelligence and machine learning;history;humanities computing;linguistics;literary studies;manuscripts description, representation, and analysis;natural language processing;text encoding and markup language creation, deployment, and analysis"
12379,2023 - Graz,Graz,Collaboration as Opportunity,2023,ADHO,ADHO,Karl-Franzens Universität Graz (University of Graz),Graz,,Austria,https://dh2023.adho.org/,Marco Polo’s Travels Revisited: From Motion Event Detection to Optimal Path Computation in 3D Maps,,Andreas Niekler;Magdalena Wolska;Marvin Thiel;Matti Wiegmann;Benno Stein;Manuel Burghardt,"paper, specified ""long paper""",,txt,,,3d;gis;nlp;spatio-temporal analysis;travel literature,English,"electronic literature production and analysis;spatial & spatio-temporal analysis, modeling and visualization"
12387,2023 - Graz,Graz,Collaboration as Opportunity,2023,ADHO,ADHO,Karl-Franzens Universität Graz (University of Graz),Graz,,Austria,https://dh2023.adho.org/,Nestroy Corpus Analysis (NestroyCA): NLP for 19th century Austrian Drama,,Sabine Laszakovits;Christina Katsikadeli,"paper, specified ""short paper""",,txt,,,austrian german;drama;historical corpus;nestroy;nlp,English,"electronic literature production and analysis;humanities computing;linguistics;literary studies;natural language processing;performance studies: dance, theatre"
12423,2023 - Graz,Graz,Collaboration as Opportunity,2023,ADHO,ADHO,Karl-Franzens Universität Graz (University of Graz),Graz,,Austria,https://dh2023.adho.org/,"Digitizing the Messkataloge : Revealing the History of German Publishers, Authors and Translators",,Jeffrey Tharsen;David Kretz,"paper, specified ""short paper""",,txt,,,critical editions;german book history;messkatologe;ocr,English,"bibliographic analysis;book and print history;cultural analytics;cultural studies;database creation, management, and analysis;text mining and analysis;translation studies"
12499,2023 - Graz,Graz,Collaboration as Opportunity,2023,ADHO,ADHO,Karl-Franzens Universität Graz (University of Graz),Graz,,Austria,https://dh2023.adho.org/,Humanistic NLP: Bridging the Gap Between Digital Humanities and Natural Language Processing,,Toma Tasovac;Natalia Ermolaev;Andrew Janco;David Lassner;Nick Budak,"paper, specified ""long paper""",,txt,,,dh;humanities;interdisciplinarity;nlp;pedagogy,English,artificial intelligence and machine learning;curricular and pedagogical development and analysis;humanities computing;linguistics;natural language processing
12543,2023 - Graz,Graz,Collaboration as Opportunity,2023,ADHO,ADHO,Karl-Franzens Universität Graz (University of Graz),Graz,,Austria,https://dh2023.adho.org/,Transhistorical Resonance: Medieval Chinese Scholarship as Data,,Nicholas Andrew Budak;Gian Duri Rominger,"paper, specified ""short paper""",,txt,,,ancient;chinese;commentary;nlp;phonology,English,"annotation structures, systems, and methods;artificial intelligence and machine learning;asian studies;linguistics;natural language processing;philology;text mining and analysis"
12547,2023 - Graz,Graz,Collaboration as Opportunity,2023,ADHO,ADHO,Karl-Franzens Universität Graz (University of Graz),Graz,,Austria,https://dh2023.adho.org/,Modeling Eco-Poetics and Eco-Politics in 20th Century Anglophone Climate Fiction: Toxic Water,,Dez Mary Miller;Henry Alexander Wermer-Colan;SaraGrace Stefan;Megan Kane,"paper, specified ""short paper""",,txt,,,ecocriticism;hathitrust;science fiction;topic modeling;word2vec,English,"book and print history;cultural analytics;cultural studies;digital libraries creation, management, and analysis;eco-criticism and environmental analysis;environmental, ocean, and waterway studies;literary studies;natural language processing"
